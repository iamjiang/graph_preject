{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --quiet --pre --upgrade dgl-cu101\n",
    "# !pip install --quiet torch==1.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import itertools\n",
    "import os\n",
    "import numpy as np\n",
    "from numpy import save,load,savetxt,loadtxt,savez_compressed\n",
    "from sklearn.metrics import roc_auc_score, f1_score,average_precision_score\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import auc as auc_score\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "import time\n",
    "from tqdm import tqdm, tqdm_notebook,tnrange\n",
    "tqdm.pandas(position=0, leave=True)\n",
    "import math \n",
    "\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import dgl\n",
    "import dgl.nn as dglnn\n",
    "import dgl.function as fn\n",
    "from dgl.ops import edge_softmax\n",
    "\n",
    "from functools import partial\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import random\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize']=(5.0,4.0)\n",
    "plt.rcParams['image.interpolation']='nearest'\n",
    "plt.rcParams['image.cmap']='gray'\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import utils\n",
    "import tsne_func\n",
    "print(\"torch version is {}\".format(th.__version__))\n",
    "print(\"DGL version is {}\".format(dgl.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    th.manual_seed(seed)\n",
    "    th.cuda.manual_seed_all(seed)\n",
    "    th.backends.cudnn.deterministic = True\n",
    "    th.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    \n",
    "seed_everything(101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KG_dir=\"/workspace/cjiang/eagle_project/CAP_graph/dataset/\"\n",
    "start=time.time()\n",
    "with open(os.path.join(KG_dir,'CAP_Graph'), 'rb') as f:\n",
    "    G, node_labels = pickle.load(f)\n",
    "end=time.time()\n",
    "print(\"It took {:0.4f} seconds to load graph\".format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils.graph_show(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usaanr_feat=[]\n",
    "for key, scheme in G.node_attr_schemes(ntype=\"usaanr\").items():\n",
    "    usaanr_feat.append(key)\n",
    "usaanr_feat=[x for x in usaanr_feat if x not in ['type','usaanr','train_mask','val_mask','test_mask']]\n",
    "\n",
    "print()\n",
    "print(\"The features associated with USAA Member are\\n \")\n",
    "for i in usaanr_feat:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## USAA Members Features Embedding\n",
    "class USAANR_Embedding(nn.Module):\n",
    "    def __init__(self,G,feature_size):\n",
    "        super(USAANR_Embedding,self).__init__()\n",
    "        self.G=G\n",
    "        self.feature_size=feature_size\n",
    "        ## Embedding matrices for features of nodes.\n",
    "        self.emb = nn.ModuleDict()\n",
    "        \n",
    "        \n",
    "        for i,col in enumerate(usaanr_feat):\n",
    "            self.emb[col]=nn.Embedding(G.nodes['usaanr'].data[col].max().item()+1, feature_size)\n",
    "        ## Embedding for the node of House Properties\n",
    "#         self.node_emb=nn.Embedding(G.filter_nodes(lambda nodes: (nodes.data['node_type'] == 0).squeeze(1),\\\n",
    "#                                        ntype='House_Properties').max().item() + 1, feature_size)\n",
    "        self.node_emb=nn.Embedding(2, feature_size)\n",
    "    \n",
    "    def forward(self,nid):\n",
    "        nid=nid.to(\"cpu\")\n",
    "        h=self.node_emb(self.G.nodes['usaanr'].data['type'][nid].squeeze(1).to(device))\n",
    "        extra_repr=[]\n",
    "        for i,col in enumerate(usaanr_feat):\n",
    "            ndata=self.G.nodes['usaanr'].data[col]\n",
    "            extra_repr.append(self.emb[col](ndata[nid].to(device)).squeeze(1))\n",
    "        return h + th.stack(extra_repr, 0).sum(0)\n",
    "\n",
    "## zipcode Embedding\n",
    "class Zipcode_Embedding(nn.Module):\n",
    "    def __init__(self,G,feature_size):\n",
    "        super(Zipcode_Embedding,self).__init__()\n",
    "        self.G=G\n",
    "        self.feature_size=feature_size\n",
    "        \n",
    "        ## Embedding for the node of zipcode.\n",
    "#         self.node_emb=nn.Embedding(G.filter_nodes(lambda nodes: (nodes.data['type'] == 1).squeeze(1),\\\n",
    "#                                        ntype='zipcode').max().item() + 1, feature_size)\n",
    "        self.node_emb=nn.Embedding(2, feature_size)\n",
    "    \n",
    "    def forward(self,nid):\n",
    "        nid=nid.to(\"cpu\")\n",
    "        h=self.node_emb(self.G.nodes['zipcode'].data['type'][nid].squeeze(1).to(device))\n",
    "        return h \n",
    "\n",
    "class HGTLayer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_dim,\n",
    "                 out_dim,\n",
    "                 node_dict,\n",
    "                 edge_dict,\n",
    "                 n_heads,\n",
    "                 dropout = 0.2,\n",
    "                 use_norm = False):\n",
    "        super(HGTLayer, self).__init__()\n",
    "\n",
    "        self.in_dim        = in_dim\n",
    "        self.out_dim       = out_dim\n",
    "        self.node_dict     = node_dict\n",
    "        self.edge_dict     = edge_dict\n",
    "        self.num_types     = len(node_dict)\n",
    "        self.num_relations = len(edge_dict)\n",
    "        self.total_rel     = self.num_types * self.num_relations * self.num_types\n",
    "        self.n_heads       = n_heads\n",
    "        self.d_k           = out_dim // n_heads\n",
    "        self.sqrt_dk       = math.sqrt(self.d_k)\n",
    "        self.att           = None\n",
    "\n",
    "        self.k_linears   = nn.ModuleList()\n",
    "        self.q_linears   = nn.ModuleList()\n",
    "        self.v_linears   = nn.ModuleList()\n",
    "        self.a_linears   = nn.ModuleList()\n",
    "        self.norms       = nn.ModuleList()\n",
    "        self.use_norm    = use_norm\n",
    "\n",
    "        for t in range(self.num_types):\n",
    "            self.k_linears.append(nn.Linear(in_dim,   out_dim))\n",
    "            self.q_linears.append(nn.Linear(in_dim,   out_dim))\n",
    "            self.v_linears.append(nn.Linear(in_dim,   out_dim))\n",
    "            self.a_linears.append(nn.Linear(out_dim,  out_dim))\n",
    "            if use_norm:\n",
    "                self.norms.append(nn.LayerNorm(out_dim))\n",
    "\n",
    "        self.relation_pri   = nn.Parameter(th.ones(self.num_relations, self.n_heads))\n",
    "        self.relation_att   = nn.Parameter(th.Tensor(self.num_relations, n_heads, self.d_k, self.d_k))\n",
    "        self.relation_msg   = nn.Parameter(th.Tensor(self.num_relations, n_heads, self.d_k, self.d_k))\n",
    "        self.skip           = nn.Parameter(th.ones(self.num_types))\n",
    "        self.drop           = nn.Dropout(dropout)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.relation_att)\n",
    "        nn.init.xavier_uniform_(self.relation_msg)\n",
    "\n",
    "    def forward(self, sg, h):\n",
    "        with sg.local_scope():\n",
    "            node_dict, edge_dict = self.node_dict, self.edge_dict\n",
    "            \n",
    "            #### remove the edges where the number of edges==0\n",
    "            etypes_with_nonzero_edges = [etype for etype in sg.etypes if sg.num_edges(etype) > 0]\n",
    "            sg=sg.edge_type_subgraph(etypes_with_nonzero_edges) \n",
    "            \n",
    "            for srctype, etype, dsttype in sg.canonical_etypes:\n",
    "                sub_graph = sg[srctype, etype, dsttype]\n",
    "                \n",
    "                if sub_graph.num_edges()==0:\n",
    "                    continue\n",
    "                else:   \n",
    "                    k_linear = self.k_linears[node_dict[srctype]]\n",
    "                    v_linear = self.v_linears[node_dict[srctype]]\n",
    "                    q_linear = self.q_linears[node_dict[dsttype]]\n",
    "\n",
    "                    k = k_linear(h[srctype]).view(-1, self.n_heads, self.d_k)\n",
    "                    v = v_linear(h[srctype]).view(-1, self.n_heads, self.d_k)\n",
    "                    q = q_linear(h[dsttype][0:sg.num_dst_nodes()]).view(-1, self.n_heads, self.d_k)\n",
    "\n",
    "                    e_id = self.edge_dict[etype]\n",
    "\n",
    "                    relation_att = self.relation_att[e_id]\n",
    "                    relation_pri = self.relation_pri[e_id]\n",
    "                    relation_msg = self.relation_msg[e_id]\n",
    "\n",
    "                    k = th.einsum(\"bij,ijk->bik\", k, relation_att)\n",
    "                    v = th.einsum(\"bij,ijk->bik\", v, relation_msg)\n",
    "\n",
    "                    sub_graph.srcdata['k'] = k\n",
    "                    sub_graph.dstdata['q'] = q\n",
    "                    sub_graph.srcdata['v'] = v\n",
    "\n",
    "                    sub_graph.apply_edges(fn.v_dot_u('q', 'k', 't'))\n",
    "                    attn_score = sub_graph.edata.pop('t').sum(-1) * relation_pri / self.sqrt_dk\n",
    "                    attn_score = edge_softmax(sub_graph, attn_score, norm_by='dst')\n",
    "\n",
    "                    sub_graph.edata['t'] = attn_score.unsqueeze(-1)\n",
    "\n",
    "            sg.multi_update_all({etype : (fn.u_mul_e('v', 't', 'm'), fn.sum('m', 't')) \\\n",
    "                                for etype in edge_dict}, cross_reducer = 'mean')\n",
    "\n",
    "            new_h = {}\n",
    "            for ntype in sg.ntypes:\n",
    "                '''\n",
    "                    Step 3: Target-specific Aggregation\n",
    "                    x = norm( W[node_type] * gelu( Agg(x) ) + x )\n",
    "                '''\n",
    "                n_id = node_dict[ntype]\n",
    "                alpha = th.sigmoid(self.skip[n_id])\n",
    "                t = sg.nodes[ntype].data['t'].view(-1, self.out_dim)\n",
    "                trans_out = self.drop(self.a_linears[n_id](t))\n",
    "                trans_out = trans_out * alpha + h[ntype] * (1-alpha)\n",
    "                if self.use_norm:\n",
    "                    new_h[ntype] = self.norms[n_id](trans_out)\n",
    "                else:\n",
    "                    new_h[ntype] = trans_out\n",
    "            return new_h\n",
    "        \n",
    "class HGT(nn.Module):\n",
    "    def __init__(self, G, node_dict, edge_dict, in_feat, h_dim, out_feat, n_layers, n_heads, use_norm = True):\n",
    "        super(HGT, self).__init__()\n",
    "        self.G=G\n",
    "        self.node_dict = node_dict\n",
    "        self.edge_dict = edge_dict\n",
    "        self.gcs = nn.ModuleList()\n",
    "        self.in_feat = in_feat\n",
    "        self.h_dim = h_dim\n",
    "        self.out_feat = out_feat\n",
    "        self.n_layers = n_layers\n",
    "        self.adapt_ws  = nn.ModuleList()\n",
    "        for t in range(len(node_dict)):\n",
    "            self.adapt_ws.append(nn.Linear(in_feat,   h_dim))\n",
    "        for _ in range(n_layers):\n",
    "            self.gcs.append(HGTLayer(h_dim, h_dim, node_dict, edge_dict, n_heads, use_norm = use_norm))\n",
    "        self.out = nn.Linear(h_dim, out_feat)\n",
    "        \n",
    "        self.node_embed=nn.ModuleDict()\n",
    "        self.node_embed['usaanr'] = USAANR_Embedding(self.G,self.in_feat)\n",
    "        self.node_embed['zipcode'] = Zipcode_Embedding(self.G,self.in_feat)\n",
    "           \n",
    "    def forward(self, input_nodes, blocks=None):\n",
    "        H = {}\n",
    "        for ntype, nid in input_nodes.items():\n",
    "            nid = input_nodes[ntype]\n",
    "            H[ntype] = F.gelu(self.adapt_ws[self.node_dict[ntype]](self.node_embed[ntype](nid)))\n",
    "        \n",
    "        if blocks is None:\n",
    "            for layer in self.gcs:\n",
    "                H = layer(self.G, H)\n",
    "        else:\n",
    "            for layer, block in zip(self.gcs, blocks):\n",
    "                H = layer(block, H)\n",
    "        return self.out(H[\"usaanr\"])       \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, labels, category, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "#     total_precision=0\n",
    "#     total_recall=0\n",
    "#     total_fscore=0\n",
    "    total_auc=0\n",
    "#     total_pr_auc=0\n",
    "    \n",
    "    count = 0\n",
    "    count_loss=0\n",
    "    \n",
    "    for input_nodes, seeds, blocks in tqdm(loader, position=0, leave=True):\n",
    "        blocks = [blk.to(device) for blk in blocks]\n",
    "        seeds = seeds[category]\n",
    "        input_nodes={k : e.to(device) for k, e in input_nodes.items()}\n",
    "        lbl = labels[seeds].to(device)\n",
    "        logits,h = model(input_nodes,blocks)\n",
    "        loss = F.cross_entropy(logits, lbl.squeeze(1).to(device))\n",
    "#         loss = F.cross_entropy(logits, lbl.squeeze(1),weight=th.Tensor([1,args.weight]).to(device))\n",
    "        acc = th.sum(logits.argmax(dim=1) == lbl.squeeze(1)).item() / logits.shape[0]\n",
    "#         precision, recall, fscore, support = score(lbl.squeeze(1).cpu().numpy(), logits.argmax(dim=1).cpu().numpy())\n",
    "\n",
    "        tempt=lbl.detach().cpu().numpy()\n",
    "        labels_train_one_hot=np.zeros(shape=(tempt.shape[0],8),dtype=np.float32)\n",
    "        labels_train_one_hot[np.arange(tempt.shape[0]),np.array([ele.item() for ele in tempt])]=1\n",
    "        auc = roc_auc_score(labels_train_one_hot.ravel(), th.sigmoid(logits).detach().cpu().numpy().ravel())\n",
    "        \n",
    "#         auc = roc_auc_score(lbl.detach().cpu().numpy().ravel(), th.sigmoid(logits)[:,1].detach().cpu().numpy().ravel())\n",
    "#         prec,rec,_ = precision_recall_curve(lbl.detach().cpu().numpy().ravel(), th.sigmoid(logits)[:,1].detach().cpu().numpy().ravel())\n",
    "#         pr_auc=auc_score(rec,prec)\n",
    "        \n",
    "        total_loss += loss.item() * len(seeds) \n",
    "        total_acc += acc\n",
    "#         total_precision += precision[1]\n",
    "#         total_recall += recall[1]\n",
    "#         total_fscore += fscore[1]\n",
    "        total_auc += auc\n",
    "#         total_pr_auc += pr_auc\n",
    "        count += 1\n",
    "        count_loss += len(seeds)\n",
    "    \n",
    "    ACCURACY=total_acc / count\n",
    "    LOSS=total_loss / count_loss\n",
    "#     PRECISION=total_precision/count\n",
    "#     RECALL=total_recall/count\n",
    "#     F1_SCORE=total_fscore/count\n",
    "    AUC=total_auc/count\n",
    "#     PR_AUC=total_pr_auc/count\n",
    "    \n",
    "    return ACCURACY, LOSS, AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create subgraph for the purpose of preliminary test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_nodes={\"usaanr\":th.arange(G.num_nodes('usaanr'))[0:1000],'zipcode':th.arange(G.num_nodes('zipcode'))[0:100]}\n",
    "# sg=dgl.node_subgraph(G,dict_nodes)\n",
    "\n",
    "dict_edges={}\n",
    "for etype in G.etypes:\n",
    "    dict_edges[etype]=th.arange(G.num_edges(etype))[0:5000]\n",
    "sg=dgl.edge_subgraph(G,dict_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.graph_show(sg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sg.nodes['usaanr'].data[\"_ID\"].numpy().shape,node_labels.shape, node_labels[sg.nodes['usaanr'].data[\"_ID\"]].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='HGT')\n",
    "parser.add_argument(\"--dropout\", type=float, default=0,\n",
    "        help=\"dropout probability\")\n",
    "parser.add_argument(\"--h_dim\", type=int, default=128,\n",
    "        help=\"number of hidden units\")\n",
    "parser.add_argument(\"--out_dim\", type=int, default=1,\n",
    "        help=\"output dimension\")\n",
    "parser.add_argument(\"--gpu\", type=int, default=0,\n",
    "        help=\"gpu\")\n",
    "parser.add_argument(\"--lr\", type=float, default=1e-5,\n",
    "        help=\"learning rate\")\n",
    "parser.add_argument('--clip',    type=int, default=1.0) \n",
    "parser.add_argument('--max_lr',  type=float, default=1e-3) \n",
    "\n",
    "parser.add_argument(\"--num_layers\", type=int, default=1,\n",
    "        help=\"number of propagation rounds\")\n",
    "parser.add_argument(\"-e\", \"--n_epochs\", type=int, default=1,\n",
    "        help=\"number of training epochs\")\n",
    "parser.add_argument(\"--model_path\", type=str, default=\"/workspace/cjiang/eagle_project/CAP_graph/hgt_model_param.pt\",\n",
    "        help='path for save the model')\n",
    "parser.add_argument(\"--l2norm\", type=float, default=0,\n",
    "        help=\"l2 norm coef\")\n",
    "\n",
    "parser.add_argument(\"--batch-size\", type=int, default=1024,\n",
    "        help=\"Mini-batch size. If -1, use full graph training.\")\n",
    "parser.add_argument(\"--num_mini_batch\", type=int, default=8,\n",
    "        help=\"Number of minibatch.\")\n",
    "parser.add_argument(\"--fanout\", type=int, default=None,\n",
    "        help=\"Fan-out of neighbor sampling.\")\n",
    "\n",
    "parser.add_argument(\"--seed\",  type=int,default=101,\n",
    "        help=\"random seed for np.random.seed, torch.manual_seed and torch.cuda.manual_seed.\")\n",
    "\n",
    "parser.add_argument(\"--num_worker\",  type=int,default=4,  \n",
    "        help=\"number of worker for neighbor sampling\") \n",
    "\n",
    "args,unknown=parser.parse_known_args()\n",
    "\n",
    "args.num_layers=1\n",
    "args.dropout=0.2\n",
    "args.lr=1e-3\n",
    "args.l2norm=1e-3\n",
    "args.n_epochs=1\n",
    "args.h_dim=64\n",
    "args.batch_size=1024\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### setting up training, validation and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rels=len(G.etypes)\n",
    "LABEL=th.tensor(node_labels[sg.nodes['usaanr'].data[\"_ID\"]]).long()\n",
    "labels, count=th.unique(LABEL,return_counts=True)\n",
    "num_classes=labels.shape[0]\n",
    "pd.DataFrame({\"label_class\":labels, \"count\":count}).style.format({'count':'{:,}'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mask=sg.nodes[\"usaanr\"].data.pop('train_mask')\n",
    "val_mask=sg.nodes[\"usaanr\"].data.pop('val_mask')\n",
    "test_mask=sg.nodes[\"usaanr\"].data.pop('test_mask')\n",
    "\n",
    "train_idx=th.nonzero(train_mask.squeeze(1)).numpy()\n",
    "val_idx=th.nonzero(val_mask.squeeze(1)).numpy()\n",
    "test_idx=th.nonzero(test_mask.squeeze(1)).numpy()\n",
    "\n",
    "train_idx=th.from_numpy(train_idx).squeeze(1)    \n",
    "val_idx=th.from_numpy(val_idx).squeeze(1)    \n",
    "test_idx=th.from_numpy(test_idx).squeeze(1)\n",
    "\n",
    "train_label=LABEL[train_idx]\n",
    "val_label=LABEL[val_idx]\n",
    "test_label=LABEL[test_idx]\n",
    "\n",
    "print('{:<15} {:<10,}'.format(\"Training set\",train_idx.shape[0]))\n",
    "print('{:<15} {:<10,}'.format(\"validation set\",val_idx.shape[0]))\n",
    "print('{:<15} {:<10,}'.format(\"test set\",test_idx.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert train_idx.shape[0]+val_idx.shape[0]+test_idx.shape[0] == sg.nodes['usaanr'].data[\"_ID\"].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### check cuda\n",
    "device=\"cpu\"\n",
    "use_cuda=args.gpu>=0 and th.cuda.is_available()\n",
    "if use_cuda:\n",
    "    th.cuda.set_device(args.gpu)\n",
    "    device='cuda:%d' % args.gpu\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_dict = {}\n",
    "edge_dict = {}\n",
    "for ntype in sg.ntypes:\n",
    "    node_dict[ntype] = len(node_dict)\n",
    "for etype in sg.etypes:\n",
    "    edge_dict[etype] = len(edge_dict)\n",
    "    sg.edges[etype].data['id'] = th.ones(sg.number_of_edges(etype), dtype=th.long) * edge_dict[etype] \n",
    "\n",
    "print(node_dict)\n",
    "print(edge_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for etype in sg.etypes:\n",
    "    for key,val in sg.edges[etype].data.items():\n",
    "        if key==\"id\":\n",
    "            print(\"{:<15}{:<10}{:<10}\".format(etype,key,th.unique(val).item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "\n",
    "model = HGT(sg,\n",
    "            node_dict, \n",
    "            edge_dict,\n",
    "            in_feat=args.h_dim,\n",
    "            h_dim=args.h_dim,\n",
    "            out_feat=num_classes,\n",
    "            n_layers=2,\n",
    "            n_heads=4,\n",
    "            use_norm = True)\n",
    "\n",
    "if use_cuda:\n",
    "    model.cuda()\n",
    "\n",
    "model.gcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = th.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.l2norm)\n",
    "optimizer = th.optim.AdamW(model.parameters())\n",
    "scheduler = th.optim.lr_scheduler.OneCycleLR(optimizer, total_steps=args.n_epochs, max_lr = args.max_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train sampler\n",
    "sampler = dgl.dataloading.MultiLayerNeighborSampler([args.fanout] * args.num_layers)\n",
    "train_loader = dgl.dataloading.NodeDataLoader(\n",
    "    sg, {'usaanr': train_idx}, sampler,\n",
    "    batch_size=args.batch_size, shuffle=True, num_workers=args.num_worker)\n",
    "# validation sampler\n",
    "# we do not use full neighbor to save computation resources\n",
    "val_sampler = dgl.dataloading.MultiLayerNeighborSampler([args.fanout] * args.num_layers)\n",
    "val_loader = dgl.dataloading.NodeDataLoader(\n",
    "    sg, {'usaanr': val_idx}, val_sampler,\n",
    "    batch_size=args.batch_size, shuffle=False, num_workers=args.num_worker)\n",
    "\n",
    "test_sampler = dgl.dataloading.MultiLayerNeighborSampler([args.fanout] * args.num_layers)\n",
    "test_loader = dgl.dataloading.NodeDataLoader(\n",
    "    sg, {'usaanr': test_idx}, test_sampler,\n",
    "    batch_size=args.batch_size, shuffle=False, num_workers=args.num_worker)\n",
    "\n",
    "print(\"The number of minibatch in training set is {:,}\".format(len(train_loader)))\n",
    "print(\"The number of minibatch in validation set is {:,}\".format(len(val_loader)))\n",
    "print(\"The number of minibatch in test set is {:,}\".format(len(test_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The total # of parameter is {:,}\".format(sum([p.nelement() for p in model.parameters()]) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dict={n: p.nelement() for n, p in model.named_parameters()}\n",
    "for i,j in param_dict.items():\n",
    "    print(\"{:<70}{:<15,}\".format(i,j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pdb\n",
    "LOSS_EPOCH=[]\n",
    "LABEL_TRAIN=[]\n",
    "ACC_EPOCH=[]\n",
    "# training loop\n",
    "print(\"start training...\")\n",
    "dur = []\n",
    "total_loss=0\n",
    "losses=[]\n",
    "\n",
    "# th.manual_seed(args.seed)\n",
    "# th.cuda.manual_seed(args.seed)\n",
    "# th.cuda.manual_seed_all(args.seed)\n",
    "# np.ranom.seed(args.seed)\n",
    "# random.seed(args.seed)\n",
    "# th.backends.cudnn.deterministic=True\n",
    "\n",
    "for epoch in tqdm(range(0,args.n_epochs)):\n",
    "    \n",
    "    model.train()\n",
    "    IDX=[]\n",
    "    H=[]\n",
    "    \n",
    "    #====================================#\n",
    "    #            Traning                 #\n",
    "    #====================================#\n",
    "    print(\"\")\n",
    "    print(\"========= Epoch {:} /{:}\".format(epoch+1,args.n_epochs))\n",
    "    print(\"Training...\")\n",
    "    t0 = time.time()\n",
    "    for step, (input_nodes, seeds, blocks) in enumerate(train_loader):\n",
    "        blocks = [blk.to(device) for blk in blocks]\n",
    "        seeds=seeds[\"usaanr\"].to(device)\n",
    "        labels_train=LABEL[seeds]       \n",
    "        labels_train = labels_train.to(device)\n",
    "        input_nodes={k : e.to(device) for k, e in input_nodes.items()}\n",
    "        logits,h = model(input_nodes,blocks)\n",
    "        optimizer.zero_grad()\n",
    "#         loss = F.cross_entropy(logits, labels_train.squeeze(1),weight=th.Tensor([1,args.weight]).to(device))\n",
    "        loss = F.cross_entropy(logits, labels_train.squeeze(1).to(device))\n",
    "        total_loss+=loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        train_acc = th.sum(logits.argmax(dim=1) == labels_train.squeeze(1)).item() / len(seeds)\n",
    "#         precision, recall, fscore, support = score(labels_train.squeeze(1).cpu().numpy(), logits.argmax(dim=1).cpu().numpy())\n",
    "\n",
    "        tempt=labels_train.detach().cpu().numpy()\n",
    "        labels_train_one_hot=np.zeros(shape=(tempt.shape[0],8),dtype=np.float32)\n",
    "        labels_train_one_hot[np.arange(tempt.shape[0]),np.array([ele.item() for ele in tempt])]=1\n",
    "        train_auc = roc_auc_score(labels_train_one_hot.ravel(), th.sigmoid(logits).detach().cpu().numpy().ravel())\n",
    "#         train_auc = roc_auc_score(labels_train.detach().cpu().numpy().ravel(), th.sigmoid(logits)[:,1].detach().cpu().numpy().ravel())\n",
    "        \n",
    "#         prec,rec,_ = precision_recall_curve(labels_train.detach().cpu().numpy().ravel(), th.sigmoid(logits)[:,1].detach().cpu().numpy().ravel())\n",
    "#         train_pr_auc=auc_score(rec,prec)\n",
    "\n",
    "        IDX.extend(seeds.detach().cpu().numpy().tolist())\n",
    "        H.extend(h[\"usaanr\"].detach().cpu().numpy().tolist())\n",
    "        \n",
    "        if step%(len(train_loader)//10)==0 and not step==0:\n",
    "\n",
    "            t1 = time.time()\n",
    "            elapsed=utils.format_time(t1-t0)\n",
    "            print(\"Batch {:} of {:} | Loss {:.3f}  | Accuracy {:.2%} | ROC_AUC {:.2%}  | Elapsed: {:}\".\\\n",
    "                  format(step,len(train_loader),np.mean(losses[-10:]),train_acc,train_auc,elapsed))    \n",
    "    \n",
    "    LOSS_EPOCH.append(loss)\n",
    "\n",
    "    LABEL_TRAIN.append(LABEL[blocks[-1].nodes[\"usaanr\"].data[dgl.NID].cpu().numpy()])\n",
    "#     ACC_EPOCH.append(train_acc)\n",
    "\n",
    "    model.eval()\n",
    "    print()\n",
    "    print(\"\")\n",
    "    print(\"Running Validation on training set\")\n",
    "    print(\"\")\n",
    "    \n",
    "    \n",
    "    acc_train, loss_train, train_auc = evaluate(model, train_loader,  LABEL, \"usaanr\", device)\n",
    "    \n",
    "    t2=time.time()\n",
    "    \n",
    "    print(\"loss: {:.3f} |  Accuracy {:.2%} | ROC_AUC: {:.2%}  | Elapsed: {:}\"\\\n",
    "      .format(loss_train, acc_train,  train_auc, utils.format_time(t2-t1)))\n",
    "\n",
    "    #====================================#\n",
    "    #            Validation-set          #\n",
    "    #====================================#\n",
    "    \n",
    "    model.eval()\n",
    "    print()\n",
    "    print(\"\")\n",
    "    print(\"Running Validation\")\n",
    "    print(\"\")\n",
    "    acc_val, loss_val, val_auc = evaluate(model, val_loader,  LABEL, \"usaanr\", device)\n",
    "    \n",
    "    t3=time.time()\n",
    "    \n",
    "    print(\"loss: {:.3f} |  Accuracy {:.2%}  | ROC_AUC: {:.2%} | Elapsed: {:}\"\\\n",
    "      .format(loss_val, acc_val, val_auc, utils.format_time(t3-t2)))\n",
    "    \n",
    "if args.model_path is not None:\n",
    "    th.save(model.state_dict(), args.model_path)\n",
    "    \n",
    "#====================================#\n",
    "#            Test-set                #\n",
    "#====================================#\n",
    "print()\n",
    "print(\"\")\n",
    "print(\"Running Validation in Test Dataset\")\n",
    "print(\"\")\n",
    "model.eval()\n",
    "\n",
    "acc_test, loss_test, test_auc= evaluate(model, test_loader,  LABEL, \"usaanr\", device)\n",
    "\n",
    "t3=time.time()\n",
    "print(\"loss: {:.3f} |  Accuracy {:.2%} | ROC_AUC: {:.2%} | Elapsed: {:}\"\\\n",
    "      .format(loss_test, acc_test, test_auc, utils.format_time(t3-t2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits,h=model(input_nodes,blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adapt_ws  = nn.ModuleList()\n",
    "for t in range(len(node_dict)):\n",
    "    adapt_ws.append(nn.Linear(args.h_dim,   args.h_dim))\n",
    "adapt_ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_heads=4\n",
    "gcs = nn.ModuleList()\n",
    "for _ in range(args.num_layers):\n",
    "    gcs.append(HGTLayer(args.h_dim, args.h_dim, node_dict, edge_dict, n_heads, use_norm = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = {}\n",
    "for ntype, nid in input_nodes.items():\n",
    "    nid = input_nodes[ntype]\n",
    "    H[ntype] = F.gelu(adapt_ws[nid](node_embed[ntype](nid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_embed=nn.ModuleDict()\n",
    "node_embed['usaanr'] = USAANR_Embedding(sg,args.h_dim)\n",
    "node_embed['zipcode'] = Zipcode_Embedding(sg,args.h_dim)\n",
    "node_embed[ntype](nid).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sparsity rate of embedding vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H=np.array(H)\n",
    "non_zero=np.count_nonzero(H)\n",
    "total_val=np.product(H.shape)\n",
    "sparsity=(total_val-non_zero)/total_val\n",
    "density=non_zero/total_val\n",
    "print(\"sparsity rate is {:.2%}\".format(sparsity))\n",
    "print(\"density rate is {:.2%}\".format(density))\n",
    "print(\"embedding vector shape is {}\".format(H.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDX_train=np.array(IDX)\n",
    "H_train=np.array(H)\n",
    "mask_train=np.array(['train']*len(IDX_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDX_val=[]\n",
    "H_val=[]\n",
    "for input_nodes, seeds, blocks in tqdm(val_loader, position=0, leave=True):\n",
    "    blocks = [blk.to(device) for blk in blocks]\n",
    "    seeds = seeds[\"usaanr\"]\n",
    "    input_nodes={k : e.to(device) for k, e in input_nodes.items()}\n",
    "    model.eval()\n",
    "    logits,h = model(input_nodes,blocks)\n",
    "    IDX_val.extend(seeds.detach().cpu().numpy().tolist())\n",
    "    H_val.extend(h[\"usaanr\"].detach().cpu().numpy().tolist())\n",
    "    \n",
    "IDX_val=np.array(IDX_val)\n",
    "H_val=np.array(H_val)\n",
    "mask_val=np.array(['val']*len(IDX_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDX_test=[]\n",
    "H_test=[]\n",
    "for input_nodes, seeds, blocks in tqdm(test_loader, position=0, leave=True):\n",
    "    blocks = [blk.to(device) for blk in blocks]\n",
    "    seeds = seeds[\"usaanr\"]\n",
    "    input_nodes={k : e.to(device) for k, e in input_nodes.items()}\n",
    "    model.eval()\n",
    "    logits,h = model(input_nodes,blocks)\n",
    "    IDX_test.extend(seeds.detach().cpu().numpy().tolist())\n",
    "    H_test.extend(h[\"usaanr\"].detach().cpu().numpy().tolist())\n",
    "    \n",
    "IDX_test=np.array(IDX_test)\n",
    "H_test=np.array(H_test)\n",
    "mask_test=np.array(['test']*len(IDX_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDX=np.concatenate((IDX_train,IDX_val, IDX_test))\n",
    "H=np.concatenate((H_train,H_val, H_test))\n",
    "mask=np.concatenate((mask_train,mask_val, mask_test))\n",
    "\n",
    "_idx=IDX.argsort()  #### sort the node id from 0 to max_num, so that it can be matched with LABEL\n",
    "embedding_vector=H[_idx]\n",
    "mask=mask[_idx]\n",
    "\n",
    "print(\"{:<30}{}\".format(\"shape of embedding vector\",embedding_vector.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "savez_compressed(\"embedding_vector.npz\",embedding_vector)\n",
    "embedding_vector=load(\"embedding_vector.npz\")['arr_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LABEL.shape, H.shape,IDX_train.shape,IDX_val.shape, IDX_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### visualization of embedding vectors for different categories of USAA Members"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF=pd.DataFrame({\"IDX\":_idx.tolist(), \"MASK\":mask.tolist(), \"Product\":LABEL.squeeze().tolist()})\n",
    "prod_map={0:\"No_Product\",1:\"Rental_only\",2:\"Home_only\",3:\"Home+Rental\",4:\"Auto_only\",5:\"Auto+Rental\",6:\"Auto+Home\",7:\"Auto+Home_Rental\"}\n",
    "DF['Product'] = list(map(prod_map.get, DF['Product']))\n",
    "\n",
    "### the caterogry of \"Home+Rental\" is too rare, only 2767 in total\n",
    "inx=np.where(DF[\"Product\"].values!=\"Home+Rental\")[0]\n",
    "offset=np.where(DF[\"Product\"].values==\"Home+Rental\")[0].shape[0]\n",
    "DF=DF[DF[\"Product\"]!=\"Home+Rental\"]\n",
    "DF[\"IDX\"]=DF[\"IDX\"]- offset ## since removing \"Home+Rental\", the index should be offset\n",
    "embedding_vector=embedding_vector[inx]\n",
    "\n",
    "N=1000\n",
    "\n",
    "# sample_df=DF.groupby(['Product'], group_keys=False).apply(lambda x: x.sample(n=N))\n",
    "# sample_id=sample_df[\"IDX\"].values\n",
    "# embedding_sample=embedding_vector[sample_id]\n",
    "# prod_sample=DF['Product'].values[sample_id]\n",
    "\n",
    "sample_df=DF.groupby(['MASK','Product'], group_keys=False).apply(lambda x: x.sample(n=N))\n",
    "sample_id_train=sample_df[sample_df[\"MASK\"]==\"train\"][\"IDX\"].values\n",
    "sample_id_val=sample_df[sample_df[\"MASK\"]==\"val\"][\"IDX\"].values\n",
    "sample_id_test=sample_df[sample_df[\"MASK\"]==\"test\"][\"IDX\"].values\n",
    "\n",
    "embedding_sample_train=embedding_vector[sample_id_train]\n",
    "embedding_sample_val=embedding_vector[sample_id_val]\n",
    "embedding_sample_test=embedding_vector[sample_id_test]\n",
    "\n",
    "prod_sample_train=DF['Product'].values[sample_id_train]\n",
    "prod_sample_val=DF['Product'].values[sample_id_val]\n",
    "prod_sample_test=DF['Product'].values[sample_id_test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b=np.unique(prod_sample_train,return_counts=True)\n",
    "pd.DataFrame({\"product\":a,\"sampled_#\":b}).style.format({'sampled_#':'{:,}'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b=np.unique(prod_sample_val,return_counts=True)\n",
    "pd.DataFrame({\"product\":a,\"sampled_#\":b}).style.format({'sampled_#':'{:,}'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b=np.unique(prod_sample_test,return_counts=True)\n",
    "pd.DataFrame({\"product\":a,\"sampled_#\":b}).style.format({'sampled_#':'{:,}'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(\"t-SNE dimension reduction for training embedding vector:\")\n",
    "print()\n",
    "train_embedding = tsne_func.fit(embedding_sample_train)\n",
    "print(\"t-SNE dimension reduction for validation embedding:\")\n",
    "print()\n",
    "val_embedding = tsne_func.fit(embedding_sample_val)\n",
    "print(\"t-SNE dimension reduction for test embedding:\")\n",
    "print()\n",
    "test_embedding = tsne_func.fit(embedding_sample_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "palette = sns.color_palette(\"bright\", 8)\n",
    "ax=sns.scatterplot(train_embedding[:,0], train_embedding[:,1], hue=prod_sample_train.squeeze(), legend='full', palette=palette)\n",
    "ax.set_title(\"Embedding Vectors t-SNE(sample=1000) \\n Training Set \",fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "palette = sns.color_palette(\"bright\", 2)\n",
    "ax=sns.scatterplot(val_embedding[:,0], val_embedding[:,1], hue=prod_sample_val.squeeze(), legend='full', palette=palette)\n",
    "ax.set_title(\"Embedding Vectors t-SNE (sample=1000) \\n Validation Set \",fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "palette = sns.color_palette(\"bright\", 2)\n",
    "ax=sns.scatterplot(test_embedding[:,0], test_embedding[:,1], hue=prod_sample_test.squeeze(), legend='full', palette=palette)\n",
    "ax.set_title(\"Embedding Vectors t-SNE (sample=1000) \\n Test Set \",fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp_gpu)",
   "language": "python",
   "name": "nlp_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
