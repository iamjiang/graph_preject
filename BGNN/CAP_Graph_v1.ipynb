{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --quiet --pre --upgrade dgl-cu101\n",
    "# !pip install --quiet torch==1.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import dgl\n",
    "import dgl.function as fn\n",
    "from dgl.nn.pytorch import GATConv\n",
    "# from bipartite_gatconv import BipartiteGATConv\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "tqdm().pandas()\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize']=(5.0,4.0)\n",
    "plt.rcParams['image.interpolation']='nearest'\n",
    "plt.rcParams['image.cmap']='gray'\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "data_dir=\"/workspace/cjiang/eagle_project/CAP_graph/dataset/\"\n",
    "root_dir=\"/workspace/cjiang/eagle_project/CAP_graph/\"\n",
    "\n",
    "os.chdir(root_dir)\n",
    "\n",
    "print(\"{:<20}{:<20}\".format(\"torch version\",torch.__version__))\n",
    "print(\"{:<20}{:<20}\".format(\"DGL version\",dgl.__version__))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv(data_dir, file):\n",
    "    start=time.time()\n",
    "    df=pd.read_csv(os.path.join(data_dir,file))\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    end=time.time()\n",
    "    print(\"Dataloading running time is {:0.4f}\".format(end-start))\n",
    "    print(\"The Shape of Dataset is {}\".format(df.shape))\n",
    "    return df\n",
    "\n",
    "def to_pickle(data_dir,file_in,file_out):\n",
    "    start=time.time()\n",
    "    file_in.to_pickle(os.path.join(data_dir,file_out))\n",
    "    end=time.time()\n",
    "    print(\"pickle time is {:0.4f}\".format(end-start))\n",
    "    \n",
    "def read_pickle(data_dir,file):\n",
    "    start=time.time()\n",
    "    df=pd.read_pickle(os.path.join(data_dir,file))\n",
    "    end=time.time()\n",
    "    print(\"loading time is {:0.4f}\".format(end-start))\n",
    "    print(\"The Shape of Dataset is {}\".format(df.shape))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pers_Edges=read_csv(data_dir, 'Pers_Edges.csv')\n",
    "# to_pickle(data_dir,Pers_Edges,'Pers_Edges_pickle')\n",
    "Pers_Edges=read_pickle(data_dir,'Pers_Edges_pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Busi_Edges=read_csv(data_dir, 'Busi_Edges.csv')\n",
    "# to_pickle(data_dir,Busi_Edges,'Busi_Edges_pickle')\n",
    "Busi_Edges=read_pickle(data_dir,'Busi_Edges_pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zipcode_Edges=read_csv(data_dir, 'zipcode_Edges.csv')\n",
    "# to_pickle(data_dir,zipcode_Edges,'zipcode_Edges_pickle')\n",
    "zipcode_Edges=read_pickle(data_dir,'zipcode_Edges_pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Product_Edges=read_csv(data_dir, 'Product_Edges.csv')\n",
    "# to_pickle(data_dir,Product_Edges,'Product_Edges_pickle')\n",
    "Product_Edges=read_pickle(data_dir,'Product_Edges_pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vertices=read_csv(data_dir, 'vertices_clean.csv')\n",
    "# to_pickle(data_dir,vertices,'vertices_pickle')\n",
    "vertices=read_pickle(data_dir,'vertices_pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform product edge data and create product bit vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Product_Edges=Product_Edges[Product_Edges[\"dst\"].isin([\"P_AUTO\",\"P_HOME\",\"P_RENT\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def map_prod(LIST):\n",
    "#     if (LIST.count(\"P_AUTO\")==1) and (LIST.count(\"P_HOME\")==1) and (LIST.count(\"P_RENT\")==1):\n",
    "#         return [1,1,1,\"111\"]\n",
    "#     elif (LIST.count(\"P_AUTO\")==1) and (LIST.count(\"P_HOME\")==1) and (LIST.count(\"P_RENT\")==0):\n",
    "#         return [1,1,0,\"110\"]\n",
    "#     elif (LIST.count(\"P_AUTO\")==1) and (LIST.count(\"P_HOME\")==0) and (LIST.count(\"P_RENT\")==1):\n",
    "#         return [1,0,1,\"101\"]\n",
    "#     elif (LIST.count(\"P_AUTO\")==0) and (LIST.count(\"P_HOME\")==1) and (LIST.count(\"P_RENT\")==1):\n",
    "#         return [0,1,1,\"011\"]\n",
    "#     elif (LIST.count(\"P_AUTO\")==1) and (LIST.count(\"P_HOME\")==0) and (LIST.count(\"P_RENT\")==0):\n",
    "#         return [1,0,0,\"100\"]\n",
    "#     elif (LIST.count(\"P_AUTO\")==0) and (LIST.count(\"P_HOME\")==1) and (LIST.count(\"P_RENT\")==0):\n",
    "#         return [0,1,0,\"010\"]\n",
    "#     elif (LIST.count(\"P_AUTO\")==0) and (LIST.count(\"P_HOME\")==0) and (LIST.count(\"P_RENT\")==1):\n",
    "#         return [0,0,1,\"001\"]\n",
    "#     elif (LIST.count(\"P_AUTO\")==0) and (LIST.count(\"P_HOME\")==0) and (LIST.count(\"P_RENT\")==0):\n",
    "#         return [0,0,0,\"000\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import numba as nb\n",
    "# # @nb.jit(nopython=True)\n",
    "# def Flatten_Product(Product_Edges):\n",
    "#     Product_Edges.sort_values(by=\"src\",inplace=True)\n",
    "#     src, _, dst=np.array(Product_Edges).transpose()\n",
    "#     row=np.unique(src).shape[0]\n",
    "#     col=np.unique(dst).shape[0]\n",
    "#     Flatten_df=np.zeros([row,col+2],dtype=int)   ### usaa_number,  P_AUTO,  P_HOME,  P_RENT,  Product_bit_Vector\n",
    "    \n",
    "#     uniq_v, uniq_e=np.unique(Product_Edges[\"src\"],return_inverse=True)\n",
    "#     _, idx=np.unique(uniq_e,return_index=True)\n",
    "\n",
    "#     for i in tqdm(np.arange(len(uniq_v))):\n",
    "        \n",
    "#         if i!=len(uniq_v)-1:\n",
    "#             Flatten_df[i][0]=list(set(Product_Edges[idx[i]:idx[i+1]].src.astype(int)))[0]\n",
    "#             Flatten_df[i][1:5]=map_prod(list(set(Product_Edges[idx[i]:idx[i+1]].dst)))\n",
    "#         else:\n",
    "#             Flatten_df[i][0]=list(set(Product_Edges[idx[i]:].src.astype(int)))[0]\n",
    "#             Flatten_df[i][1:5]=map_prod(list(set(Product_Edges[idx[i]:].dst)))\n",
    "    \n",
    "#     Flatten_df=pd.DataFrame(Flatten_df,columns=[\"USAA_Number\",\"P_AUTO\",\"P_HOME\",\"P_RENT\",\"Prod_Trace\"])\n",
    "#     Flatten_df[\"Prod_Trace\"]=Flatten_df[\"Prod_Trace\"].apply(lambda x : '{0:0>3}'.format(x)) ## add the leading zero in some prod_trace\n",
    "    \n",
    "#     return Flatten_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten_df=Flatten_Product(Product_Edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_pickle(data_dir,Flatten_df,'Flatten_Product_pickle')\n",
    "Flatten_Product_pickle=read_pickle(data_dir,'Flatten_Product_pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempt1=pd.DataFrame(Flatten_Product_pickle[\"Prod_Trace\"].value_counts()).reset_index().rename(columns={'index':'product','Prod_Trace':'count'})\n",
    "tempt2=pd.DataFrame(Flatten_Product_pickle[\"Prod_Trace\"].value_counts(normalize=True)).reset_index().rename(columns={'index':'product','Prod_Trace':'percentage'})\n",
    "tempt1.merge(tempt2, on=\"product\", how=\"inner\").style.format({'count':'{:,}','percentage':'{:.2%}'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Pers_Edges Dataframe:\")\n",
    "print(Pers_Edges.head(2))\n",
    "print()\n",
    "print(\"Busi_Edges Dataframe:\")\n",
    "print(Busi_Edges.head(2))\n",
    "print()\n",
    "# print(\"zipcode_Edges Dataframe:\")\n",
    "# print(zipcode_Edges.head(2))\n",
    "# print()\n",
    "print(\"Product_Edges Dataframe:\")\n",
    "print(Flatten_Product_pickle.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create bi-directional relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datashow(dataframe):\n",
    "    tempt1=pd.DataFrame(dataframe.rel.value_counts()).reset_index().rename(columns={'index':'rels','rel':'count'})\n",
    "    tempt2=pd.DataFrame(dataframe.rel.value_counts(normalize=True)).reset_index().rename(columns={'index':'rels','rel':'percentage'})\n",
    "    return tempt1.merge(tempt2, on=\"rels\", how=\"inner\").style.format({'count':'{:,}','percentage':'{:.2%}'})\n",
    "\n",
    "def make_bidirectional(dataframe, rel_types, rev_rel_types):\n",
    "    \"\"\"\n",
    "    dataframe :    triplet(src, rel, dst)\n",
    "    rel_types:     relationship to be inversed bidirectiohally\n",
    "    rev_rel_types: the inversed relationship name\n",
    "    \"\"\"\n",
    "    np_df=np.array(dataframe)\n",
    "    src,rel,dst=np_df.transpose()\n",
    "    rel_v2=rel.copy()\n",
    "    for idx, val in enumerate(rel_types):\n",
    "        rel_v2[rel==val]=rev_rel_types[idx]\n",
    "        \n",
    "    src,dst=np.concatenate((src,dst)), np.concatenate((dst, src))\n",
    "    rel=np.concatenate((rel, rel_v2))\n",
    "    \n",
    "    DF=pd.DataFrame(sorted(zip(src, rel, dst)),columns=['src','rel','dst'])\n",
    "    DF.drop_duplicates(inplace=True)\n",
    "    \n",
    "    return DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datashow(Busi_Edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_types=Busi_Edges.rel.unique().tolist()  ## ['Busi_rel_Other', 'SPONSOR', 'AUTO_RELATED']\n",
    "rev_rel_types=['Busi_rel_Other','SPONSEE','AUTO_RELATED']\n",
    "for idx, val in enumerate(rel_types):\n",
    "    print(\"{:<30}{:<30}\".format(val, rev_rel_types[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "rel_types=Busi_Edges.rel.unique().tolist()  ## ['Busi_rel_Other', 'SPONSOR', 'AUTO_RELATED']\n",
    "rev_rel_types=['Busi_rel_Other','SPONSEE','AUTO_RELATED']\n",
    "Busi_Edges_bi=make_bidirectional(Busi_Edges, rel_types, rev_rel_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datashow(Busi_Edges_bi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datashow(zipcode_Edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zipcode_Edges['dst'].unique().shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# rel_types=zipcode_Edges.rel.unique().tolist()  ## ['Located_In']\n",
    "# rev_rel_types=[\"Location_of\"]\n",
    "# zipcode_Edges_bi=make_bidirectional(zipcode_Edges, rel_types, rev_rel_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datashow(zipcode_Edges_bi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pers_Edges=Pers_Edges.replace(to_replace =\"Step-Parent/Mother\",value =\"Step-Parent\")\n",
    "datashow(Pers_Edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_types=Pers_Edges.rel.unique().tolist()  \n",
    "rev_rel_types=['Child','Spouse','Parent','Ex-Spouse','Pers_rel_Other','Brother_Sister','Step-Child','Step-Parent']\n",
    "for idx, val in enumerate(rel_types):\n",
    "    print(\"{:<30}{:<30}\".format(val, rev_rel_types[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "Pers_Edges_bi=make_bidirectional(Pers_Edges, rel_types, rev_rel_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datashow(Pers_Edges_bi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct a Heterogeneous Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### re-label source nodes and destination nodes for each node type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#### relabel the nodes of USAA Member such that they are continous integers from 0 to max\n",
    "src_pers, _ ,dst_pers=np.array(Pers_Edges_bi).transpose()\n",
    "src_busi, _ ,dst_busi=np.array(Busi_Edges_bi).transpose()\n",
    "# src_zipcode, _ ,dst_zipcode=np.array(zipcode_Edges_bi[zipcode_Edges_bi['rel']==\"Located_In\"]).transpose()\n",
    "\n",
    "all_usaanr=np.concatenate((src_pers,dst_pers,src_busi,dst_busi))\n",
    "uniq_usaanr = np.unique(all_usaanr)\n",
    "# uniq_zipcode = np.unique(dst_zipcode)\n",
    "\n",
    "vertices=vertices[vertices['usaanr'].isin(uniq_usaanr)]\n",
    "\n",
    "uniq_usaanr.sort()\n",
    "# uniq_zipcode.sort()\n",
    "usaanr_map = {id:idx for idx, id in enumerate(uniq_usaanr)}\n",
    "# zipcode_map = {id:idx for idx, id in enumerate(uniq_zipcode)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pers_Edges_bi['src'] = list(map(usaanr_map.get, Pers_Edges_bi['src']))\n",
    "Pers_Edges_bi['dst'] = list(map(usaanr_map.get, Pers_Edges_bi['dst']))\n",
    "print(len(usaanr_map))\n",
    "print(Pers_Edges_bi['src'].unique().max())\n",
    "print(Pers_Edges_bi['dst'].unique().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Busi_Edges_bi['src'] = list(map(usaanr_map.get, Busi_Edges_bi['src']))\n",
    "Busi_Edges_bi['dst'] = list(map(usaanr_map.get, Busi_Edges_bi['dst']))\n",
    "print(len(usaanr_map))\n",
    "print(Busi_Edges_bi['src'].unique().max())\n",
    "print(Busi_Edges_bi['dst'].unique().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zipcode_Edges_v1=zipcode_Edges_bi[zipcode_Edges_bi['rel']==\"Located_In\"]\n",
    "# zipcode_Edges_v1['src'] = list(map(usaanr_map.get, zipcode_Edges_v1['src']))\n",
    "# zipcode_Edges_v1['dst'] = list(map(zipcode_map.get, zipcode_Edges_v1['dst']))\n",
    "\n",
    "# zipcode_Edges_v2=zipcode_Edges_bi[zipcode_Edges_bi['rel']==\"Location_of\"]\n",
    "# zipcode_Edges_v2['src'] = list(map(zipcode_map.get, zipcode_Edges_v2['src']))\n",
    "# zipcode_Edges_v2['dst'] = list(map(usaanr_map.get, zipcode_Edges_v2['dst']))\n",
    "\n",
    "# zipcode_Edges_bi=zipcode_Edges_v1.append(zipcode_Edges_v2,ignore_index=True)\n",
    "# print(len(usaanr_map))\n",
    "# print(zipcode_Edges_bi['src'].unique().max())\n",
    "# print(len(zipcode_map))\n",
    "# print(zipcode_Edges_bi['dst'].unique().max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Re-label Edges types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relation_encoder=LabelEncoder()\n",
    "# relation_encoder.fit(pd.concat([Pers_Edges['rel'],Busi_Edges['rel'],zipcode_Edges['rel']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pers_rel_nums = {\"rel\":     {\"Parent\": 0, \"Child\": 1, \"Spouse\": 2,\"Ex-Spouse\": 3,\"Brother_Sister\": 4, \"Step-Parent\": 5, \"Step-Child\": 6, \"Pers_rel_Other\": 7}}\n",
    "busi_rel_nums = {\"rel\":     {\"SPONSOR\": 8, \"SPONSEE\": 9, \"AUTO_RELATED\": 10,\"Busi_rel_Other\":11}}\n",
    "# zipcode_rel_nums = {\"rel\":     {\"Located_In\": 12,\"Location_of\": 13}}\n",
    "\n",
    "# start=time.time()\n",
    "Pers_Edges_bi.replace(pers_rel_nums, inplace=True)\n",
    "Busi_Edges_bi.replace(busi_rel_nums, inplace=True)\n",
    "# zipcode_Edges_bi.replace(zipcode_rel_nums, inplace=True)\n",
    "\n",
    "# end=time.time()\n",
    "# print(\"running time is {:0.4f}\".format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Pers_Edges_bi['rel'].unique())\n",
    "print(Busi_Edges_bi['rel'].unique())\n",
    "# print(zipcode_Edges_bi['rel'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict=dict()\n",
    "\n",
    "### Personal Relationship ###\n",
    "rel=np.array(Pers_Edges_bi['rel'])\n",
    "src_pers=np.array(Pers_Edges_bi['src'])\n",
    "dst_pers=np.array(Pers_Edges_bi['dst'])\n",
    "\n",
    "data_dict.update({('usaanr', 'Parent', 'usaanr')  :         (src_pers[rel==0], dst_pers[rel==0])})\n",
    "data_dict.update({('usaanr', 'Child', 'usaanr')   :         (src_pers[rel==1], dst_pers[rel==1])})\n",
    "data_dict.update({('usaanr', 'Spouse', 'usaanr')  :         (src_pers[rel==2], dst_pers[rel==2])})\n",
    "data_dict.update({('usaanr', 'Ex-Spouse', 'usaanr')  :      (src_pers[rel==3], dst_pers[rel==3])})\n",
    "data_dict.update({('usaanr', 'Brother_Sister', 'usaanr')  : (src_pers[rel==4], dst_pers[rel==4])})\n",
    "data_dict.update({('usaanr', 'Step-Parent', 'usaanr')     : (src_pers[rel==5], dst_pers[rel==5])})\n",
    "data_dict.update({('usaanr', 'Step-Child', 'usaanr')    :   (src_pers[rel==6], dst_pers[rel==6])})\n",
    "data_dict.update({('usaanr', 'Pers_rel_Other', 'usaanr')  : (src_pers[rel==7], dst_pers[rel==7])})\n",
    "\n",
    "### Business Relationship ###\n",
    "rel=np.array(Busi_Edges_bi['rel'])\n",
    "src_busi=np.array(Busi_Edges_bi['src'])\n",
    "dst_busi=np.array(Busi_Edges_bi['dst'])\n",
    "\n",
    "data_dict.update({('usaanr', 'SPONSOR', 'usaanr')    :      (src_busi[rel==8], dst_busi[rel==8])})\n",
    "data_dict.update({('usaanr', 'SPONSEE', 'usaanr')    :      (src_busi[rel==9], dst_busi[rel==9])})\n",
    "data_dict.update({('usaanr', 'AUTO_RELATED', 'usaanr')   :  (src_busi[rel==10], dst_busi[rel==10])})\n",
    "data_dict.update({('usaanr', 'Busi_rel_Other', 'usaanr') :  (src_busi[rel==11], dst_busi[rel==11])})\n",
    "\n",
    "# ### zipcode relationship ###\n",
    "# zipcode_Edges_v1=zipcode_Edges_bi[zipcode_Edges_bi['rel']==12]\n",
    "# src_v1=np.array(zipcode_Edges_v1['src'])\n",
    "# dst_v1=np.array(zipcode_Edges_v1['dst'])\n",
    "\n",
    "# zipcode_Edges_v2=zipcode_Edges_bi[zipcode_Edges_bi['rel']==13]\n",
    "# src_v2=np.array(zipcode_Edges_v2['src'])\n",
    "# dst_v2=np.array(zipcode_Edges_v2['dst'])\n",
    "\n",
    "# data_dict.update({('usaanr', 'Located_In', 'zipcode')  :   (src_v1, dst_v1)})\n",
    "# data_dict.update({('zipcode', 'Location_of','usaanr')  :   (src_v2, dst_v2)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start=time.time()\n",
    "g = dgl.heterograph(data_dict)\n",
    "end=time.time()\n",
    "print(\"running time is {:0.4f}\".format(end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assign Edges Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.edges['Parent'].data[\"etype\"]=torch.zeros(g.num_edges(\"Parent\"))\n",
    "g.edges['Child'].data[\"etype\"]=torch.ones(g.num_edges(\"Child\"))\n",
    "g.edges['Spouse'].data[\"etype\"]=torch.ones(g.num_edges(\"Spouse\"))*2\n",
    "g.edges['Ex-Spouse'].data[\"etype\"]=torch.ones(g.num_edges(\"Ex-Spouse\"))*3\n",
    "g.edges['Brother_Sister'].data[\"etype\"]=torch.ones(g.num_edges(\"Brother_Sister\"))*4\n",
    "g.edges['Step-Parent'].data[\"etype\"]=torch.ones(g.num_edges(\"Step-Parent\"))*5\n",
    "g.edges['Step-Child'].data[\"etype\"]=torch.ones(g.num_edges(\"Step-Child\"))*6\n",
    "g.edges['Pers_rel_Other'].data[\"etype\"]=torch.ones(g.num_edges(\"Pers_rel_Other\"))*7\n",
    "g.edges['SPONSOR'].data[\"etype\"]=torch.ones(g.num_edges(\"SPONSOR\"))*8\n",
    "g.edges['SPONSEE'].data[\"etype\"]=torch.ones(g.num_edges(\"SPONSEE\"))*9\n",
    "g.edges['AUTO_RELATED'].data[\"etype\"]=torch.ones(g.num_edges(\"AUTO_RELATED\"))*10\n",
    "g.edges['Busi_rel_Other'].data[\"etype\"]=torch.ones(g.num_edges(\"Busi_rel_Other\"))*11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_show(G):\n",
    "    print('*'*50)\n",
    "    print(\"Node_types: \" , G.ntypes)\n",
    "    print(\"Edge_types: \" , G.etypes)\n",
    "    print('*'*50)\n",
    "    print(\"Canonical Etypes of Graph is:\\n\")\n",
    "    for srctype, etype, dsttype in G.canonical_etypes:\n",
    "        print(\"{:<20}{:<20}{:<20}\".format(srctype, etype, dsttype))\n",
    "    print('*'*50)\n",
    "    Total_ntype_num=0\n",
    "    for i in G.ntypes:\n",
    "        print(f\"number of ntype={i:<20}  {G.number_of_nodes(i):<15,}\")\n",
    "        Total_ntype_num+=G.number_of_nodes(i)\n",
    "    print('*'*50)\n",
    "    print(\"Total number of nodes is {:,}\".format(Total_ntype_num))\n",
    "    print('*'*50)\n",
    "    Total_edge_num=0\n",
    "    for j in G.etypes:\n",
    "        print(f\"number of etype={j:<20}  {G.number_of_edges(j):<15,}\")\n",
    "        Total_edge_num+=G.number_of_edges(j)\n",
    "    print('*'*50)\n",
    "    print(\"Total number of edges is {:,}\".format(Total_edge_num))\n",
    "    print('*'*50)\n",
    "    for nty in G.ntypes:\n",
    "        if G.nodes[nty].data!={}:\n",
    "            print('*'*50)\n",
    "            print(f\"The attributes for the node type={nty}\")\n",
    "            print('*'*50)\n",
    "            for key, scheme in G.node_attr_schemes(ntype=nty).items():\n",
    "                print(\"{:<40}{}\".format(key,G.nodes[nty].data[key].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_show(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding features/attributes to the nodes of USAA Members\n",
    "The CAP dataset has some features for USAA Member.\n",
    "\n",
    "* usaayr:  USAA Number Issue Year\n",
    "* AGE / AGE BUCKET\n",
    "* ORIGEL :  Original Eligibility\n",
    "* ELIG2 : Current Eligibility\n",
    "* cmpyelig: Company Eligibility\n",
    "* Segment: Alpha ~ Juliet\n",
    "* SEX\n",
    "* MARST : Marriage Status\n",
    "* MILST : Military Status\n",
    "* MLIST_OrigStat: Original Military Status\n",
    "* ENLPAYGD: Military Pay Grade\n",
    "* BRANCH: Military BRANCH of Service\n",
    "* ACTCORP : Corporate Active Status\n",
    "* STATE\n",
    "\n",
    "We use label encoding for all categorial variables. <br>\n",
    "In addition, there is a node data \"type\" that indicates the node type of usaa member, zipcode in the heterogenous graph. <br>\n",
    "The nodes of zipcode don't have the same features as the node of member. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertices.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert g.number_of_nodes(ntype=\"usaanr\")==vertices.shape[0], \"the shape of feature data is not equal to the number of USAA member\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sort the vertices dataframe based on the order of nodes in graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if g.number_of_nodes(ntype=\"usaanr\")==vertices.shape[0]:\n",
    "    vertices_v2=vertices\n",
    "else:\n",
    "    vertices_v2=vertices[vertices['usaanr'].isin(all_usaanr)]\n",
    "print(\"{:<20} {:<15,}\".format(\"size of original vertices\",vertices.shape[0]))\n",
    "print(\"{:<20} {:<15,}\".format(\"size of updated vertices\",vertices_v2.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertices_v2['usaanr'] = list(map(usaanr_map.get, vertices_v2['usaanr']))\n",
    "vertices_v2.sort_values(by=[\"usaanr\"],inplace=True)\n",
    "\n",
    "to_pickle(data_dir,vertices_v2,'vertices_reindex_pickle')\n",
    "vertices=read_pickle(data_dir,'vertices_reindex_pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create node type feature\n",
    "The reason I create node type feature is to embedding the node USAA Member , product and zipcode separately. Unlike the node of usaanr,  the nodes of zipcode don't have other features except for node type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Nodes type feature\n",
    "# g.nodes['usaanr'].data['type'] = torch.zeros(size=[g.number_of_nodes(ntype='usaanr'),1]).long()\n",
    "# g.nodes['zipcode'].data['type'] = torch.ones(size=[g.number_of_nodes(ntype='zipcode'),1]).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Bin the numerical variable\n",
    "def Bin_Numerical(args,b):\n",
    "#     if args==0:\n",
    "#         return str(0)\n",
    "    for i in range(len(b)-1):\n",
    "        if args>=b[i] and args<=b[i+1]:\n",
    "#             return str((int(b[i]), int(b[i+1])))\n",
    "            return int(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat=['usaanr','usaayr','AGE_BAND','ORIGEL', 'ELIG2', 'cmpyelig','SEX', 'MARST','BRANCH','ENLPAYGD','MILST',\n",
    "       'MLIST_OrigStat','ACTCORP', 'STATE', 'Segment']\n",
    "vertices_feat=vertices.loc[:,feat]\n",
    "\n",
    "for col in vertices_feat:\n",
    "    if col !='usaanr':\n",
    "        vertices_feat[col]=vertices_feat[col].astype('str')\n",
    "    \n",
    "class_le=LabelEncoder()\n",
    "\n",
    "for col in vertices_feat.columns:\n",
    "    if vertices_feat[col].dtype==\"object\" and col !='usaanr':\n",
    "        vertices_feat[col]=vertices_feat[col].astype('str')\n",
    "        vertices_feat[col]=class_le.fit_transform(vertices_feat[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in vertices_feat.columns:\n",
    "    g.nodes['usaanr'].data[col]= torch.tensor( np.expand_dims(np.array(vertices_feat[col]), 1) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create label\n",
    "There are two kinds of labels.\n",
    "\n",
    "* multi-classification label:     Auto, Home and Rental\n",
    "* binary-classification label:    Auto or Non-Auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Flatten_Product_pickle.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_df=Flatten_Product_pickle[Flatten_Product_pickle[\"USAA_Number\"].isin(all_usaanr)]\n",
    "prod_df=prod_df.loc[:,[\"USAA_Number\",\"P_AUTO\",\"Prod_Trace\"]].rename(columns={\"USAA_Number\":\"usaanr\"})\n",
    "prod_df['usaanr'] = list(map(usaanr_map.get, prod_df['usaanr']))\n",
    "prod_df['usaanr'].unique().max()  ### usaanr is not indexed consecutively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertices=read_pickle(data_dir,'vertices_reindex_pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertices.usaanr.dtypes, prod_df.usaanr.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertices[\"usaanr\"]=vertices[\"usaanr\"].astype(str)\n",
    "prod_df[\"usaanr\"]=prod_df[\"usaanr\"].astype(str)\n",
    "vertices_v1=vertices.merge(prod_df, on='usaanr', how=\"left\").loc[:,['usaanr','P_AUTO','Prod_Trace']]\n",
    "vertices_v1[\"Prod_Trace\"]=vertices_v1[\"Prod_Trace\"].astype(str)\n",
    "vertices_v1[\"P_AUTO\"]=vertices_v1[\"P_AUTO\"].astype(str)                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(vertices_v1[\"P_AUTO\"],return_counts=True), np.unique(vertices_v1[\"Prod_Trace\"],return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 : customers own Auto,  0: customers owns some products but not Auto, 2: customers didn't own any product\n",
    "rep_nums = {\"P_AUTO\":     {\"1.0\": \"1\", \"0.0\": \"0\", \"nan\": \"2\"}}  \n",
    "vertices_v1.replace(rep_nums, inplace=True)\n",
    "\n",
    "rep_nums = {\"Prod_Trace\":     {\"nan\": \"000\"}}  \n",
    "vertices_v1.replace(rep_nums, inplace=True)\n",
    "\n",
    "# vertices_v1.sort_values(by=[\"usaanr\"],inplace=True)  ### will the sort change the order of node in graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempt1=pd.DataFrame(vertices_v1[\"Prod_Trace\"].value_counts()).reset_index().rename(columns={'index':'product','Prod_Trace':'count'})\n",
    "tempt2=pd.DataFrame(vertices_v1[\"Prod_Trace\"].value_counts(normalize=True)).reset_index().rename(columns={'index':'product','Prod_Trace':'percentage'})\n",
    "tempt3=tempt1.merge(tempt2, on=\"product\", how=\"inner\")\n",
    "tempt3[\"product_type\"]=[\"No Product\", \"Auto+Home\",\"Auto Only\",\"Auto+Rental\",\"Rental Only\",\"Home Only\",\"Auto+Home+Rental\",\"Home+Rental\"]\n",
    "tempt3=tempt3[[\"product\",\"product_type\",\"count\",\"percentage\"]]\n",
    "tempt3.style.format({'count':'{:,}','percentage':'{:.2%}'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### due to some rare category,  merge \"Auto + Home + Rental\" to \"Auto + Home\", merge \"Home + Rental\" to \"Home Only\"\n",
    "rep_nums = {\"Prod_Trace\":     {\"111\": \"110\",  \"011\":\"010\"}}  \n",
    "vertices_v1.replace(rep_nums, inplace=True)\n",
    "tempt1=pd.DataFrame(vertices_v1[\"Prod_Trace\"].value_counts()).reset_index().rename(columns={'index':'product','Prod_Trace':'count'})\n",
    "tempt2=pd.DataFrame(vertices_v1[\"Prod_Trace\"].value_counts(normalize=True)).reset_index().rename(columns={'index':'product','Prod_Trace':'percentage'})\n",
    "tempt1.merge(tempt2, on=\"product\", how=\"inner\").style.format({'count':'{:,}','percentage':'{:.2%}'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Prod_Trace = vertices_v1[\"Prod_Trace\"].unique()\n",
    "Prod_Trace.sort()\n",
    "product_map = {id:idx for idx, id in enumerate(Prod_Trace)}\n",
    "product_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertices_v1['Prod_Trace_map'] = list(map(product_map.get, vertices_v1['Prod_Trace']))\n",
    "multi_label=torch.tensor(np.expand_dims(np.array(vertices_v1.loc[:,\"Prod_Trace_map\"]),1))\n",
    "torch.unique(multi_label.squeeze(),return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempt1=pd.DataFrame(vertices_v1[\"P_AUTO\"].value_counts()).reset_index().rename(columns={'index':'product','P_AUTO':'count'})\n",
    "tempt2=pd.DataFrame(vertices_v1[\"P_AUTO\"].value_counts(normalize=True)).reset_index().rename(columns={'index':'product','P_AUTO':'percentage'})\n",
    "tempt3=tempt1.merge(tempt2, on=\"product\", how=\"inner\")\n",
    "tempt3[\"product_type\"]=[\"No Product\", \"Auto only\",\"Other Products but not Auto\"]\n",
    "tempt3=tempt3[[\"product\",\"product_type\",\"count\",\"percentage\"]]\n",
    "tempt3.style.format({'count':'{:,}','percentage':'{:.2%}'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### combine the category of 0 and 2 because the category of 0 is too rare\n",
    "rep_nums = {\"P_AUTO\":     {\"2\": \"0\"}}  \n",
    "vertices_v1.replace(rep_nums, inplace=True)\n",
    "tempt1=pd.DataFrame(vertices_v1[\"P_AUTO\"].value_counts()).reset_index().rename(columns={'index':'product','P_AUTO':'count'})\n",
    "tempt2=pd.DataFrame(vertices_v1[\"P_AUTO\"].value_counts(normalize=True)).reset_index().rename(columns={'index':'product','P_AUTO':'percentage'})\n",
    "tempt3=tempt1.merge(tempt2, on=\"product\", how=\"inner\")\n",
    "tempt3[\"product_type\"]=[\"No Auto\",\"Auto Only\"]\n",
    "tempt3=tempt3[[\"product\",\"product_type\",\"count\",\"percentage\"]]\n",
    "tempt3.style.format({'count':'{:,}','percentage':'{:.2%}'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertices_v1[\"P_AUTO\"]=vertices_v1[\"P_AUTO\"].astype(int)\n",
    "binary_label=torch.tensor(np.expand_dims(np.array(vertices_v1.loc[:,\"P_AUTO\"]),1))\n",
    "torch.unique(binary_label.squeeze(),return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create training(80%), validation(10%) and test(10%) mask based on each category of product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_func(train_idx,all_idx):\n",
    "    train_idx=pd.DataFrame({\"idx\":train_idx})\n",
    "    all_idx=pd.DataFrame({\"idx\":all_idx})\n",
    "    all_index = all_idx.set_index(['idx']).index\n",
    "    train_index = train_idx.set_index(['idx']).index\n",
    "    mask = all_index.isin(train_index)\n",
    "    return mask\n",
    "\n",
    "def mask_creation(G,node_labels):\n",
    "    \n",
    "    train_idx=[]\n",
    "    val_idx=[]\n",
    "    test_idx=[]\n",
    "    \n",
    "    LABEL=node_labels.numpy().squeeze()\n",
    "    IDX=np.arange(LABEL.shape[0])\n",
    "    prod_list=np.unique(LABEL).tolist()\n",
    "    \n",
    "    for i in tqdm(range(len(prod_list)),position=0,leave=True):\n",
    "        _idx=IDX[LABEL==prod_list[i]]\n",
    "        np.random.seed(101)\n",
    "        np.random.shuffle(_idx)\n",
    "        test_idx.extend(_idx[:len(_idx)//10])\n",
    "        val_idx.extend(_idx[len(_idx)//10 : len(_idx)//5])\n",
    "        train_idx.extend(_idx[len(_idx)//5:])\n",
    "\n",
    "    all_idx=np.arange(G.number_of_nodes(ntype=\"usaanr\"))\n",
    "\n",
    "    test_idx=np.array(test_idx)\n",
    "    val_idx=np.array(val_idx)\n",
    "    train_idx=np.array(train_idx)\n",
    "\n",
    "    train_mask=mask_func(train_idx,all_idx)\n",
    "    val_mask=mask_func(val_idx,all_idx)\n",
    "    test_mask=mask_func(test_idx,all_idx)\n",
    "\n",
    "    train_mask=torch.tensor(train_mask,dtype=bool)\n",
    "    val_mask=torch.tensor(val_mask,dtype=bool)\n",
    "    test_mask=torch.tensor(test_mask,dtype=bool)\n",
    "    \n",
    "    return train_mask, val_mask, test_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mask_multi_label, val_mask_multi_label, test_mask_multi_label=mask_creation(g,multi_label)\n",
    "print(\"{:<30}{:<10,}\".format(\"dimension of training mask\", torch.sum(train_mask_multi_label).item()))\n",
    "print(\"{:<30}{:<10,}\".format(\"dimension of val mask\", torch.sum(val_mask_multi_label).item()))\n",
    "print(\"{:<30}{:<10,}\".format(\"dimension of test mask\", torch.sum(test_mask_multi_label).item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mask_binary_label, val_mask_binary_label, test_mask_binary_label=mask_creation(g,binary_label)\n",
    "print(\"{:<30}{:<10,}\".format(\"dimension of training mask\", torch.sum(train_mask_binary_label).item()))\n",
    "print(\"{:<30}{:<10,}\".format(\"dimension of val mask\", torch.sum(val_mask_binary_label).item()))\n",
    "print(\"{:<30}{:<10,}\".format(\"dimension of test mask\", torch.sum(test_mask_binary_label).item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempt1=pd.DataFrame(vertices_v1.iloc[np.where(train_mask_binary_label.numpy())]['P_AUTO'].value_counts(dropna=False)).reset_index().\\\n",
    "rename(columns={'index':'Auto or Not','P_AUTO':'count'}).replace({'Auto or Not':     {0: \"No\",1:\"Yes\"}})\n",
    "tempt2=pd.DataFrame(vertices_v1.iloc[np.where(train_mask_binary_label.numpy())]['P_AUTO'].value_counts(normalize=True,dropna=False)).reset_index().\\\n",
    "rename(columns={'index':'Auto or Not','P_AUTO':'percentage'}).replace({'Auto or Not':     {0: \"No\",1:\"Yes\"}})\n",
    "tempt3=tempt1.merge(tempt2, on=\"Auto or Not\", how=\"inner\")\n",
    "tempt3.style.format({'count':'{:,}','percentage':'{:.2%}'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempt1=pd.DataFrame(vertices_v1.iloc[np.where(train_mask_multi_label.numpy())]['Prod_Trace'].value_counts(dropna=False)).reset_index().\\\n",
    "rename(columns={'index':'Product_Type','Prod_Trace':'count'}).replace({'Product_Type':     {'000': \"No Product\", '001': \"Rental Only\", '010': \"Home Only\", '100': \"Auto Only\", '101': \"Auto + Rental\", '110': \"Auto + Home\"}})\n",
    "tempt2=pd.DataFrame(vertices_v1.iloc[np.where(train_mask_multi_label.numpy())]['Prod_Trace'].value_counts(normalize=True,dropna=False)).reset_index().\\\n",
    "rename(columns={'index':'Product_Type','Prod_Trace':'percentage'}).replace({'Product_Type':     {'000': \"No Product\", '001': \"Rental Only\", '010': \"Home Only\", '100': \"Auto Only\", '101': \"Auto + Rental\", '110': \"Auto + Home\"}})\n",
    "tempt3=tempt1.merge(tempt2, on=\"Product_Type\", how=\"inner\")\n",
    "tempt3.style.format({'count':'{:,}','percentage':'{:.2%}'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempt1=pd.DataFrame(vertices_v1.iloc[np.where(val_mask_binary_label.numpy())]['P_AUTO'].value_counts(dropna=False)).reset_index().\\\n",
    "rename(columns={'index':'Auto or Not','P_AUTO':'count'}).replace({'Auto or Not':     {0: \"No\",1:\"Yes\"}})\n",
    "tempt2=pd.DataFrame(vertices_v1.iloc[np.where(val_mask_binary_label.numpy())]['P_AUTO'].value_counts(normalize=True,dropna=False)).reset_index().\\\n",
    "rename(columns={'index':'Auto or Not','P_AUTO':'percentage'}).replace({'Auto or Not':     {0: \"No\",1:\"Yes\"}})\n",
    "tempt3=tempt1.merge(tempt2, on=\"Auto or Not\", how=\"inner\")\n",
    "tempt3.style.format({'count':'{:,}','percentage':'{:.2%}'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempt1=pd.DataFrame(vertices_v1.iloc[np.where(val_mask_multi_label.numpy())]['Prod_Trace'].value_counts(dropna=False)).reset_index().\\\n",
    "rename(columns={'index':'Product_Type','Prod_Trace':'count'}).replace({'Product_Type':     {'000': \"No Product\", '001': \"Rental Only\", '010': \"Home Only\", '100': \"Auto Only\", '101': \"Auto + Rental\", '110': \"Auto + Home\"}})\n",
    "tempt2=pd.DataFrame(vertices_v1.iloc[np.where(val_mask_multi_label.numpy())]['Prod_Trace'].value_counts(normalize=True,dropna=False)).reset_index().\\\n",
    "rename(columns={'index':'Product_Type','Prod_Trace':'percentage'}).replace({'Product_Type':     {'000': \"No Product\", '001': \"Rental Only\", '010': \"Home Only\", '100': \"Auto Only\", '101': \"Auto + Rental\", '110': \"Auto + Home\"}})\n",
    "tempt3=tempt1.merge(tempt2, on=\"Product_Type\", how=\"inner\")\n",
    "tempt3.style.format({'count':'{:,}','percentage':'{:.2%}'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempt1=pd.DataFrame(vertices_v1.iloc[np.where(test_mask_binary_label.numpy())]['P_AUTO'].value_counts(dropna=False)).reset_index().\\\n",
    "rename(columns={'index':'Auto or Not','P_AUTO':'count'}).replace({'Auto or Not':     {0: \"No\",1:\"Yes\"}})\n",
    "tempt2=pd.DataFrame(vertices_v1.iloc[np.where(test_mask_binary_label.numpy())]['P_AUTO'].value_counts(normalize=True,dropna=False)).reset_index().\\\n",
    "rename(columns={'index':'Auto or Not','P_AUTO':'percentage'}).replace({'Auto or Not':     {0: \"No\",1:\"Yes\"}})\n",
    "tempt3=tempt1.merge(tempt2, on=\"Auto or Not\", how=\"inner\")\n",
    "tempt3.style.format({'count':'{:,}','percentage':'{:.2%}'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempt1=pd.DataFrame(vertices_v1.iloc[np.where(test_mask_multi_label.numpy())]['Prod_Trace'].value_counts(dropna=False)).reset_index().\\\n",
    "rename(columns={'index':'Product_Type','Prod_Trace':'count'}).replace({'Product_Type':     {'000': \"No Product\", '001': \"Rental Only\", '010': \"Home Only\", '100': \"Auto Only\", '101': \"Auto + Rental\", '110': \"Auto + Home\"}})\n",
    "tempt2=pd.DataFrame(vertices_v1.iloc[np.where(test_mask_multi_label.numpy())]['Prod_Trace'].value_counts(normalize=True,dropna=False)).reset_index().\\\n",
    "rename(columns={'index':'Product_Type','Prod_Trace':'percentage'}).replace({'Product_Type':     {'000': \"No Product\", '001': \"Rental Only\", '010': \"Home Only\", '100': \"Auto Only\", '101': \"Auto + Rental\", '110': \"Auto + Home\"}})\n",
    "tempt3=tempt1.merge(tempt2, on=\"Product_Type\", how=\"inner\")\n",
    "tempt3.style.format({'count':'{:,}','percentage':'{:.2%}'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir=\"/workspace/cjiang/eagle_project/CAP_graph/BGNN/\"\n",
    "start=time.time()\n",
    "with open(os.path.join(data_dir,\"CAP_Graph_v1\"),\"wb\") as f:\n",
    "    pickle.dump((g,multi_label,binary_label,\\\n",
    "                 train_mask_multi_label,  val_mask_multi_label,  test_mask_multi_label,\\\n",
    "                 train_mask_binary_label, val_mask_binary_label, test_mask_binary_label),f)\n",
    "end=time.time()\n",
    "print(\"It took {:0.4f} seconds to save graph database\".format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start=time.time()\n",
    "with open(os.path.join(data_dir,\"CAP_Graph_v1\"),\"rb\") as f:\n",
    "    G,multi_label,binary_label,\\\n",
    "    train_mask_multi_label,  val_mask_multi_label,  test_mask_multi_label,\\\n",
    "    train_mask_binary_label, val_mask_binary_label, test_mask_binary_label=pickle.load(f)\n",
    "end=time.time()\n",
    "print(\"It took {:0.4f} seconds to load graph database\".format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_show(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### count the number of nodes whose in-degree ==0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C=torch.empty(G.num_nodes('usaanr'))\n",
    "# for etype in G.etypes:\n",
    "#     C+=G.in_degrees(etype=etype)\n",
    "# print(\"{:<35}{:<10,} \".format(\"The number of zero in-degree nodes is \",torch.sum(C==0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find metapaths in the generated heterogeneous graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.metagraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def get_all_possible_metapaths(g, K):\n",
    "    possible_metapaths = []\n",
    "    metagraph = g.metagraph\n",
    "    # Iterate over all possible K-length sequences of all canonical edge types\n",
    "    for metapath in itertools.product(g.canonical_etypes, repeat=K):\n",
    "        # Check if the sequence indeed connects and forms a metapath.\n",
    "        # If the destination node type of an edge type is not the same as the source\n",
    "        # node type of the next edge type, then the sequence is not a valid metapath.\n",
    "        is_valid = True\n",
    "        previous_ntype = None\n",
    "        for srctype, etype, dsttype in metapath:\n",
    "            if previous_ntype is not None and srctype != previous_ntype:\n",
    "                is_valid = False\n",
    "                break\n",
    "            previous_ntype = dsttype\n",
    "\n",
    "        if is_valid:\n",
    "            possible_metapaths.append(metapath)\n",
    "    filtered_metapaths = []\n",
    "    for metapath in possible_metapaths:\n",
    "        result_g = dgl.metapath_reachable_graph(g, metapath)\n",
    "        if result_g.number_of_edges() > 0:\n",
    "            filtered_metapaths.append(metapath)\n",
    "    return filtered_metapaths\n",
    "\n",
    "def pretty_print_metapath(metapath):\n",
    "    # This function just pretty-prints the metapath\n",
    "    item_list = sum([['(' + etype[1] + ')', etype[2]] for etype in metapath], [])\n",
    "    item_list.insert(0, metapath[0][0])\n",
    "    return ' -- '.join(item_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for K in range(1, 3):\n",
    "    print('### Length', K, 'metapaths ###')\n",
    "    possible_metapaths = get_all_possible_metapaths(G, K)\n",
    "    # print the metapaths\n",
    "    for metapath in possible_metapaths:\n",
    "        print(pretty_print_metapath(metapath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total number of metapath(length 2) is {}\".format(len(possible_metapaths)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
