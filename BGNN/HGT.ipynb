{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version is 1.6.0\n",
      "DGL version is 0.8a210818\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import itertools\n",
    "import os\n",
    "import numpy as np\n",
    "from numpy import save,load,savetxt,loadtxt,savez_compressed\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score, f1_score,average_precision_score\n",
    "from sklearn.metrics import precision_recall_fscore_support \n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import auc as auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.preprocessing import LabelEncoder, label_binarize\n",
    "\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "import time\n",
    "from tqdm import tqdm, tqdm_notebook,tnrange\n",
    "tqdm.pandas(position=0, leave=True)\n",
    "import math \n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import dgl\n",
    "from dgl import edge_subgraph\n",
    "from dgl.nn.functional import edge_softmax\n",
    "\n",
    "import dgl.nn as dglnn\n",
    "import dgl.function as fn\n",
    "\n",
    "import functools\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize']=(5.0,4.0)\n",
    "plt.rcParams['image.interpolation']='nearest'\n",
    "plt.rcParams['image.cmap']='gray'\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import utils\n",
    "\n",
    "print(\"torch version is {}\".format(th.__version__))\n",
    "print(\"DGL version is {}\".format(dgl.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    th.manual_seed(seed)\n",
    "    th.cuda.manual_seed_all(seed)\n",
    "    th.backends.cudnn.deterministic = True\n",
    "    th.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    \n",
    "seed_everything(101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load graph data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 13.0979 seconds to load graph\n"
     ]
    }
   ],
   "source": [
    "KG_dir=\"/workspace/cjiang/eagle_project/CAP_graph/BGNN/\"\n",
    "\n",
    "start=time.time()\n",
    "with open(os.path.join(KG_dir,'CAP_Graph_v1'), 'rb') as f:\n",
    "    G,multi_label,binary_label,\\\n",
    "    train_mask_multi_label,  val_mask_multi_label,  test_mask_multi_label,\\\n",
    "    train_mask_binary_label, val_mask_binary_label, test_mask_binary_label= pickle.load(f)\n",
    "end=time.time()\n",
    "print(\"It took {:0.4f} seconds to load graph\".format(end-start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The features associated with USAA Member are\n",
      " \n",
      "usaayr\n",
      "AGE_BAND\n",
      "ORIGEL\n",
      "ELIG2\n",
      "SEX\n",
      "MARST\n",
      "BRANCH\n",
      "ENLPAYGD\n",
      "MILST\n",
      "MLIST_OrigStat\n",
      "STATE\n"
     ]
    }
   ],
   "source": [
    "usaanr_feat=[]\n",
    "for key, scheme in G.node_attr_schemes(ntype=\"usaanr\").items():\n",
    "    usaanr_feat.append(key)\n",
    "\n",
    "usaanr_feat=[x for x in usaanr_feat if x not in ['usaanr','cmpyelig','ACTCORP','Segment']]\n",
    "\n",
    "print()\n",
    "print(\"The features associated with USAA Member are\\n \")\n",
    "for i in usaanr_feat:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## USAA Members Features Embedding\n",
    "class USAANR_Embedding(nn.Module):\n",
    "    def __init__(self,G,feature_size):\n",
    "        super(USAANR_Embedding,self).__init__()\n",
    "        self.G=G.to(device)\n",
    "        self.feature_size=feature_size\n",
    "        ## Embedding matrices for features of nodes.\n",
    "        self.emb = nn.ModuleDict()\n",
    "        \n",
    "        for i,col in enumerate(usaanr_feat):\n",
    "            self.emb[col]=nn.Embedding(G.nodes['usaanr'].data[col].max().item()+1, feature_size)\n",
    "    \n",
    "    def forward(self,nid):\n",
    "        nid=nid.to(device)\n",
    "        extra_repr=[]\n",
    "        for i,col in enumerate(usaanr_feat):\n",
    "            ndata=self.G.nodes['usaanr'].data[col]\n",
    "            extra_repr.append(self.emb[col](ndata[nid]).squeeze(1))\n",
    "        return th.stack(extra_repr, 0).sum(0)\n",
    "\n",
    "\n",
    "class HGTLayer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_dim,\n",
    "                 out_dim,\n",
    "                 node_dict,\n",
    "                 edge_dict,\n",
    "                 n_heads,\n",
    "                 dropout = 0.2,\n",
    "                 use_norm = False):\n",
    "        super(HGTLayer, self).__init__()\n",
    "\n",
    "        self.in_dim        = in_dim\n",
    "        self.out_dim       = out_dim\n",
    "        self.node_dict     = node_dict\n",
    "        self.edge_dict     = edge_dict\n",
    "        self.num_types     = len(node_dict)\n",
    "        self.num_relations = len(edge_dict)\n",
    "        self.total_rel     = self.num_types * self.num_relations * self.num_types\n",
    "        self.n_heads       = n_heads\n",
    "        self.d_k           = out_dim // n_heads\n",
    "        self.sqrt_dk       = math.sqrt(self.d_k)\n",
    "        self.att           = None\n",
    "\n",
    "        self.k_linears   = nn.ModuleList()\n",
    "        self.q_linears   = nn.ModuleList()\n",
    "        self.v_linears   = nn.ModuleList()\n",
    "        self.a_linears   = nn.ModuleList()\n",
    "        self.norms       = nn.ModuleList()\n",
    "        self.use_norm    = use_norm\n",
    "\n",
    "        for t in range(self.num_types):\n",
    "            self.k_linears.append(nn.Linear(in_dim,   out_dim))\n",
    "            self.q_linears.append(nn.Linear(in_dim,   out_dim))\n",
    "            self.v_linears.append(nn.Linear(in_dim,   out_dim))\n",
    "            self.a_linears.append(nn.Linear(out_dim,  out_dim))\n",
    "            if use_norm:\n",
    "                self.norms.append(nn.LayerNorm(out_dim))\n",
    "\n",
    "        self.relation_pri   = nn.Parameter(th.ones(self.num_relations, self.n_heads))\n",
    "        self.relation_att   = nn.Parameter(th.Tensor(self.num_relations, n_heads, self.d_k, self.d_k))\n",
    "        self.relation_msg   = nn.Parameter(th.Tensor(self.num_relations, n_heads, self.d_k, self.d_k))\n",
    "        self.skip           = nn.Parameter(th.ones(self.num_types))\n",
    "        self.drop           = nn.Dropout(dropout)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.relation_att)\n",
    "        nn.init.xavier_uniform_(self.relation_msg)\n",
    "\n",
    "    def forward(self, sg, h):\n",
    "        with sg.local_scope():\n",
    "            node_dict, edge_dict = self.node_dict, self.edge_dict\n",
    "            \n",
    "            #### remove the edges where the number of edges==0\n",
    "#             etypes_with_nonzero_edges = [etype for etype in sg.etypes if sg.num_edges(etype) > 0]\n",
    "#             sg=sg.edge_type_subgraph(etypes_with_nonzero_edges)\n",
    "            \n",
    "            for srctype, etype, dsttype in sg.canonical_etypes:\n",
    "                sub_graph = sg[srctype, etype, dsttype]\n",
    "\n",
    "                if sub_graph.num_edges()==0:\n",
    "                    sub_graph.edata['t'] = th.zeros([sub_graph.num_edges(),self.n_heads,1]).to(device)\n",
    "                else:\n",
    "                    k_linear = self.k_linears[node_dict[srctype]]\n",
    "                    v_linear = self.v_linears[node_dict[srctype]]\n",
    "                    q_linear = self.q_linears[node_dict[dsttype]]\n",
    "\n",
    "                    k = k_linear(h[srctype]).view(-1, self.n_heads, self.d_k)\n",
    "                    v = v_linear(h[srctype]).view(-1, self.n_heads, self.d_k)\n",
    "                    q = q_linear(h[dsttype][0:sg.num_dst_nodes()]).view(-1, self.n_heads, self.d_k)\n",
    "\n",
    "                    e_id = self.edge_dict[etype]\n",
    "\n",
    "                    relation_att = self.relation_att[e_id]\n",
    "                    relation_pri = self.relation_pri[e_id]\n",
    "                    relation_msg = self.relation_msg[e_id]\n",
    "\n",
    "                    k = th.einsum(\"bij,ijk->bik\", k, relation_att)\n",
    "                    v = th.einsum(\"bij,ijk->bik\", v, relation_msg)\n",
    "\n",
    "                    sub_graph.srcdata['k'] = k\n",
    "                    sub_graph.dstdata['q'] = q\n",
    "                    sub_graph.srcdata['v'] = v\n",
    "\n",
    "                    sub_graph.apply_edges(fn.v_dot_u('q', 'k', 't'))\n",
    "                    attn_score = sub_graph.edata.pop('t').sum(-1) * relation_pri / self.sqrt_dk\n",
    "                    attn_score = edge_softmax(sub_graph, attn_score, norm_by='dst')\n",
    "\n",
    "                    sub_graph.edata['t'] = attn_score.unsqueeze(-1)\n",
    "                    \n",
    "#                     if 't' not in list(sub_graph.edata.keys()): \n",
    "#                         sub_graph.edata['t'] = th.zeros([sub_graph.num_edges(),self.n_heads,1])\n",
    "\n",
    "#             for etype in sg.etypes:\n",
    "#                 if 't' not in list(sg.edges[etype].data.keys()): \n",
    "#                     sg.edges[etype].data['t'] = th.zeros([sg.num_edges(etype),self.n_heads,1])\n",
    "        \n",
    "            sg.multi_update_all({etype : (fn.u_mul_e('v', 't', 'm'), fn.sum('m', 't')) \\\n",
    "                                for etype in edge_dict}, cross_reducer = 'mean')\n",
    "\n",
    "            new_h = {}\n",
    "            for ntype in sg.ntypes:\n",
    "                '''\n",
    "                    Step 3: Target-specific Aggregation\n",
    "                    x = norm( W[node_type] * gelu( Agg(x) ) + x )\n",
    "                '''\n",
    "                n_id = node_dict[ntype]\n",
    "                alpha = th.sigmoid(self.skip[n_id])\n",
    "                t = sg.dstnodes[ntype].data['t'].view(-1, self.out_dim)\n",
    "                trans_out = self.drop(self.a_linears[n_id](t))\n",
    "                trans_out = trans_out * alpha + h[ntype][0:sg.num_dst_nodes()] * (1-alpha)\n",
    "                if self.use_norm:\n",
    "                    new_h[ntype] = self.norms[n_id](trans_out)\n",
    "                else:\n",
    "                    new_h[ntype] = trans_out\n",
    "            return new_h\n",
    "\n",
    "class HGT(nn.Module):\n",
    "    def __init__(self, G, node_dict, edge_dict, in_feat, h_dim, out_feat, n_layers, n_heads, use_norm = True):\n",
    "        super(HGT, self).__init__()\n",
    "        self.G=G\n",
    "        self.node_dict = node_dict\n",
    "        self.edge_dict = edge_dict\n",
    "        self.gcs = nn.ModuleList()\n",
    "        self.in_feat = in_feat\n",
    "        self.h_dim = h_dim\n",
    "        self.out_feat = out_feat\n",
    "        self.n_layers = n_layers\n",
    "        self.adapt_ws  = nn.ModuleList()\n",
    "        for t in range(len(node_dict)):\n",
    "            self.adapt_ws.append(nn.Linear(in_feat,   h_dim))\n",
    "        for _ in range(n_layers):\n",
    "            self.gcs.append(HGTLayer(h_dim, h_dim, node_dict, edge_dict, n_heads, use_norm = use_norm))\n",
    "        self.out = nn.Linear(h_dim, out_feat)\n",
    "        \n",
    "        self.node_embed=nn.ModuleDict()\n",
    "        self.node_embed['usaanr'] = USAANR_Embedding(self.G,self.in_feat)\n",
    "           \n",
    "    def forward(self, input_nodes, blocks=None):\n",
    "        H = {}\n",
    "        for ntype, nid in input_nodes.items():\n",
    "            nid = input_nodes[ntype]\n",
    "            H[ntype] = F.gelu(self.adapt_ws[self.node_dict[ntype]](self.node_embed[ntype](nid)))\n",
    "        \n",
    "        if blocks is None:\n",
    "            for layer in self.gcs:\n",
    "                H = layer(self.G, H)\n",
    "        else:\n",
    "            for layer, block in zip(self.gcs, blocks):\n",
    "                H = layer(block, H)\n",
    "        return self.out(H[\"usaanr\"]), H[\"usaanr\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lift_gain_eval(logit,label,topk):\n",
    "    DF=pd.DataFrame(columns=[\"pred_score\",\"actual_label\"])\n",
    "    DF[\"pred_score\"]=logit\n",
    "    DF[\"actual_label\"]=label\n",
    "    DF.sort_values(by=\"pred_score\", ascending=False, inplace=True)\n",
    "    gain={}\n",
    "    for p in topk:\n",
    "        N=math.ceil(int(DF.shape[0]*p))\n",
    "        DF2=DF.nlargest(N,\"pred_score\",keep=\"first\")\n",
    "        gain[str(int(p*100))+\"%\"]=round(DF2.actual_label.sum()/(DF.actual_label.sum()),2)\n",
    "    return gain\n",
    "\n",
    "def get_class_count_weight(y,n_classes):\n",
    "    classes_count=[]\n",
    "    weight=[]\n",
    "    for i in range(n_classes):\n",
    "        count=np.sum(y.squeeze()==i)\n",
    "        classes_count.append(count)\n",
    "        weight.append(len(y)/(n_classes*count))\n",
    "    return classes_count,weight\n",
    "\n",
    "def eval_loop_func(model, loader, labels, device, loss_weight, num_classes):\n",
    "    model.eval()\n",
    "    fin_targets=[]\n",
    "    fin_outputs=[]\n",
    "    losses=[]\n",
    "    for input_nodes_raw, seeds_raw, blocks in tqdm(loader, position=0, leave=True):\n",
    "        blocks = [blk.to(device) for blk in blocks]\n",
    "        seeds = seeds_raw.to(device)\n",
    "        \n",
    "        input_nodes={}\n",
    "        input_nodes[\"usaanr\"]=input_nodes_raw\n",
    "        input_nodes={k : e.to(device) for k, e in input_nodes.items()}\n",
    "\n",
    "        lbl = labels[seeds].squeeze().to(device)\n",
    "        \n",
    "        with th.no_grad():\n",
    "            logits,h = model(input_nodes,blocks)\n",
    "            \n",
    "            if loss_weight is None:\n",
    "                loss = F.cross_entropy(logits.view(-1, num_classes), lbl.to(device))\n",
    "            else:\n",
    "                loss = F.cross_entropy(logits.view(-1, num_classes), lbl.to(device),weight=loss_weight.float())        \n",
    "            losses.append(loss.item())\n",
    "        fin_targets.append(lbl.cpu().detach().numpy())\n",
    "        fin_outputs.append(logits.cpu().detach().numpy())\n",
    "    return np.concatenate(fin_outputs), np.concatenate(fin_targets), losses\n",
    "\n",
    "def evaluate(target, predicted):\n",
    "    true_label_mask=[1 if (np.argmax(x)-target[i])==0 else 0 for i,x in enumerate(predicted)]\n",
    "    nb_prediction=len(true_label_mask)\n",
    "    true_prediction=sum(true_label_mask)\n",
    "    false_prediction=nb_prediction-true_prediction\n",
    "    accuracy=true_prediction/nb_prediction\n",
    "    \n",
    "    precision, recall, fscore, support = precision_recall_fscore_support(target, predicted.argmax(axis=1))\n",
    "    auc = roc_auc_score(target.ravel(), th.sigmoid(th.from_numpy(predicted))[:,1].numpy().ravel())\n",
    "    \n",
    "    prec,rec,_ = precision_recall_curve(target.ravel(), th.sigmoid(th.from_numpy(predicted))[:,1].numpy().ravel())\n",
    "    \n",
    "    pr_auc=auc_score(rec,prec)\n",
    "    \n",
    "    arg1=predicted[:,1]\n",
    "    arg2=target\n",
    "    gain = lift_gain_eval(arg1,arg2,topk=[0.01,0.05,0.10])\n",
    "    \n",
    "    return {\n",
    "        \"nb_example\":len(target),\n",
    "        \"true_prediction\":true_prediction,\n",
    "        \"false_prediction\":false_prediction,\n",
    "        \"accuracy\":accuracy,\n",
    "        \"precision\":precision[1], \n",
    "        \"recall\":recall[1], \n",
    "        \"f1_score\":fscore[1],\n",
    "        \"AUC\":auc,\n",
    "        \"pr_auc\":pr_auc,\n",
    "        \"GAIN\":gain\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=10240, clip=1.0, dropout=0.2, fanout=None, gpu=0, h_dim=64, l2norm=0.001, loss_weight=True, lr=0.001, max_lr=0.001, model_path='/workspace/cjiang/eagle_project/CAP_graph/hgt_model_param.pt', n_epochs=3, n_head=4, num_layers=2, num_mini_batch=8, num_worker=0, out_dim=1, seed=101)\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='HGT')\n",
    "parser.add_argument(\"--dropout\", type=float, default=0,\n",
    "        help=\"dropout probability\")\n",
    "parser.add_argument(\"--h_dim\", type=int, default=128,\n",
    "        help=\"number of hidden units\")\n",
    "parser.add_argument(\"--out_dim\", type=int, default=1,\n",
    "        help=\"output dimension\")\n",
    "parser.add_argument(\"--gpu\", type=int, default=0,\n",
    "        help=\"gpu\")\n",
    "\n",
    "parser.add_argument(\"--n_head\", type=int, default=4,\n",
    "        help=\"number of head\")\n",
    "parser.add_argument(\"--lr\", type=float, default=1e-5,\n",
    "        help=\"learning rate\")\n",
    "parser.add_argument('--clip',    type=int, default=1.0) \n",
    "parser.add_argument('--max_lr',  type=float, default=1e-3) \n",
    "\n",
    "parser.add_argument(\"--num_layers\", type=int, default=1,\n",
    "        help=\"number of propagation rounds\")\n",
    "parser.add_argument(\"-e\", \"--n_epochs\", type=int, default=1,\n",
    "        help=\"number of training epochs\")\n",
    "parser.add_argument(\"--model_path\", type=str, default=\"/workspace/cjiang/eagle_project/CAP_graph/hgt_model_param.pt\",\n",
    "        help='path for save the model')\n",
    "parser.add_argument(\"--l2norm\", type=float, default=0,\n",
    "        help=\"l2 norm coef\")\n",
    "\n",
    "parser.add_argument(\"--batch-size\", type=int, default=1024,\n",
    "        help=\"Mini-batch size. If -1, use full graph training.\")\n",
    "parser.add_argument(\"--num_mini_batch\", type=int, default=8,\n",
    "        help=\"Number of minibatch.\")\n",
    "parser.add_argument(\"--fanout\", type=int, default=None,\n",
    "        help=\"Fan-out of neighbor sampling.\")\n",
    "\n",
    "parser.add_argument(\"--seed\",  type=int,default=101,\n",
    "        help=\"random seed for np.random.seed, torch.manual_seed and torch.cuda.manual_seed.\")\n",
    "\n",
    "parser.add_argument(\"--loss_weight\",  type=bool,default=True,  ## number of label=0/number of label=1\n",
    "        help=\"weight for unbalance data\")\n",
    "\n",
    "parser.add_argument(\"--num_worker\",  type=int,default=0,  \n",
    "        help=\"number of worker for neighbor sampling\") \n",
    "\n",
    "args,unknown=parser.parse_known_args()\n",
    "\n",
    "args.num_layers=2\n",
    "args.dropout=0.2\n",
    "args.lr=1e-3\n",
    "args.l2norm=1e-3\n",
    "args.n_epochs=3\n",
    "args.h_dim=64\n",
    "args.n_head=4\n",
    "# args.batch_size=1024\n",
    "args.batch_size=10240\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "</style><table id=\"T_f6dd035c_029f_11ec_8bcb_0242ac110003\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >Auto or Not</th>        <th class=\"col_heading level0 col1\" >count</th>        <th class=\"col_heading level0 col2\" >weight</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_f6dd035c_029f_11ec_8bcb_0242ac110003level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "                        <td id=\"T_f6dd035c_029f_11ec_8bcb_0242ac110003row0_col0\" class=\"data row0 col0\" >No</td>\n",
       "                        <td id=\"T_f6dd035c_029f_11ec_8bcb_0242ac110003row0_col1\" class=\"data row0 col1\" >19,358,913</td>\n",
       "                        <td id=\"T_f6dd035c_029f_11ec_8bcb_0242ac110003row0_col2\" class=\"data row0 col2\" >0.66</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_f6dd035c_029f_11ec_8bcb_0242ac110003level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "                        <td id=\"T_f6dd035c_029f_11ec_8bcb_0242ac110003row1_col0\" class=\"data row1 col0\" >Yes</td>\n",
       "                        <td id=\"T_f6dd035c_029f_11ec_8bcb_0242ac110003row1_col1\" class=\"data row1 col1\" >6,309,591</td>\n",
       "                        <td id=\"T_f6dd035c_029f_11ec_8bcb_0242ac110003row1_col2\" class=\"data row1 col2\" >2.03</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f3f9ff757b8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=binary_label.squeeze().numpy()\n",
    "n_classes=th.unique(binary_label).shape[0]\n",
    "classes_count,weight=get_class_count_weight(y,n_classes)\n",
    "imbalance_classes={}\n",
    "# imbalance_classes[\"category\"]=th.unique(binary_label).tolist()\n",
    "imbalance_classes[\"Auto or Not\"]=[\"No\",\"Yes\"]\n",
    "imbalance_classes[\"count\"]=classes_count\n",
    "imbalance_classes[\"weight\"]=weight\n",
    "imbalance_classes=pd.DataFrame(imbalance_classes)\n",
    "imbalance_classes.style.format({\"count\":\"{:,}\",\"weight\":\"{:.2f}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Distribution of Target Variables')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.barplot(x=\"Auto or Not\",y=\"count\",data=imbalance_classes)\n",
    "plt.title(\"Distribution of Target Variables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set    20,534,804\n",
      "validation set  2,566,850 \n",
      "test set        2,566,850 \n"
     ]
    }
   ],
   "source": [
    "train_idx=th.nonzero(train_mask_binary_label.squeeze()).numpy()\n",
    "val_idx=th.nonzero(val_mask_binary_label.squeeze()).numpy()\n",
    "test_idx=th.nonzero(test_mask_binary_label.squeeze()).numpy()\n",
    "\n",
    "train_idx=th.from_numpy(train_idx).squeeze()    \n",
    "val_idx=th.from_numpy(val_idx).squeeze()    \n",
    "test_idx=th.from_numpy(test_idx).squeeze()\n",
    "\n",
    "label_train=binary_label[train_idx].squeeze().numpy()\n",
    "label_val=binary_label[val_idx].squeeze().numpy()\n",
    "label_test=binary_label[test_idx].squeeze().numpy()\n",
    "\n",
    "\n",
    "print('{:<15} {:<10,}'.format(\"Training set\",train_idx.shape[0]))\n",
    "print('{:<15} {:<10,}'.format(\"validation set\",val_idx.shape[0]))\n",
    "print('{:<15} {:<10,}'.format(\"test set\",test_idx.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "##### check cuda\n",
    "device=\"cpu\"\n",
    "use_cuda=args.gpu>=0 and th.cuda.is_available()\n",
    "if use_cuda:\n",
    "    th.cuda.set_device(args.gpu)\n",
    "    device='cuda:%d' % args.gpu\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes=th.unique(th.from_numpy(label_train)).shape[0]\n",
    "if args.loss_weight:\n",
    "    train_classes_num, train_classes_weight = get_class_count_weight(label_train,num_classes)\n",
    "    loss_weight=th.tensor(train_classes_weight).to(device)\n",
    "else:\n",
    "    loss_weight=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'usaanr': 0}\n",
      "{'AUTO_RELATED': 0, 'Brother_Sister': 1, 'Busi_rel_Other': 2, 'Child': 3, 'Ex-Spouse': 4, 'Parent': 5, 'Pers_rel_Other': 6, 'SPONSEE': 7, 'SPONSOR': 8, 'Spouse': 9, 'Step-Child': 10, 'Step-Parent': 11}\n"
     ]
    }
   ],
   "source": [
    "node_dict = {}\n",
    "edge_dict = {}\n",
    "for ntype in G.ntypes:\n",
    "    node_dict[ntype] = len(node_dict)\n",
    "for etype in G.etypes:\n",
    "    edge_dict[etype] = len(edge_dict)\n",
    "    G.edges[etype].data['id'] = th.ones(G.number_of_edges(etype), dtype=th.long) * edge_dict[etype] \n",
    "\n",
    "print(node_dict)\n",
    "print(edge_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUTO_RELATED   id        0         \n",
      "Brother_Sister id        1         \n",
      "Busi_rel_Other id        2         \n",
      "Child          id        3         \n",
      "Ex-Spouse      id        4         \n",
      "Parent         id        5         \n",
      "Pers_rel_Other id        6         \n",
      "SPONSEE        id        7         \n",
      "SPONSOR        id        8         \n",
      "Spouse         id        9         \n",
      "Step-Child     id        10        \n",
      "Step-Parent    id        11        \n"
     ]
    }
   ],
   "source": [
    "for etype in G.etypes:\n",
    "    for key,val in G.edges[etype].data.items():\n",
    "        if key==\"id\":\n",
    "            print(\"{:<15}{:<10}{:<10}\".format(etype,key,th.unique(val).item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0): HGTLayer(\n",
       "    (k_linears): ModuleList(\n",
       "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "    )\n",
       "    (q_linears): ModuleList(\n",
       "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "    )\n",
       "    (v_linears): ModuleList(\n",
       "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "    )\n",
       "    (a_linears): ModuleList(\n",
       "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "    )\n",
       "    (norms): ModuleList(\n",
       "      (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (drop): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (1): HGTLayer(\n",
       "    (k_linears): ModuleList(\n",
       "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "    )\n",
       "    (q_linears): ModuleList(\n",
       "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "    )\n",
       "    (v_linears): ModuleList(\n",
       "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "    )\n",
       "    (a_linears): ModuleList(\n",
       "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "    )\n",
       "    (norms): ModuleList(\n",
       "      (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (drop): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = HGT(G,\n",
    "            node_dict, \n",
    "            edge_dict,\n",
    "            in_feat=args.h_dim,\n",
    "            h_dim=args.h_dim,\n",
    "            out_feat=num_classes,\n",
    "            n_layers=args.num_layers,\n",
    "            n_heads=args.n_head,\n",
    "            use_norm = True)\n",
    "\n",
    "if use_cuda:\n",
    "    model.cuda()\n",
    "\n",
    "model.gcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = th.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.l2norm)\n",
    "# optimizer = th.optim.AdamW(model.parameters())\n",
    "optimizer = th.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.l2norm)\n",
    "### the number of n_epochs affects the learning rate, therefore affecting model evaluation results.\n",
    "scheduler = th.optim.lr_scheduler.OneCycleLR(optimizer, total_steps=args.n_epochs, max_lr = args.max_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of minibatch in training set is 2,006\n",
      "The number of minibatch in validation set is 251\n",
      "The number of minibatch in test set is 251\n"
     ]
    }
   ],
   "source": [
    "# train sampler\n",
    "train_sampler = dgl.dataloading.MultiLayerNeighborSampler([args.fanout] * args.num_layers)\n",
    "train_loader = dgl.dataloading.NodeDataLoader(\n",
    "    G, {'usaanr': train_idx}, train_sampler,\n",
    "    batch_size=args.batch_size, shuffle=True, num_workers=args.num_worker)\n",
    "# validation sampler\n",
    "# we do not use full neighbor to save computation resources\n",
    "val_sampler = dgl.dataloading.MultiLayerNeighborSampler([args.fanout] * args.num_layers)\n",
    "val_loader = dgl.dataloading.NodeDataLoader(\n",
    "    G, {'usaanr': val_idx}, val_sampler,\n",
    "    batch_size=args.batch_size, shuffle=False, num_workers=args.num_worker)\n",
    "\n",
    "test_sampler = dgl.dataloading.MultiLayerNeighborSampler([args.fanout] * args.num_layers)\n",
    "test_loader = dgl.dataloading.NodeDataLoader(\n",
    "    G, {'usaanr': test_idx}, test_sampler,\n",
    "    batch_size=args.batch_size, shuffle=False, num_workers=args.num_worker)\n",
    "\n",
    "print(\"The number of minibatch in training set is {:,}\".format(len(train_loader)))\n",
    "print(\"The number of minibatch in validation set is {:,}\".format(len(val_loader)))\n",
    "print(\"The number of minibatch in test set is {:,}\".format(len(test_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total # of parameter is 104,164\n"
     ]
    }
   ],
   "source": [
    "print(\"The total # of parameter is {:,}\".format(sum([p.nelement() for p in model.parameters()]) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcs.0.relation_pri                                                    48             \n",
      "gcs.0.relation_att                                                    12,288         \n",
      "gcs.0.relation_msg                                                    12,288         \n",
      "gcs.0.skip                                                            1              \n",
      "gcs.0.k_linears.0.weight                                              4,096          \n",
      "gcs.0.k_linears.0.bias                                                64             \n",
      "gcs.0.q_linears.0.weight                                              4,096          \n",
      "gcs.0.q_linears.0.bias                                                64             \n",
      "gcs.0.v_linears.0.weight                                              4,096          \n",
      "gcs.0.v_linears.0.bias                                                64             \n",
      "gcs.0.a_linears.0.weight                                              4,096          \n",
      "gcs.0.a_linears.0.bias                                                64             \n",
      "gcs.0.norms.0.weight                                                  64             \n",
      "gcs.0.norms.0.bias                                                    64             \n",
      "gcs.1.relation_pri                                                    48             \n",
      "gcs.1.relation_att                                                    12,288         \n",
      "gcs.1.relation_msg                                                    12,288         \n",
      "gcs.1.skip                                                            1              \n",
      "gcs.1.k_linears.0.weight                                              4,096          \n",
      "gcs.1.k_linears.0.bias                                                64             \n",
      "gcs.1.q_linears.0.weight                                              4,096          \n",
      "gcs.1.q_linears.0.bias                                                64             \n",
      "gcs.1.v_linears.0.weight                                              4,096          \n",
      "gcs.1.v_linears.0.bias                                                64             \n",
      "gcs.1.a_linears.0.weight                                              4,096          \n",
      "gcs.1.a_linears.0.bias                                                64             \n",
      "gcs.1.norms.0.weight                                                  64             \n",
      "gcs.1.norms.0.bias                                                    64             \n",
      "adapt_ws.0.weight                                                     4,096          \n",
      "adapt_ws.0.bias                                                       64             \n",
      "out.weight                                                            128            \n",
      "out.bias                                                              2              \n",
      "node_embed.usaanr.emb.usaayr.weight                                   5,440          \n",
      "node_embed.usaanr.emb.AGE_BAND.weight                                 448            \n",
      "node_embed.usaanr.emb.ORIGEL.weight                                   2,752          \n",
      "node_embed.usaanr.emb.ELIG2.weight                                    384            \n",
      "node_embed.usaanr.emb.SEX.weight                                      320            \n",
      "node_embed.usaanr.emb.MARST.weight                                    640            \n",
      "node_embed.usaanr.emb.BRANCH.weight                                   1,088          \n",
      "node_embed.usaanr.emb.ENLPAYGD.weight                                 1,600          \n",
      "node_embed.usaanr.emb.MILST.weight                                    512            \n",
      "node_embed.usaanr.emb.MLIST_OrigStat.weight                           192            \n",
      "node_embed.usaanr.emb.STATE.weight                                    3,712          \n"
     ]
    }
   ],
   "source": [
    "param_dict={n: p.nelement() for n, p in model.named_parameters()}\n",
    "for i,j in param_dict.items():\n",
    "    print(\"{:<70}{:<15,}\".format(i,j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned ON\n",
      "start training...\n",
      "\n",
      "========= Epoch 1 /3\n",
      "Training...\n",
      "Batch 200 of 2006 | Loss 0.389  | Elapsed: 0:02:58\n",
      "Batch 400 of 2006 | Loss 0.383  | Elapsed: 0:05:28\n",
      "Batch 600 of 2006 | Loss 0.377  | Elapsed: 0:07:54\n",
      "Batch 800 of 2006 | Loss 0.375  | Elapsed: 0:10:20\n",
      "Batch 1000 of 2006 | Loss 0.369  | Elapsed: 0:12:50\n",
      "Batch 1200 of 2006 | Loss 0.369  | Elapsed: 0:15:07\n",
      "Batch 1400 of 2006 | Loss 0.371  | Elapsed: 0:17:36\n",
      "Batch 1600 of 2006 | Loss 0.368  | Elapsed: 0:20:02\n",
      "Batch 1800 of 2006 | Loss 0.365  | Elapsed: 0:22:32\n",
      "Batch 2000 of 2006 | Loss 0.367  | Elapsed: 0:25:09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2006 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Running Validation on training set\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2006/2006 [08:33<00:00,  3.91it/s] \n",
      "  0%|          | 0/251 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_loss: 0.49 | True_Prediction: 16,860,255 | False_Prediction: 3,674,549 | accuracy: 82.11% |  precision: 65.57% | recall: 57.27% | F1_score: 61.14% | Gain_top-10%: 0.3 |    ROC_AUC: 87.0% | PR_AUC: 67.3% | Elapsed: 0:10:26\n",
      "\n",
      "\n",
      "Running Validation on validation set\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 251/251 [00:58<00:00,  4.31it/s]\n",
      " 33%|███▎      | 1/3 [36:45<1:13:30, 2205.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_loss: 0.49 | True_Prediction: 2,107,423 | False_Prediction: 459,427 | accuracy: 82.10% |  precision: 65.57% | recall: 57.23% | F1_score: 61.12% | Gain_top-10%: 0.3 |    ROC_AUC: 87.0% | PR_AUC: 67.4% | Elapsed: 0:01:11\n",
      "\n",
      "========= Epoch 2 /3\n",
      "Training...\n",
      "Batch 200 of 2006 | Loss 0.367  | Elapsed: 0:02:12\n",
      "Batch 400 of 2006 | Loss 0.366  | Elapsed: 0:04:25\n",
      "Batch 600 of 2006 | Loss 0.364  | Elapsed: 0:06:28\n",
      "Batch 800 of 2006 | Loss 0.366  | Elapsed: 0:08:43\n",
      "Batch 1000 of 2006 | Loss 0.361  | Elapsed: 0:11:05\n",
      "Batch 1200 of 2006 | Loss 0.366  | Elapsed: 0:13:16\n",
      "Batch 1400 of 2006 | Loss 0.364  | Elapsed: 0:15:46\n",
      "Batch 1600 of 2006 | Loss 0.362  | Elapsed: 0:17:57\n",
      "Batch 1800 of 2006 | Loss 0.364  | Elapsed: 0:20:37\n",
      "Batch 2000 of 2006 | Loss 0.365  | Elapsed: 0:22:51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2006 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Running Validation on training set\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2006/2006 [08:46<00:00,  3.81it/s] \n",
      "  0%|          | 0/251 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_loss: 0.52 | True_Prediction: 16,928,910 | False_Prediction: 3,605,894 | accuracy: 82.44% |  precision: 69.29% | recall: 51.29% | F1_score: 58.95% | Gain_top-10%: 0.3 |    ROC_AUC: 87.5% | PR_AUC: 68.4% | Elapsed: 0:10:41\n",
      "\n",
      "\n",
      "Running Validation on validation set\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 251/251 [00:58<00:00,  4.27it/s]\n",
      " 67%|██████▋   | 2/3 [1:12:12<36:21, 2181.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_loss: 0.52 | True_Prediction: 2,116,397 | False_Prediction: 450,453 | accuracy: 82.45% |  precision: 69.35% | recall: 51.27% | F1_score: 58.95% | Gain_top-10%: 0.3 |    ROC_AUC: 87.5% | PR_AUC: 68.5% | Elapsed: 0:01:11\n",
      "\n",
      "========= Epoch 3 /3\n",
      "Training...\n",
      "Batch 200 of 2006 | Loss 0.363  | Elapsed: 0:02:09\n",
      "Batch 400 of 2006 | Loss 0.363  | Elapsed: 0:04:25\n",
      "Batch 600 of 2006 | Loss 0.366  | Elapsed: 0:06:30\n",
      "Batch 800 of 2006 | Loss 0.362  | Elapsed: 0:08:52\n",
      "Batch 1000 of 2006 | Loss 0.362  | Elapsed: 0:10:57\n",
      "Batch 1200 of 2006 | Loss 0.362  | Elapsed: 0:13:04\n",
      "Batch 1400 of 2006 | Loss 0.365  | Elapsed: 0:15:36\n",
      "Batch 1600 of 2006 | Loss 0.364  | Elapsed: 0:17:44\n",
      "Batch 1800 of 2006 | Loss 0.362  | Elapsed: 0:19:51\n",
      "Batch 2000 of 2006 | Loss 0.365  | Elapsed: 0:22:37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2006 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Running Validation on training set\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2006/2006 [07:29<00:00,  4.46it/s]\n",
      "  0%|          | 0/251 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_loss: 0.51 | True_Prediction: 16,927,053 | False_Prediction: 3,607,751 | accuracy: 82.43% |  precision: 67.76% | recall: 54.42% | F1_score: 60.36% | Gain_top-10%: 0.3 |    ROC_AUC: 87.5% | PR_AUC: 68.4% | Elapsed: 0:09:23\n",
      "\n",
      "\n",
      "Running Validation on validation set\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 251/251 [00:46<00:00,  5.43it/s]\n",
      "100%|██████████| 3/3 [1:46:09<00:00, 2123.11s/it]\n",
      "  0%|          | 0/251 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_loss: 0.51 | True_Prediction: 2,115,734 | False_Prediction: 451,116 | accuracy: 82.43% |  precision: 67.77% | recall: 54.36% | F1_score: 60.33% | Gain_top-10%: 0.3 |    ROC_AUC: 87.5% | PR_AUC: 68.5% | Elapsed: 0:00:58\n",
      "\n",
      "\n",
      "Running Validation in Test Dataset\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 251/251 [00:50<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_loss: 0.51 | True_Prediction: 2,116,481 | False_Prediction: 450,369 | accuracy: 82.45% |  precision: 67.80% | recall: 54.52% | F1_score: 60.44% | Gain_top-10%: 0.3 |ROC_AUC: 87.5% | PR_AUC: 68.4% | Elapsed: 0:01:04\n"
     ]
    }
   ],
   "source": [
    "%pdb\n",
    "LOSS_EPOCH=[]\n",
    "LABEL_TRAIN=[]\n",
    "\n",
    "# training loop\n",
    "print(\"start training...\")\n",
    "\n",
    "total_loss=0\n",
    "losses=[]\n",
    "\n",
    "LOGIT_train=[]\n",
    "LABEL_train=[]\n",
    "\n",
    "# th.manual_seed(args.seed)\n",
    "# th.cuda.manual_seed(args.seed)\n",
    "# th.cuda.manual_seed_all(args.seed)\n",
    "# np.ranom.seed(args.seed)\n",
    "# random.seed(args.seed)\n",
    "# th.backends.cudnn.deterministic=True\n",
    "\n",
    "for epoch in tqdm(range(0,args.n_epochs)):\n",
    "    \n",
    "    model.train()\n",
    "    IDX=[]\n",
    "    H=[]\n",
    "    \n",
    "    #====================================#\n",
    "    #            Traning                 #\n",
    "    #====================================#\n",
    "    print(\"\")\n",
    "    print(\"========= Epoch {:} /{:}\".format(epoch+1,args.n_epochs))\n",
    "    print(\"Training...\")\n",
    "    t0 = time.time()\n",
    "    for step, (input_nodes_raw, seeds_raw, blocks) in enumerate(train_loader):\n",
    "        blocks = [blk.to(device) for blk in blocks]\n",
    "        \n",
    "        seeds=seeds_raw.to(device)\n",
    "        \n",
    "        labels_train=binary_label[seeds].to(device)       \n",
    "        \n",
    "        input_nodes={}\n",
    "        input_nodes[\"usaanr\"]=input_nodes_raw\n",
    "        input_nodes={k : e.to(device) for k, e in input_nodes.items()}\n",
    "        \n",
    "        logits,h = model(input_nodes,blocks)\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        if args.loss_weight :\n",
    "            loss = F.cross_entropy(logits.view(-1, num_classes), labels_train.squeeze().to(device))\n",
    "        else:\n",
    "            loss = F.cross_entropy(logits.view(-1, num_classes), labels_train.squeeze().to(device),weight=loss_weight.to(device))\n",
    "        \n",
    "        total_loss+=loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        arg1=logits[:,1].detach().cpu().numpy()\n",
    "        arg2=labels_train.cpu().numpy()\n",
    "                \n",
    "        train_gain = lift_gain_eval(arg1,arg2,topk=[0.01,0.05,0.10])\n",
    "        \n",
    "        train_acc = th.sum(logits.argmax(dim=1) == labels_train).item() / len(seeds)\n",
    "        precision, recall, fscore, support = precision_recall_fscore_support(labels_train.cpu().numpy(), logits.argmax(dim=1).cpu().numpy())\n",
    "\n",
    "        try:\n",
    "            train_auc = roc_auc_score(labels_train.detach().cpu().numpy().ravel(), th.sigmoid(logits)[:,1].detach().cpu().numpy().ravel())\n",
    "        except ValueError:\n",
    "            pass\n",
    "        \n",
    "        prec,rec,_ = precision_recall_curve(labels_train.detach().cpu().numpy().ravel(), th.sigmoid(logits)[:,1].detach().cpu().numpy().ravel())\n",
    "        if math.isnan(rec[0])==False:\n",
    "            train_pr_auc=auc_score(rec,prec)\n",
    "\n",
    "        IDX.extend(seeds.detach().cpu().numpy().tolist())\n",
    "        H.extend(h.detach().cpu().numpy().tolist())\n",
    "        LOGIT_train.extend(logits.detach().cpu().numpy().tolist())\n",
    "        LABEL_train.extend(binary_label[blocks[-1].dstnodes['usaanr'].data[dgl.NID].cpu().numpy()].tolist())\n",
    "        \n",
    "        if step%(len(train_loader)//10)==0 and not step==0:\n",
    "\n",
    "            t1 = time.time()\n",
    "            elapsed=utils.format_time(t1-t0)\n",
    "            print(\"Batch {:} of {:} | Loss {:.3f}  | Elapsed: {:}\".\\\n",
    "                  format(step,len(train_loader),np.mean(losses[-10:]),elapsed)) \n",
    "            \n",
    "    LOSS_EPOCH.append(loss)\n",
    "\n",
    "    LABEL_TRAIN.append(binary_label[blocks[-1].nodes['usaanr'].data[dgl.NID].cpu().numpy()])\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    print()\n",
    "    print(\"\")\n",
    "    print(\"Running Validation on training set\")\n",
    "    print(\"\")\n",
    "    fin_outputs, fin_targets, losses_tmp=eval_loop_func(model, train_loader, binary_label,  device, loss_weight, num_classes)\n",
    "    \n",
    "    avg_loss_train=np.mean(losses_tmp)\n",
    "    \n",
    "    tmp_mean_pool_train=evaluate(fin_targets.reshape(-1),fin_outputs)\n",
    "    \n",
    "    t2=time.time()\n",
    "    \n",
    "    print(\"avg_loss: {:.2f} | True_Prediction: {:,} | False_Prediction: {:,} | accuracy: {:.2%} |  precision: {:.2%} | recall: {:.2%} | F1_score: {:.2%} | Gain_top-10%: {:.1f} |\\\n",
    "    ROC_AUC: {:.1%} | PR_AUC: {:.1%} | Elapsed: {:}\".format(avg_loss_train, tmp_mean_pool_train[\"true_prediction\"], tmp_mean_pool_train[\"false_prediction\"], tmp_mean_pool_train[\"accuracy\"], \\\n",
    "                                                            tmp_mean_pool_train[\"precision\"], tmp_mean_pool_train[\"recall\"], tmp_mean_pool_train[\"f1_score\"], tmp_mean_pool_train[\"GAIN\"]['10%'], \\\n",
    "                                                            tmp_mean_pool_train[\"AUC\"], tmp_mean_pool_train[\"pr_auc\"], utils.format_time(t2-t1)))\n",
    "\n",
    "    #====================================#\n",
    "    #            Validation-set          #\n",
    "    #====================================#\n",
    "    \n",
    "    model.eval()\n",
    "    print()\n",
    "    print(\"\")\n",
    "    print(\"Running Validation on validation set\")\n",
    "    print(\"\")\n",
    "    \n",
    "    fin_outputs, fin_targets, losses_tmp=eval_loop_func(model, val_loader, binary_label,  device, loss_weight, num_classes)\n",
    "    \n",
    "    avg_loss_val=np.mean(losses_tmp)\n",
    "    \n",
    "    tmp_mean_pool_val=evaluate(fin_targets.reshape(-1),fin_outputs)\n",
    "    \n",
    "    t3=time.time()\n",
    "    \n",
    "    print(\"avg_loss: {:.2f} | True_Prediction: {:,} | False_Prediction: {:,} | accuracy: {:.2%} |  precision: {:.2%} | recall: {:.2%} | F1_score: {:.2%} | Gain_top-10%: {:.1f} |\\\n",
    "    ROC_AUC: {:.1%} | PR_AUC: {:.1%} | Elapsed: {:}\".format(avg_loss_val, tmp_mean_pool_val[\"true_prediction\"], tmp_mean_pool_val[\"false_prediction\"], tmp_mean_pool_val[\"accuracy\"], \\\n",
    "                                                            tmp_mean_pool_val[\"precision\"], tmp_mean_pool_val[\"recall\"], tmp_mean_pool_val[\"f1_score\"], tmp_mean_pool_val[\"GAIN\"]['10%'], \\\n",
    "                                                            tmp_mean_pool_val[\"AUC\"], tmp_mean_pool_val[\"pr_auc\"], utils.format_time(t3-t2)))\n",
    "    \n",
    "# if args.model_path is not None:\n",
    "#     th.save(model.state_dict(), args.model_path)\n",
    "    \n",
    "#====================================#\n",
    "#            Test-set                #\n",
    "#====================================#\n",
    "print()\n",
    "print(\"\")\n",
    "print(\"Running Validation in Test Dataset\")\n",
    "print(\"\")\n",
    "model.eval()\n",
    "\n",
    "fin_outputs, fin_targets, losses_tmp=eval_loop_func(model, test_loader, binary_label,  device, loss_weight, num_classes)\n",
    "    \n",
    "avg_loss_test=np.mean(losses_tmp)\n",
    "\n",
    "tmp_mean_pool_test=evaluate(fin_targets.reshape(-1),fin_outputs)\n",
    "\n",
    "t4=time.time()\n",
    "\n",
    "print(\"avg_loss: {:.2f} | True_Prediction: {:,} | False_Prediction: {:,} | accuracy: {:.2%} |  precision: {:.2%} | recall: {:.2%} | F1_score: {:.2%} | Gain_top-10%: {:.1f} |\\\n",
    "ROC_AUC: {:.1%} | PR_AUC: {:.1%} | Elapsed: {:}\".format(avg_loss_test, tmp_mean_pool_test[\"true_prediction\"], tmp_mean_pool_test[\"false_prediction\"], tmp_mean_pool_test[\"accuracy\"], \\\n",
    "                                                        tmp_mean_pool_test[\"precision\"], tmp_mean_pool_test[\"recall\"], tmp_mean_pool_test[\"f1_score\"], tmp_mean_pool_test[\"GAIN\"]['10%'], \\\n",
    "                                                        tmp_mean_pool_test[\"AUC\"], tmp_mean_pool_test[\"pr_auc\"], utils.format_time(t4-t3)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nb_example': 20534804,\n",
       " 'true_prediction': 16927053,\n",
       " 'false_prediction': 3607751,\n",
       " 'accuracy': 0.8243104243897337,\n",
       " 'precision': 0.6775846002132118,\n",
       " 'recall': 0.5442218622323594,\n",
       " 'f1_score': 0.6036248199406297,\n",
       " 'AUC': 0.875024867957377,\n",
       " 'pr_auc': 0.6837390084566433,\n",
       " 'GAIN': {'1%': 0.04, '5%': 0.17, '10%': 0.32}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_mean_pool_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nb_example': 2566850,\n",
       " 'true_prediction': 2115734,\n",
       " 'false_prediction': 451116,\n",
       " 'accuracy': 0.82425307283246,\n",
       " 'precision': 0.6776574579522712,\n",
       " 'recall': 0.5436121839929378,\n",
       " 'f1_score': 0.6032784866899421,\n",
       " 'AUC': 0.8750637208082545,\n",
       " 'pr_auc': 0.684522840543195,\n",
       " 'GAIN': {'1%': 0.04, '5%': 0.17, '10%': 0.32}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_mean_pool_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nb_example': 2566850,\n",
       " 'true_prediction': 2116481,\n",
       " 'false_prediction': 450369,\n",
       " 'accuracy': 0.8245440910064865,\n",
       " 'precision': 0.6779702775149795,\n",
       " 'recall': 0.5451622054681842,\n",
       " 'f1_score': 0.604356072419067,\n",
       " 'AUC': 0.8751656074330677,\n",
       " 'pr_auc': 0.6840378897340809,\n",
       " 'GAIN': {'1%': 0.04, '5%': 0.17, '10%': 0.32}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_mean_pool_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not os.path.exists(\"model_save\"):\n",
    "#     os.makedirs(\"model_save\")\n",
    "# model.save_model(\"model_save/HGT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Score Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# fin_outputs, fin_targets, losses_tmp=eval_loop_func(model, test_loader, binary_label,  device, loss_weight, num_classes)\n",
    "\n",
    "# savez_compressed(\"model_save/HGT_LOGIT.npz\", fin_outputs)\n",
    "# savez_compressed(\"model_save/HGT_LABEL.npz\", fin_targets)\n",
    "\n",
    "# # os.system(\"rm model_save/HGT_LOGIT.npz\")\n",
    "# # os.system(\"rm model_save/HGT_LABEL.npz\")\n",
    "\n",
    "# fin_logits = load(\"model_save/HGT_LOGIT.npz\")['arr_0']\n",
    "# fin_targets = load(\"model_save/HGT_LABEL.npz\")['arr_0']\n",
    "\n",
    "# tmp_test=evaluate(fin_targets.reshape(-1),fin_logits)\n",
    "\n",
    "# print(\"\")\n",
    "# print(\"==> Running Validation on validation set \\n\")\n",
    "# print(\"\")\n",
    "\n",
    "# print(\"True_Prediction: {:,} | False_Prediction: {:,} | accuracy: {:.2%} |  precision: {:.2%} | recall: {:.2%} | F1_score: {:.2%} | Gain_top-10%: {:.1f} |\\\n",
    "# ROC_AUC: {:.1%} | PR_AUC: {:.1%}\".format(tmp_test[\"true_prediction\"], tmp_test[\"false_prediction\"], tmp_test[\"accuracy\"], \\\n",
    "#                                                         tmp_test[\"precision\"], tmp_test[\"recall\"], tmp_test[\"f1_score\"], tmp_test[\"GAIN\"]['10%'], \\\n",
    "#                                                         tmp_test[\"AUC\"], tmp_test[\"pr_auc\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.state_dict()tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
