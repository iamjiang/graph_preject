{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version is 1.6.0\n",
      "DGL version is 0.7a210721\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import itertools\n",
    "import os\n",
    "import numpy as np\n",
    "from numpy import save,load,savetxt,loadtxt,savez_compressed\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score, f1_score,average_precision_score\n",
    "from sklearn.metrics import precision_recall_fscore_support \n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import auc as auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.preprocessing import LabelEncoder, label_binarize\n",
    "\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "import time\n",
    "from tqdm import tqdm, tqdm_notebook,tnrange\n",
    "tqdm.pandas(position=0, leave=True)\n",
    "import math \n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import dgl\n",
    "from dgl import edge_subgraph\n",
    "import dgl.nn as dglnn\n",
    "import dgl.function as fn\n",
    "\n",
    "import functools\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize']=(5.0,4.0)\n",
    "plt.rcParams['image.interpolation']='nearest'\n",
    "plt.rcParams['image.cmap']='gray'\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import utils\n",
    "\n",
    "print(\"torch version is {}\".format(th.__version__))\n",
    "print(\"DGL version is {}\".format(dgl.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    th.manual_seed(seed)\n",
    "    th.cuda.manual_seed_all(seed)\n",
    "    th.backends.cudnn.deterministic = True\n",
    "    th.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    \n",
    "seed_everything(101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load graph data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 20.7793 seconds to load graph\n"
     ]
    }
   ],
   "source": [
    "KG_dir=\"/workspace/cjiang/eagle_project/CAP_graph/BGNN/\"\n",
    "\n",
    "start=time.time()\n",
    "with open(os.path.join(KG_dir,'CAP_Graph_07082021'), 'rb') as f:\n",
    "    hg,multi_label,binary_label,\\\n",
    "    train_mask_multi_label,  val_mask_multi_label,  test_mask_multi_label,\\\n",
    "    train_mask_binary_label, val_mask_binary_label, test_mask_binary_label= pickle.load(f)\n",
    "end=time.time()\n",
    "print(\"It took {:0.4f} seconds to load graph\".format(end-start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hg.nodes['usaanr'].data[\"binary_label\"]= binary_label\n",
    "hg.nodes['usaanr'].data[\"train_mask\"]= th.tensor( np.expand_dims(np.array(train_mask_binary_label), 1) )\n",
    "hg.nodes['usaanr'].data[\"val_mask\"]= th.tensor( np.expand_dims(np.array(val_mask_binary_label), 1) )\n",
    "hg.nodes['usaanr'].data[\"test_mask\"]= th.tensor( np.expand_dims(np.array(test_mask_binary_label), 1) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sampling data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Node_types:  ['usaanr']\n",
      "Edge_types:  ['AUTO_RELATED', 'Brother_Sister', 'Busi_rel_Other', 'Child', 'Ex-Spouse', 'Parent', 'Pers_rel_Other', 'SPONSEE', 'SPONSOR', 'Spouse', 'Step-Child', 'Step-Parent']\n",
      "**************************************************\n",
      "Canonical Etypes of Graph is:\n",
      "\n",
      "usaanr              AUTO_RELATED        usaanr              \n",
      "usaanr              Brother_Sister      usaanr              \n",
      "usaanr              Busi_rel_Other      usaanr              \n",
      "usaanr              Child               usaanr              \n",
      "usaanr              Ex-Spouse           usaanr              \n",
      "usaanr              Parent              usaanr              \n",
      "usaanr              Pers_rel_Other      usaanr              \n",
      "usaanr              SPONSEE             usaanr              \n",
      "usaanr              SPONSOR             usaanr              \n",
      "usaanr              Spouse              usaanr              \n",
      "usaanr              Step-Child          usaanr              \n",
      "usaanr              Step-Parent         usaanr              \n",
      "**************************************************\n",
      "number of ntype=usaanr                130,620        \n",
      "**************************************************\n",
      "Total number of nodes is 130,620\n",
      "**************************************************\n",
      "number of etype=AUTO_RELATED          10,000         \n",
      "number of etype=Brother_Sister        10,000         \n",
      "number of etype=Busi_rel_Other        10,000         \n",
      "number of etype=Child                 10,000         \n",
      "number of etype=Ex-Spouse             10,000         \n",
      "number of etype=Parent                10,000         \n",
      "number of etype=Pers_rel_Other        10,000         \n",
      "number of etype=SPONSEE               10,000         \n",
      "number of etype=SPONSOR               10,000         \n",
      "number of etype=Spouse                10,000         \n",
      "number of etype=Step-Child            10,000         \n",
      "number of etype=Step-Parent           10,000         \n",
      "**************************************************\n",
      "Total number of edges is 120,000\n",
      "**************************************************\n",
      "**************************************************\n",
      "The attributes for the node type=usaanr\n",
      "**************************************************\n",
      "usaayr                                  torch.Size([130620, 1])\n",
      "AGE                                     torch.Size([130620, 1])\n",
      "AGE_BAND                                torch.Size([130620, 1])\n",
      "ORIGEL                                  torch.Size([130620, 1])\n",
      "ELIG2                                   torch.Size([130620, 1])\n",
      "cmpyelig                                torch.Size([130620, 1])\n",
      "SEX                                     torch.Size([130620, 1])\n",
      "MARST                                   torch.Size([130620, 1])\n",
      "PERSST                                  torch.Size([130620, 1])\n",
      "DEATHSDT                                torch.Size([130620, 1])\n",
      "BRANCH                                  torch.Size([130620, 1])\n",
      "MILST                                   torch.Size([130620, 1])\n",
      "MLIST_OrigStat                          torch.Size([130620, 1])\n",
      "enl1stsdt                               torch.Size([130620, 1])\n",
      "COMMSDT                                 torch.Size([130620, 1])\n",
      "ENLPAYGD                                torch.Size([130620, 1])\n",
      "ACTCORP                                 torch.Size([130620, 1])\n",
      "STATE                                   torch.Size([130620, 1])\n",
      "Segment                                 torch.Size([130620, 1])\n",
      "ZIPCD                                   torch.Size([130620, 1])\n",
      "binary_label                            torch.Size([130620, 1])\n",
      "train_mask                              torch.Size([130620, 1])\n",
      "val_mask                                torch.Size([130620, 1])\n",
      "test_mask                               torch.Size([130620, 1])\n",
      "_ID                                     torch.Size([130620])\n"
     ]
    }
   ],
   "source": [
    "dict_edges={}\n",
    "for etype in hg.etypes:\n",
    "    dict_edges[etype]=th.arange(hg.num_edges(etype))[0:10000]\n",
    "G=dgl.edge_subgraph(hg,dict_edges)\n",
    "\n",
    "# binary_label=G.ndata.pop(\"binary_label\")\n",
    "# train_mask=G.ndata.pop(\"train_mask\")\n",
    "# val_mask=G.ndata.pop(\"val_mask\")\n",
    "# test_mask=G.ndata.pop(\"test_mask\")\n",
    "\n",
    "utils.graph_show(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### convert Heterogeneous Graph into Homogeneous Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.edges['Parent'].data[\"etype\"]=th.zeros(G.num_edges(\"Parent\"))\n",
    "G.edges['Child'].data[\"etype\"]=th.ones(G.num_edges(\"Child\"))\n",
    "G.edges['Spouse'].data[\"etype\"]=th.ones(G.num_edges(\"Spouse\"))*2\n",
    "G.edges['Ex-Spouse'].data[\"etype\"]=th.ones(G.num_edges(\"Ex-Spouse\"))*3\n",
    "G.edges['Brother_Sister'].data[\"etype\"]=th.ones(G.num_edges(\"Brother_Sister\"))*4\n",
    "G.edges['Step-Parent'].data[\"etype\"]=th.ones(G.num_edges(\"Step-Parent\"))*5\n",
    "G.edges['Step-Child'].data[\"etype\"]=th.ones(G.num_edges(\"Step-Child\"))*6\n",
    "G.edges['Pers_rel_Other'].data[\"etype\"]=th.ones(G.num_edges(\"Pers_rel_Other\"))*7\n",
    "G.edges['SPONSOR'].data[\"etype\"]=th.ones(G.num_edges(\"SPONSOR\"))*8\n",
    "G.edges['SPONSEE'].data[\"etype\"]=th.ones(G.num_edges(\"SPONSEE\"))*9\n",
    "G.edges['AUTO_RELATED'].data[\"etype\"]=th.ones(G.num_edges(\"AUTO_RELATED\"))*10\n",
    "G.edges['Busi_rel_Other'].data[\"etype\"]=th.ones(G.num_edges(\"Busi_rel_Other\"))*11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate norm for each edge type and store in edge\n",
    "for canonical_etype in G.canonical_etypes:\n",
    "    u, v, eid = G.all_edges(form='all', etype=canonical_etype)\n",
    "    _, inverse_index, count = th.unique(v, return_inverse=True, return_counts=True)\n",
    "    degrees = count[inverse_index]\n",
    "    norm = th.ones(eid.shape[0]).float() / degrees.float()\n",
    "    norm = norm.unsqueeze(1)\n",
    "    G.edges[canonical_etype].data['norm'] = norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "usaanr_feat=[]\n",
    "for key, scheme in G.node_attr_schemes(ntype=\"usaanr\").items():\n",
    "    usaanr_feat.append(key)\n",
    "# usaanr_feat=[x for x in usaanr_feat if x not in ['ZIPCD','AGE','train_mask','val_mask','test_mask']]\n",
    "\n",
    "# print()\n",
    "# print(\"The features associated with USAA Member are\\n \")\n",
    "# for i in usaanr_feat:\n",
    "#     print(i)\n",
    "\n",
    "g, ntype_count, etype_count=dgl.to_homogeneous(G,ndata=usaanr_feat,edata=['norm','etype'],store_type=True,return_count=True)\n",
    "\n",
    "num_nodes=g.num_nodes()\n",
    "node_ids=th.arange(num_nodes)\n",
    "edge_norm=g.edata['norm']\n",
    "edge_type=g.edata['etype'].long()\n",
    "\n",
    "g.ndata['ntype']=g.ndata.pop(dgl.NTYPE)\n",
    "\n",
    "num_rels=g.edata['etype'].unique().max().item()+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "norm\n",
      "etype\n"
     ]
    }
   ],
   "source": [
    "### Remove some features from edges \n",
    "_ID=g.edata.pop(\"_ID\")\n",
    "_TYPE=g.edata.pop(\"_TYPE\")\n",
    "\n",
    "for key, val in g.edge_attr_schemes().items():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usaayr\n",
      "AGE_BAND\n",
      "ORIGEL\n",
      "ELIG2\n",
      "cmpyelig\n",
      "SEX\n",
      "MARST\n",
      "BRANCH\n",
      "MILST\n",
      "MLIST_OrigStat\n",
      "ENLPAYGD\n",
      "ACTCORP\n",
      "STATE\n",
      "Segment\n"
     ]
    }
   ],
   "source": [
    "### Remove some features from nodes so that the graph only contain the features used in the model\n",
    "zipcd=g.ndata.pop(\"ZIPCD\")\n",
    "AGE=g.ndata.pop(\"AGE\")\n",
    "train_mask=g.ndata.pop(\"train_mask\")\n",
    "val_mask=g.ndata.pop(\"val_mask\")\n",
    "test_mask=g.ndata.pop(\"test_mask\")\n",
    "_ID=g.ndata.pop(\"_ID\")\n",
    "ntype=g.ndata.pop(\"ntype\")\n",
    "binary_label=g.ndata.pop(\"binary_label\")\n",
    "\n",
    "PERSST=g.ndata.pop(\"PERSST\")\n",
    "DEATHSDT=g.ndata.pop(\"DEATHSDT\")\n",
    "enl1stsdt=g.ndata.pop(\"enl1stsdt\")\n",
    "COMMSDT=g.ndata.pop(\"COMMSDT\")\n",
    "\n",
    "for key, val in g.node_attr_schemes().items():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"CAP_Graph_Structure.PNG\" alt=\"drawing\" width=\"750\"/> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_list=[]\n",
    "for key, scheme in g.node_attr_schemes().items():\n",
    "    feat_list.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## USAA Members Features Embedding\n",
    "class USAANR_Embedding(nn.Module):\n",
    "    def __init__(self,G,feature_size,feat_list):\n",
    "        super(USAANR_Embedding,self).__init__()\n",
    "#         self.device=G.device\n",
    "        self.G=G.to(device)\n",
    "        self.feature_size=feature_size\n",
    "        self.feat_list=feat_list\n",
    "        ## Embedding matrices for features of nodes.\n",
    "        self.emb = nn.ModuleDict()\n",
    "        \n",
    "        for i,col in enumerate(self.feat_list):\n",
    "            self.emb[col]=nn.Embedding(G.ndata[col].max().item()+1, feature_size)\n",
    "    \n",
    "    def forward(self, nid):\n",
    "        nid=nid.to(device)\n",
    "        extra_repr=[]\n",
    "        \n",
    "        for i,col in enumerate(self.feat_list):\n",
    "            ndata=self.G.ndata[col]\n",
    "            extra_repr.append(self.emb[col](ndata[nid]).squeeze(1))\n",
    "        return th.stack(extra_repr, 0).sum(0)\n",
    "    \n",
    "class RelGraphConv(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_feat,\n",
    "                 out_feat,\n",
    "                 num_rels,\n",
    "                 regularizer=\"basis\",\n",
    "                 num_bases=None,\n",
    "                 bias=True,\n",
    "                 activation=None,\n",
    "                 self_loop=True,\n",
    "                 low_mem=False,\n",
    "                 dropout=0.0,\n",
    "                 layer_norm=False):\n",
    "        super(RelGraphConv, self).__init__()\n",
    "        self.in_feat = in_feat\n",
    "        self.out_feat = out_feat\n",
    "        self.num_rels = num_rels\n",
    "        self.regularizer = regularizer\n",
    "        self.num_bases = num_bases\n",
    "        if self.num_bases is None or self.num_bases > self.num_rels or self.num_bases <= 0:\n",
    "            self.num_bases = self.num_rels\n",
    "        self.bias = bias\n",
    "        self.activation = activation\n",
    "        self.self_loop = self_loop\n",
    "        self.low_mem = low_mem\n",
    "        self.layer_norm = layer_norm\n",
    "\n",
    "        if regularizer == \"basis\":\n",
    "            # add basis weights\n",
    "            self.weight = nn.Parameter(th.Tensor(self.num_bases, self.in_feat, self.out_feat))\n",
    "            if self.num_bases < self.num_rels:\n",
    "                # linear combination coefficients\n",
    "                self.w_comp = nn.Parameter(th.Tensor(self.num_rels, self.num_bases))\n",
    "            nn.init.xavier_uniform_(self.weight, gain=nn.init.calculate_gain('relu'))\n",
    "            if self.num_bases < self.num_rels:\n",
    "                nn.init.xavier_uniform_(self.w_comp,\n",
    "                                        gain=nn.init.calculate_gain('relu'))\n",
    "            # message func\n",
    "            self.message_func = self.basis_message_func\n",
    "        else:\n",
    "            raise ValueError(\"Regularizer must be 'basis' \")\n",
    "\n",
    "        # bias\n",
    "        if self.bias:\n",
    "            self.h_bias = nn.Parameter(th.Tensor(out_feat))\n",
    "            nn.init.zeros_(self.h_bias)\n",
    "\n",
    "        # layer norm\n",
    "        if self.layer_norm:\n",
    "            self.layer_norm_weight = nn.LayerNorm(out_feat, elementwise_affine=True)\n",
    "\n",
    "        # weight for self loop\n",
    "        if self.self_loop:\n",
    "            self.loop_weight = nn.Parameter(th.Tensor(in_feat, out_feat))\n",
    "            nn.init.xavier_uniform_(self.loop_weight,\n",
    "                                    gain=nn.init.calculate_gain('relu'))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def basis_message_func(self, edges, etypes, e_weights=None):\n",
    "        \"\"\"Message function for basis regularizer.\n",
    "        Parameters\n",
    "        ----------\n",
    "        edges : dgl.EdgeBatch\n",
    "            Input to DGL message UDF.\n",
    "        etypes : torch.Tensor or list[int]\n",
    "            Edge type data. Could be either:\n",
    "                * An :math:`(|E|,)` dense tensor. Each element corresponds to the edge's type ID.\n",
    "                  Preferred format if ``lowmem == False``.\n",
    "                * An integer list. The i^th element is the number of edges of the i^th type.\n",
    "                  This requires the input graph to store edges sorted by their type IDs.\n",
    "                  Preferred format if ``lowmem == True``.\n",
    "        \"\"\"\n",
    "        if self.num_bases < self.num_rels:\n",
    "            # generate all weights from bases\n",
    "            weight = self.weight.view(self.num_bases,\n",
    "                                      self.in_feat * self.out_feat)\n",
    "            weight = th.matmul(self.w_comp, weight).view(\n",
    "                self.num_rels, self.in_feat, self.out_feat)\n",
    "        else:\n",
    "            weight = self.weight\n",
    "\n",
    "        h = edges.src['h']\n",
    "        device = h.device\n",
    "        \n",
    "        if \"e_weights\" in edges.data.keys():\n",
    "            h=h*edges.data['e_weights']\n",
    "\n",
    "        if self.low_mem:\n",
    "            # A more memory-friendly implementation.\n",
    "            # Calculate msg @ W_r before put msg into edge.\n",
    "            assert isinstance(etypes, list)\n",
    "            h_t = th.split(h, etypes)\n",
    "            msg = []\n",
    "            for etype in range(self.num_rels):\n",
    "                if h_t[etype].shape[0] == 0:\n",
    "                    continue\n",
    "                msg.append(th.matmul(h_t[etype], weight[etype]))\n",
    "            msg = th.cat(msg)\n",
    "        else:\n",
    "            # Use batched matmult\n",
    "            weight = weight.index_select(0, etypes)\n",
    "            msg = th.bmm(h.unsqueeze(1), weight).squeeze(1)\n",
    "\n",
    "        if 'norm' in edges.data:\n",
    "            msg = msg * edges.data['norm']\n",
    "        return {'msg': msg}\n",
    "    \n",
    "    def forward(self, g, feat, etypes, norm=None):\n",
    "        \"\"\"Forward computation.\n",
    "        Parameters\n",
    "        ----------\n",
    "        g : DGLGraph\n",
    "            The graph.\n",
    "        feat : torch.Tensor\n",
    "            Input node features. Could be either\n",
    "                * :math:`(|V|, D)` dense tensor\n",
    "                * :math:`(|V|,)` int64 vector, representing the categorical values of each\n",
    "                  node. It then treat the input feature as an one-hot encoding feature.\n",
    "        etypes : torch.Tensor or list[int]\n",
    "            Edge type data. Could be either\n",
    "                * An :math:`(|E|,)` dense tensor. Each element corresponds to the edge's type ID.\n",
    "                  Preferred format if ``lowmem == False``.\n",
    "                * An integer list. The i^th element is the number of edges of the i^th type.\n",
    "                  This requires the input graph to store edges sorted by their type IDs.\n",
    "                  Preferred format if ``lowmem == True``.\n",
    "        norm : torch.Tensor\n",
    "            Optional edge normalizer tensor. Shape: :math:`(|E|, 1)`.\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            New node features.\n",
    "        Notes\n",
    "        -----\n",
    "        Under the ``low_mem`` mode, DGL will sort the graph based on the edge types\n",
    "        and compute message passing one type at a time. DGL recommends sorts the\n",
    "        graph beforehand (and cache it if possible) and provides the integer list\n",
    "        format to the ``etypes`` argument. Use DGL's :func:`~dgl.to_homogeneous` API\n",
    "        to get a sorted homogeneous graph from a heterogeneous graph. Pass ``return_count=True``\n",
    "        to it to get the ``etypes`` in integer list.\n",
    "        \"\"\"\n",
    "        if isinstance(etypes, th.Tensor):\n",
    "            if len(etypes) != g.num_edges():\n",
    "                raise DGLError('\"etypes\" tensor must have length equal to the number of edges'\n",
    "                               ' in the graph. But got {} and {}.'.format(\n",
    "                                   len(etypes), g.num_edges()))\n",
    "            if self.low_mem and not (feat.dtype == th.int64 and feat.ndim == 1):\n",
    "                # Low-mem optimization is not enabled for node ID input. When enabled,\n",
    "                # it first sorts the graph based on the edge types (the sorting will not\n",
    "                # change the node IDs). It then converts the etypes tensor to an integer\n",
    "                # list, where each element is the number of edges of the type.\n",
    "                # Sort the graph based on the etypes\n",
    "                sorted_etypes, index = th.sort(etypes)\n",
    "                g = edge_subgraph(g, index, preserve_nodes=True)\n",
    "                # Create a new etypes to be an integer list of number of edges.\n",
    "                pos = th.searchsorted(sorted_etypes, th.arange(self.num_rels, device=g.device))\n",
    "                num = th.tensor([len(etypes)], device=g.device)\n",
    "                etypes = (th.cat([pos[1:], num]) - pos).tolist()\n",
    "                if norm is not None:\n",
    "                    norm = norm[index]\n",
    "\n",
    "        with g.local_scope():\n",
    "            g.srcdata['h'] = feat\n",
    "            if norm is not None:\n",
    "                g.edata['norm'] = norm\n",
    "            if self.self_loop:\n",
    "                loop_message = utils.matmul_maybe_select(feat[:g.number_of_dst_nodes()],\n",
    "                                                         self.loop_weight)\n",
    "            # message passing\n",
    "            g.update_all(functools.partial(self.message_func, etypes=etypes),\n",
    "                             fn.sum(msg='msg', out='h'))\n",
    "            # apply bias and activation\n",
    "            node_repr = g.dstdata['h']\n",
    "            if self.layer_norm:\n",
    "                node_repr = self.layer_norm_weight(node_repr)\n",
    "            if self.bias:\n",
    "                node_repr = node_repr + self.h_bias\n",
    "            if self.self_loop:\n",
    "                node_repr = node_repr + loop_message\n",
    "            if self.activation:\n",
    "                node_repr = self.activation(node_repr)\n",
    "            node_repr = self.dropout(node_repr)\n",
    "            \n",
    "            return node_repr\n",
    "        \n",
    "class EntityClassify(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 g,\n",
    "                 feat_list,\n",
    "                 h_dim,\n",
    "                 out_dim,\n",
    "                 num_rels,\n",
    "                 num_bases=None,\n",
    "                 num_hidden_layers=1,\n",
    "                 dropout=0,\n",
    "                 use_self_loop=False,\n",
    "                 low_mem=True,\n",
    "                 layer_norm=False):\n",
    "        super(EntityClassify, self).__init__()\n",
    "        self.g=g\n",
    "        self.feat_list=feat_list\n",
    "        self.h_dim = h_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.num_rels = num_rels\n",
    "        self.num_bases = None if num_bases < 0 else num_bases\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.dropout = dropout\n",
    "        self.use_self_loop = use_self_loop\n",
    "        self.low_mem = low_mem\n",
    "        self.layer_norm = layer_norm\n",
    "        \n",
    "        self.node_embed = USAANR_Embedding(self.g,self.h_dim,self.feat_list)\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "        # i2h\n",
    "        self.layers.append(RelGraphConv(\n",
    "            self.h_dim, self.h_dim, self.num_rels, \"basis\",\n",
    "            self.num_bases, activation=F.relu, self_loop=self.use_self_loop,\n",
    "            low_mem=self.low_mem, dropout=self.dropout, layer_norm = layer_norm))\n",
    "        # h2h\n",
    "        for idx in range(self.num_hidden_layers):\n",
    "            self.layers.append(RelGraphConv(\n",
    "                self.h_dim, self.h_dim, self.num_rels, \"basis\",\n",
    "                self.num_bases, activation=F.relu, self_loop=self.use_self_loop,\n",
    "                low_mem=self.low_mem, dropout=self.dropout, layer_norm = layer_norm))\n",
    "        # h2o\n",
    "#         self.layers.append(RelGraphConv(\n",
    "#             self.h_dim, self.out_dim, self.num_rels, \"basis\",\n",
    "#             self.num_bases, activation=None,\n",
    "#             self_loop=self.use_self_loop,\n",
    "#             low_mem=self.low_mem, layer_norm = layer_norm))\n",
    "\n",
    "        self.classifier = nn.Linear(self.h_dim, self.out_dim)\n",
    "\n",
    "    def forward(self, blocks, input_nodes):\n",
    "        \n",
    "        H=self.node_embed(input_nodes)\n",
    "        \n",
    "        if not isinstance(blocks,list):\n",
    "            # full graph training\n",
    "            for layer in self.layers:\n",
    "                H = layer(blocks, H)\n",
    "            \n",
    "        else:\n",
    "            for layer, block in zip(self.layers, blocks):\n",
    "                block = block\n",
    "                H = layer(block, H, block.edata['etype'], block.edata['norm'])\n",
    "        \n",
    "        logits = self.classifier(H)\n",
    "        \n",
    "        return logits, H\n",
    "\n",
    "def lift_gain_eval(logit,label,topk):\n",
    "    DF=pd.DataFrame(columns=[\"pred_score\",\"actual_label\"])\n",
    "    DF[\"pred_score\"]=logit\n",
    "    DF[\"actual_label\"]=label\n",
    "    DF.sort_values(by=\"pred_score\", ascending=False, inplace=True)\n",
    "    gain={}\n",
    "    for p in topk:\n",
    "        N=math.ceil(int(DF.shape[0]*p))\n",
    "        DF2=DF.nlargest(N,\"pred_score\",keep=\"first\")\n",
    "        gain[str(int(p*100))+\"%\"]=round(DF2.actual_label.sum()/(DF.actual_label.sum()),2)\n",
    "    return gain\n",
    "\n",
    "def get_class_count_weight(y,n_classes):\n",
    "    classes_count=[]\n",
    "    weight=[]\n",
    "    for i in range(n_classes):\n",
    "        count=np.sum(y.squeeze()==i)\n",
    "        classes_count.append(count)\n",
    "        weight.append(len(y)/(n_classes*count))\n",
    "    return classes_count,weight\n",
    "\n",
    "def eval_loop_func(model, loader, labels, device, loss_weight, num_classes):\n",
    "    model.eval()\n",
    "    fin_targets=[]\n",
    "    fin_outputs=[]\n",
    "    losses=[]\n",
    "    for input_nodes, seeds, blocks in tqdm(loader, position=0, leave=True):\n",
    "        blocks = [blk.to(device) for blk in blocks]\n",
    "        seeds = seeds.to(device)\n",
    "        \n",
    "        input_nodes=input_nodes.to(device)\n",
    "\n",
    "        lbl = labels[seeds].squeeze().to(device)\n",
    "        \n",
    "        with th.no_grad():\n",
    "            logits,h = model(blocks,input_nodes)\n",
    "            if loss_weight is None:\n",
    "                loss = F.cross_entropy(logits.view(-1, num_classes), lbl.to(device))\n",
    "            else:\n",
    "                loss = F.cross_entropy(logits.view(-1, num_classes), lbl.to(device),weight=loss_weight.float())        \n",
    "            losses.append(loss.item())\n",
    "        fin_targets.append(lbl.cpu().detach().numpy())\n",
    "        fin_outputs.append(logits.cpu().detach().numpy())\n",
    "    return np.concatenate(fin_outputs), np.concatenate(fin_targets), losses\n",
    "\n",
    "def evaluate(target, predicted):\n",
    "    true_label_mask=[1 if (np.argmax(x)-target[i])==0 else 0 for i,x in enumerate(predicted)]\n",
    "    nb_prediction=len(true_label_mask)\n",
    "    true_prediction=sum(true_label_mask)\n",
    "    false_prediction=nb_prediction-true_prediction\n",
    "    accuracy=true_prediction/nb_prediction\n",
    "    \n",
    "    precision, recall, fscore, support = precision_recall_fscore_support(target, predicted.argmax(axis=1))\n",
    "    auc = roc_auc_score(target.ravel(), th.sigmoid(th.from_numpy(predicted))[:,1].numpy().ravel())\n",
    "    \n",
    "    prec,rec,_ = precision_recall_curve(target.ravel(), th.sigmoid(th.from_numpy(predicted))[:,1].numpy().ravel())\n",
    "    \n",
    "    pr_auc=auc_score(rec,prec)\n",
    "    \n",
    "    arg1=predicted[:,1]\n",
    "    arg2=target\n",
    "    gain = lift_gain_eval(arg1,arg2,topk=[0.01,0.05,0.10])\n",
    "    \n",
    "    return {\n",
    "        \"nb_example\":len(target),\n",
    "        \"true_prediction\":true_prediction,\n",
    "        \"false_prediction\":false_prediction,\n",
    "        \"accuracy\":accuracy,\n",
    "        \"precision\":precision[1], \n",
    "        \"recall\":recall[1], \n",
    "        \"f1_score\":fscore[1],\n",
    "        \"AUC\":auc,\n",
    "        \"pr_auc\":pr_auc,\n",
    "        \"GAIN\":gain\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=10240, dropout=0.2, fanout=None, gpu=0, h_dim=64, l2norm=0.001, layer_norm=True, loss_weight=True, low_mem=True, lr=0.001, n_epochs=5, num_bases=5, num_layers=1, num_mini_batch=8, num_worker=0, out_dim=1, seed=101, use_self_loop=True)\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='RGCN')\n",
    "parser.add_argument(\"--dropout\", type=float, default=0,\n",
    "        help=\"dropout probability\")\n",
    "parser.add_argument(\"--h_dim\", type=int, default=128,\n",
    "        help=\"number of hidden units\")\n",
    "parser.add_argument(\"--out_dim\", type=int, default=1,\n",
    "        help=\"output dimension\")\n",
    "parser.add_argument(\"--gpu\", type=int, default=0,\n",
    "        help=\"gpu\")\n",
    "parser.add_argument(\"--lr\", type=float, default=1e-5,\n",
    "        help=\"learning rate\")\n",
    "parser.add_argument(\"--num_bases\", type=int, default=-1,\n",
    "        help=\"number of filter weight matrices, default: -1 [use all]\")\n",
    "parser.add_argument(\"--num_layers\", type=int, default=1,\n",
    "        help=\"number of propagation rounds\")\n",
    "parser.add_argument(\"-e\", \"--n_epochs\", type=int, default=1,\n",
    "        help=\"number of training epochs\")\n",
    "parser.add_argument(\"--l2norm\", type=float, default=0,\n",
    "        help=\"l2 norm coef\")\n",
    "parser.add_argument(\"--use_self_loop\", default=True, action='store_true',\n",
    "        help=\"include self feature as a special relation\")\n",
    "parser.add_argument(\"--batch-size\", type=int, default=1024,\n",
    "        help=\"Mini-batch size. If -1, use full graph training.\")\n",
    "parser.add_argument(\"--num_mini_batch\", type=int, default=8,\n",
    "        help=\"Number of minibatch.\")\n",
    "parser.add_argument(\"--fanout\", type=int, default=None,\n",
    "        help=\"Fan-out of neighbor sampling.\")\n",
    "parser.add_argument(\"--seed\",  type=int,default=101,\n",
    "        help=\"random seed for np.random.seed, torch.manual_seed and torch.cuda.manual_seed.\")\n",
    "\n",
    "parser.add_argument(\"--loss_weight\",  type=bool,default=True,  ## number of label=0/number of label=1\n",
    "        help=\"weight for unbalance data\")\n",
    "parser.add_argument(\"--num_worker\",  type=int,default=0,  \n",
    "        help=\"number of worker for neighbor sampling\") \n",
    "parser.add_argument('--low_mem',  action='store_true')\n",
    "parser.add_argument('--layer_norm',  action='store_true')\n",
    "args,unknown=parser.parse_known_args()\n",
    "\n",
    "args.num_layers=1\n",
    "args.dropout=0.2\n",
    "args.lr=1e-3\n",
    "args.l2norm=1e-3\n",
    "args.n_epochs=5\n",
    "args.num_bases=5\n",
    "args.h_dim=64\n",
    "args.batch_size=1024*10\n",
    "args.low_mem=True\n",
    "args.layer_norm=True\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "</style><table id=\"T_0dda2686_ea3e_11eb_92de_0242ac110003\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >Auto or Not</th>        <th class=\"col_heading level0 col1\" >count</th>        <th class=\"col_heading level0 col2\" >weight</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_0dda2686_ea3e_11eb_92de_0242ac110003level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "                        <td id=\"T_0dda2686_ea3e_11eb_92de_0242ac110003row0_col0\" class=\"data row0 col0\" >No</td>\n",
       "                        <td id=\"T_0dda2686_ea3e_11eb_92de_0242ac110003row0_col1\" class=\"data row0 col1\" >74,728</td>\n",
       "                        <td id=\"T_0dda2686_ea3e_11eb_92de_0242ac110003row0_col2\" class=\"data row0 col2\" >0.87</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0dda2686_ea3e_11eb_92de_0242ac110003level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "                        <td id=\"T_0dda2686_ea3e_11eb_92de_0242ac110003row1_col0\" class=\"data row1 col0\" >Yes</td>\n",
       "                        <td id=\"T_0dda2686_ea3e_11eb_92de_0242ac110003row1_col1\" class=\"data row1 col1\" >55,892</td>\n",
       "                        <td id=\"T_0dda2686_ea3e_11eb_92de_0242ac110003row1_col2\" class=\"data row1 col2\" >1.17</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f3268427438>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=binary_label.squeeze().numpy()\n",
    "n_classes=th.unique(binary_label).shape[0]\n",
    "classes_count,weight=get_class_count_weight(y,n_classes)\n",
    "imbalance_classes={}\n",
    "# imbalance_classes[\"category\"]=th.unique(binary_label).tolist()\n",
    "imbalance_classes[\"Auto or Not\"]=[\"No\",\"Yes\"]\n",
    "imbalance_classes[\"count\"]=classes_count\n",
    "imbalance_classes[\"weight\"]=weight\n",
    "imbalance_classes=pd.DataFrame(imbalance_classes)\n",
    "imbalance_classes.style.format({\"count\":\"{:,}\",\"weight\":\"{:.2f}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f3268427cf8>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.barplot(x=\"Auto or Not\",y=\"count\",data=imbalance_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### setting up training, validation and test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set    104,481   \n",
      "validation set  13,038    \n",
      "test set        13,101    \n"
     ]
    }
   ],
   "source": [
    "train_idx=th.nonzero(train_mask.squeeze()).numpy()\n",
    "val_idx=th.nonzero(val_mask.squeeze()).numpy()\n",
    "test_idx=th.nonzero(test_mask.squeeze()).numpy()\n",
    "\n",
    "train_idx=th.from_numpy(train_idx).squeeze()    \n",
    "val_idx=th.from_numpy(val_idx).squeeze()    \n",
    "test_idx=th.from_numpy(test_idx).squeeze()\n",
    "\n",
    "train_label=binary_label[train_idx].squeeze().numpy()\n",
    "val_label=binary_label[val_idx].squeeze().numpy()\n",
    "test_label=binary_label[test_idx].squeeze().numpy()\n",
    "\n",
    "print('{:<15} {:<10,}'.format(\"Training set\",train_idx.shape[0]))\n",
    "print('{:<15} {:<10,}'.format(\"validation set\",val_idx.shape[0]))\n",
    "print('{:<15} {:<10,}'.format(\"test set\",test_idx.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "##### check cuda\n",
    "device=\"cpu\"\n",
    "use_cuda=args.gpu>=0 and th.cuda.is_available()\n",
    "if use_cuda:\n",
    "    th.cuda.set_device(args.gpu)\n",
    "    device='cuda:%d' % args.gpu\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes=th.unique(th.from_numpy(train_label)).shape[0]\n",
    "if args.loss_weight:\n",
    "    train_classes_num, train_classes_weight = get_class_count_weight(train_label,num_classes)\n",
    "    loss_weight=th.tensor(train_classes_weight).to(device)\n",
    "else:\n",
    "    loss_weight=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0): RelGraphConv(\n",
       "    (layer_norm_weight): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (1): RelGraphConv(\n",
       "    (layer_norm_weight): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_rels=g.edata['etype'].long().unique().max().item()+1\n",
    "num_nodes=g.num_nodes()\n",
    "\n",
    "# create model\n",
    "model = EntityClassify(g,\n",
    "                       feat_list,\n",
    "                       args.h_dim,\n",
    "                       n_classes,\n",
    "                       num_rels,\n",
    "                       num_bases=args.num_bases,\n",
    "                       num_hidden_layers=args.num_layers,\n",
    "                       dropout=args.dropout,\n",
    "                       use_self_loop=args.use_self_loop,\n",
    "                       low_mem=args.low_mem,\n",
    "                       layer_norm=args.layer_norm)\n",
    "if use_cuda:\n",
    "    model.cuda()\n",
    "\n",
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = th.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.l2norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of minibatch in training set is 11\n",
      "The number of minibatch in validation set is 2\n",
      "The number of minibatch in test set is 2\n"
     ]
    }
   ],
   "source": [
    "# train sampler\n",
    "sampler = dgl.dataloading.MultiLayerNeighborSampler([args.fanout] * args.num_layers)\n",
    "train_loader = dgl.dataloading.NodeDataLoader(\n",
    "    g, train_idx, sampler,\n",
    "    batch_size=args.batch_size, shuffle=True, num_workers=args.num_worker)\n",
    "# validation sampler\n",
    "# we do not use full neighbor to save computation resources\n",
    "val_sampler = dgl.dataloading.MultiLayerNeighborSampler([args.fanout] * args.num_layers)\n",
    "val_loader = dgl.dataloading.NodeDataLoader(\n",
    "    g, val_idx, val_sampler,\n",
    "    batch_size=args.batch_size, shuffle=False, num_workers=args.num_worker)\n",
    "\n",
    "test_sampler = dgl.dataloading.MultiLayerNeighborSampler([args.fanout] * args.num_layers)\n",
    "test_loader = dgl.dataloading.NodeDataLoader(\n",
    "    g, test_idx, test_sampler,\n",
    "    batch_size=args.batch_size, shuffle=False, num_workers=args.num_worker)\n",
    "\n",
    "print(\"The number of minibatch in training set is {:,}\".format(len(train_loader)))\n",
    "print(\"The number of minibatch in validation set is {:,}\".format(len(val_loader)))\n",
    "print(\"The number of minibatch in test set is {:,}\".format(len(test_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total # of parameter is 67,706\n"
     ]
    }
   ],
   "source": [
    "print(\"The total # of parameter is {:,}\".format(sum([p.nelement() for p in model.parameters()]) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node_embed.emb.usaayr.weight                                          5,440          \n",
      "node_embed.emb.AGE_BAND.weight                                        448            \n",
      "node_embed.emb.ORIGEL.weight                                          2,752          \n",
      "node_embed.emb.ELIG2.weight                                           384            \n",
      "node_embed.emb.cmpyelig.weight                                        320            \n",
      "node_embed.emb.SEX.weight                                             320            \n",
      "node_embed.emb.MARST.weight                                           640            \n",
      "node_embed.emb.BRANCH.weight                                          1,088          \n",
      "node_embed.emb.MILST.weight                                           512            \n",
      "node_embed.emb.MLIST_OrigStat.weight                                  192            \n",
      "node_embed.emb.ENLPAYGD.weight                                        1,600          \n",
      "node_embed.emb.ACTCORP.weight                                         128            \n",
      "node_embed.emb.STATE.weight                                           3,712          \n",
      "node_embed.emb.Segment.weight                                         384            \n",
      "layers.0.weight                                                       20,480         \n",
      "layers.0.w_comp                                                       60             \n",
      "layers.0.h_bias                                                       64             \n",
      "layers.0.loop_weight                                                  4,096          \n",
      "layers.0.layer_norm_weight.weight                                     64             \n",
      "layers.0.layer_norm_weight.bias                                       64             \n",
      "layers.1.weight                                                       20,480         \n",
      "layers.1.w_comp                                                       60             \n",
      "layers.1.h_bias                                                       64             \n",
      "layers.1.loop_weight                                                  4,096          \n",
      "layers.1.layer_norm_weight.weight                                     64             \n",
      "layers.1.layer_norm_weight.bias                                       64             \n",
      "classifier.weight                                                     128            \n",
      "classifier.bias                                                       2              \n"
     ]
    }
   ],
   "source": [
    "param_dict={n: p.nelement() for n, p in model.named_parameters()}\n",
    "for i,j in param_dict.items():\n",
    "    print(\"{:<70}{:<15,}\".format(i,j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training...\n",
      "\n",
      "========= Epoch 1 /5\n",
      "Training...\n",
      "Batch 1 of 11 | Loss 0.205  | Elapsed: 0:00:00\n",
      "Batch 2 of 11 | Loss 0.202  | Elapsed: 0:00:00\n",
      "Batch 3 of 11 | Loss 0.202  | Elapsed: 0:00:01\n",
      "Batch 4 of 11 | Loss 0.203  | Elapsed: 0:00:01\n",
      "Batch 5 of 11 | Loss 0.203  | Elapsed: 0:00:01\n",
      "Batch 6 of 11 | Loss 0.201  | Elapsed: 0:00:01\n",
      "Batch 7 of 11 | Loss 0.199  | Elapsed: 0:00:01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/11 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 8 of 11 | Loss 0.199  | Elapsed: 0:00:02\n",
      "Batch 9 of 11 | Loss 0.198  | Elapsed: 0:00:02\n",
      "Batch 10 of 11 | Loss 0.195  | Elapsed: 0:00:02\n",
      "\n",
      "\n",
      "Running Validation on training set\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 11/11 [00:00<00:00, 17.61it/s]\n",
      "100%|| 2/2 [00:00<00:00, 36.81it/s]\n",
      " 20%|        | 1/5 [00:03<00:13,  3.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_loss: 0.15 | True_Prediction: 99,815 | False_Prediction: 4,666 | accuracy: 95.53% |  precision: 92.51% | recall: 97.44% | F1_score: 94.91% | Gain_top-10%: 0.2 |    ROC_AUC: 93.7% | PR_AUC: 89.5% | Elapsed: 0:00:01\n",
      "\n",
      "\n",
      "Running Validation on validation set\n",
      "\n",
      "avg_loss: 0.14 | True_Prediction: 12,485 | False_Prediction: 553 | accuracy: 95.76% |  precision: 93.17% | recall: 97.30% | F1_score: 95.19% | Gain_top-10%: 0.2 |    ROC_AUC: 93.8% | PR_AUC: 90.1% | Elapsed: 0:00:00\n",
      "\n",
      "========= Epoch 2 /5\n",
      "Training...\n",
      "Batch 1 of 11 | Loss 0.191  | Elapsed: 0:00:00\n",
      "Batch 2 of 11 | Loss 0.189  | Elapsed: 0:00:00\n",
      "Batch 3 of 11 | Loss 0.186  | Elapsed: 0:00:01\n",
      "Batch 4 of 11 | Loss 0.184  | Elapsed: 0:00:01\n",
      "Batch 5 of 11 | Loss 0.183  | Elapsed: 0:00:01\n",
      "Batch 6 of 11 | Loss 0.182  | Elapsed: 0:00:01\n",
      "Batch 7 of 11 | Loss 0.179  | Elapsed: 0:00:01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/11 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 8 of 11 | Loss 0.177  | Elapsed: 0:00:01\n",
      "Batch 9 of 11 | Loss 0.177  | Elapsed: 0:00:01\n",
      "Batch 10 of 11 | Loss 0.176  | Elapsed: 0:00:02\n",
      "\n",
      "\n",
      "Running Validation on training set\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 11/11 [00:00<00:00, 17.04it/s]\n",
      "100%|| 2/2 [00:00<00:00, 36.24it/s]\n",
      " 40%|      | 2/5 [00:06<00:09,  3.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_loss: 0.13 | True_Prediction: 100,403 | False_Prediction: 4,078 | accuracy: 96.10% |  precision: 93.09% | recall: 98.16% | F1_score: 95.55% | Gain_top-10%: 0.2 |    ROC_AUC: 94.6% | PR_AUC: 90.6% | Elapsed: 0:00:01\n",
      "\n",
      "\n",
      "Running Validation on validation set\n",
      "\n",
      "avg_loss: 0.12 | True_Prediction: 12,554 | False_Prediction: 484 | accuracy: 96.29% |  precision: 93.60% | recall: 98.10% | F1_score: 95.80% | Gain_top-10%: 0.2 |    ROC_AUC: 94.7% | PR_AUC: 91.2% | Elapsed: 0:00:00\n",
      "\n",
      "========= Epoch 3 /5\n",
      "Training...\n",
      "Batch 1 of 11 | Loss 0.173  | Elapsed: 0:00:00\n",
      "Batch 2 of 11 | Loss 0.171  | Elapsed: 0:00:00\n",
      "Batch 3 of 11 | Loss 0.170  | Elapsed: 0:00:01\n",
      "Batch 4 of 11 | Loss 0.168  | Elapsed: 0:00:01\n",
      "Batch 5 of 11 | Loss 0.166  | Elapsed: 0:00:01\n",
      "Batch 6 of 11 | Loss 0.165  | Elapsed: 0:00:01\n",
      "Batch 7 of 11 | Loss 0.164  | Elapsed: 0:00:01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/11 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 8 of 11 | Loss 0.162  | Elapsed: 0:00:01\n",
      "Batch 9 of 11 | Loss 0.162  | Elapsed: 0:00:02\n",
      "Batch 10 of 11 | Loss 0.160  | Elapsed: 0:00:02\n",
      "\n",
      "\n",
      "Running Validation on training set\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 11/11 [00:00<00:00, 23.26it/s]\n",
      "100%|| 2/2 [00:00<00:00, 33.57it/s]\n",
      " 60%|    | 3/5 [00:09<00:06,  3.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_loss: 0.12 | True_Prediction: 100,701 | False_Prediction: 3,780 | accuracy: 96.38% |  precision: 93.48% | recall: 98.39% | F1_score: 95.88% | Gain_top-10%: 0.2 |    ROC_AUC: 95.4% | PR_AUC: 91.7% | Elapsed: 0:00:01\n",
      "\n",
      "\n",
      "Running Validation on validation set\n",
      "\n",
      "avg_loss: 0.11 | True_Prediction: 12,577 | False_Prediction: 461 | accuracy: 96.46% |  precision: 93.91% | recall: 98.17% | F1_score: 95.99% | Gain_top-10%: 0.2 |    ROC_AUC: 95.5% | PR_AUC: 92.3% | Elapsed: 0:00:00\n",
      "\n",
      "========= Epoch 4 /5\n",
      "Training...\n",
      "Batch 1 of 11 | Loss 0.158  | Elapsed: 0:00:00\n",
      "Batch 2 of 11 | Loss 0.156  | Elapsed: 0:00:00\n",
      "Batch 3 of 11 | Loss 0.155  | Elapsed: 0:00:01\n",
      "Batch 4 of 11 | Loss 0.155  | Elapsed: 0:00:01\n",
      "Batch 5 of 11 | Loss 0.154  | Elapsed: 0:00:01\n",
      "Batch 6 of 11 | Loss 0.152  | Elapsed: 0:00:01\n",
      "Batch 7 of 11 | Loss 0.153  | Elapsed: 0:00:01\n",
      "Batch 8 of 11 | Loss 0.152  | Elapsed: 0:00:01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|       | 3/11 [00:00<00:00, 22.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 9 of 11 | Loss 0.151  | Elapsed: 0:00:02\n",
      "Batch 10 of 11 | Loss 0.153  | Elapsed: 0:00:02\n",
      "\n",
      "\n",
      "Running Validation on training set\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 11/11 [00:00<00:00, 24.15it/s]\n",
      "100%|| 2/2 [00:00<00:00, 32.74it/s]\n",
      " 80%|  | 4/5 [00:12<00:03,  3.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_loss: 0.12 | True_Prediction: 100,888 | False_Prediction: 3,593 | accuracy: 96.56% |  precision: 93.66% | recall: 98.64% | F1_score: 96.08% | Gain_top-10%: 0.2 |    ROC_AUC: 96.0% | PR_AUC: 92.6% | Elapsed: 0:00:01\n",
      "\n",
      "\n",
      "Running Validation on validation set\n",
      "\n",
      "avg_loss: 0.11 | True_Prediction: 12,598 | False_Prediction: 440 | accuracy: 96.63% |  precision: 94.03% | recall: 98.42% | F1_score: 96.18% | Gain_top-10%: 0.2 |    ROC_AUC: 96.1% | PR_AUC: 93.0% | Elapsed: 0:00:00\n",
      "\n",
      "========= Epoch 5 /5\n",
      "Training...\n",
      "Batch 1 of 11 | Loss 0.151  | Elapsed: 0:00:00\n",
      "Batch 2 of 11 | Loss 0.150  | Elapsed: 0:00:00\n",
      "Batch 3 of 11 | Loss 0.148  | Elapsed: 0:00:01\n",
      "Batch 4 of 11 | Loss 0.148  | Elapsed: 0:00:01\n",
      "Batch 5 of 11 | Loss 0.147  | Elapsed: 0:00:01\n",
      "Batch 6 of 11 | Loss 0.146  | Elapsed: 0:00:01\n",
      "Batch 7 of 11 | Loss 0.146  | Elapsed: 0:00:01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/11 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 8 of 11 | Loss 0.144  | Elapsed: 0:00:01\n",
      "Batch 9 of 11 | Loss 0.142  | Elapsed: 0:00:01\n",
      "Batch 10 of 11 | Loss 0.141  | Elapsed: 0:00:02\n",
      "\n",
      "\n",
      "Running Validation on training set\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 11/11 [00:00<00:00, 24.69it/s]\n",
      "100%|| 2/2 [00:00<00:00, 35.82it/s]\n",
      "100%|| 5/5 [00:14<00:00,  2.98s/it]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_loss: 0.11 | True_Prediction: 101,027 | False_Prediction: 3,454 | accuracy: 96.69% |  precision: 93.80% | recall: 98.80% | F1_score: 96.23% | Gain_top-10%: 0.2 |    ROC_AUC: 96.4% | PR_AUC: 93.2% | Elapsed: 0:00:01\n",
      "\n",
      "\n",
      "Running Validation on validation set\n",
      "\n",
      "avg_loss: 0.10 | True_Prediction: 12,619 | False_Prediction: 419 | accuracy: 96.79% |  precision: 94.25% | recall: 98.56% | F1_score: 96.36% | Gain_top-10%: 0.2 |    ROC_AUC: 96.5% | PR_AUC: 93.6% | Elapsed: 0:00:00\n",
      "\n",
      "\n",
      "Running Validation in Test Dataset\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2/2 [00:00<00:00, 29.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_loss: 0.11 | True_Prediction: 12,644 | False_Prediction: 457 | accuracy: 96.51% |  precision: 93.55% | recall: 98.66% | F1_score: 96.04% | Gain_top-10%: 0.2 |ROC_AUC: 96.1% | PR_AUC: 92.6% | Elapsed: 0:00:00\n"
     ]
    }
   ],
   "source": [
    "# %pdb\n",
    "LOSS_EPOCH=[]\n",
    "LABEL_TRAIN=[]\n",
    "\n",
    "# training loop\n",
    "print(\"start training...\")\n",
    "\n",
    "total_loss=0\n",
    "losses=[]\n",
    "\n",
    "LOGIT_train=[]\n",
    "LABEL_train=[]\n",
    "\n",
    "# th.manual_seed(args.seed)\n",
    "# th.cuda.manual_seed(args.seed)\n",
    "# th.cuda.manual_seed_all(args.seed)\n",
    "# np.ranom.seed(args.seed)\n",
    "# random.seed(args.seed)\n",
    "# th.backends.cudnn.deterministic=True\n",
    "\n",
    "for epoch in tqdm(range(0,args.n_epochs)):\n",
    "    \n",
    "    model.train()\n",
    "    IDX=[]\n",
    "    H=[]\n",
    "    \n",
    "    #====================================#\n",
    "    #            Traning                 #\n",
    "    #====================================#\n",
    "    print(\"\")\n",
    "    print(\"========= Epoch {:} /{:}\".format(epoch+1,args.n_epochs))\n",
    "    print(\"Training...\")\n",
    "    t0 = time.time()\n",
    "    for step, (input_nodes, seeds, blocks) in enumerate(train_loader):\n",
    "        blocks = [blk.to(device) for blk in blocks]\n",
    "        \n",
    "        seeds=seeds.to(device)\n",
    "        \n",
    "        labels_train=binary_label[seeds].to(device)       \n",
    "        \n",
    "        input_nodes=input_nodes.to(device)\n",
    "        \n",
    "        logits,h = model(blocks,input_nodes)\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        if args.loss_weight :\n",
    "            loss = F.cross_entropy(logits.view(-1, num_classes), labels_train.squeeze().to(device))\n",
    "        else:\n",
    "            loss = F.cross_entropy(logits.view(-1, num_classes), labels_train.squeeze().to(device),weight=loss_weight.to(device))\n",
    "        \n",
    "        total_loss+=loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        arg1=logits[:,1].detach().cpu().numpy()\n",
    "        arg2=labels_train.cpu().numpy()\n",
    "                \n",
    "        train_gain = lift_gain_eval(arg1,arg2,topk=[0.01,0.05,0.10])\n",
    "        \n",
    "        train_acc = th.sum(logits.argmax(dim=1) == labels_train).item() / len(seeds)\n",
    "        precision, recall, fscore, support = precision_recall_fscore_support(labels_train.cpu().numpy(), logits.argmax(dim=1).cpu().numpy())\n",
    "\n",
    "        try:\n",
    "            train_auc = roc_auc_score(labels_train.detach().cpu().numpy().ravel(), th.sigmoid(logits)[:,1].detach().cpu().numpy().ravel())\n",
    "        except ValueError:\n",
    "            pass\n",
    "        \n",
    "        prec,rec,_ = precision_recall_curve(labels_train.detach().cpu().numpy().ravel(), th.sigmoid(logits)[:,1].detach().cpu().numpy().ravel())\n",
    "        if math.isnan(rec[0])==False:\n",
    "            train_pr_auc=auc_score(rec,prec)\n",
    "\n",
    "        IDX.extend(seeds.detach().cpu().numpy().tolist())\n",
    "        H.extend(h.detach().cpu().numpy().tolist())\n",
    "        LOGIT_train.extend(logits.detach().cpu().numpy().tolist())\n",
    "        LABEL_train.extend(binary_label[blocks[-1].dstdata[dgl.NID].cpu().numpy()].tolist())\n",
    "        \n",
    "        if step%(len(train_loader)//10)==0 and not step==0:\n",
    "\n",
    "            t1 = time.time()\n",
    "            elapsed=utils.format_time(t1-t0)\n",
    "            print(\"Batch {:} of {:} | Loss {:.3f}  | Elapsed: {:}\".\\\n",
    "                  format(step,len(train_loader),np.mean(losses[-10:]),elapsed)) \n",
    "            \n",
    "    LOSS_EPOCH.append(loss)\n",
    "\n",
    "    LABEL_TRAIN.append(binary_label[blocks[-1].ndata[dgl.NID]['_N'].cpu().numpy()])\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    print()\n",
    "    print(\"\")\n",
    "    print(\"Running Validation on training set\")\n",
    "    print(\"\")\n",
    "    fin_outputs, fin_targets, losses_tmp=eval_loop_func(model, train_loader, binary_label,  device, loss_weight, num_classes)\n",
    "    \n",
    "    avg_loss_train=np.mean(losses_tmp)\n",
    "    \n",
    "    tmp_mean_pool_train=evaluate(fin_targets.reshape(-1),fin_outputs)\n",
    "    \n",
    "    t2=time.time()\n",
    "    \n",
    "    print(\"avg_loss: {:.2f} | True_Prediction: {:,} | False_Prediction: {:,} | accuracy: {:.2%} |  precision: {:.2%} | recall: {:.2%} | F1_score: {:.2%} | Gain_top-10%: {:.1f} |\\\n",
    "    ROC_AUC: {:.1%} | PR_AUC: {:.1%} | Elapsed: {:}\".format(avg_loss_train, tmp_mean_pool_train[\"true_prediction\"], tmp_mean_pool_train[\"false_prediction\"], tmp_mean_pool_train[\"accuracy\"], \\\n",
    "                                                            tmp_mean_pool_train[\"precision\"], tmp_mean_pool_train[\"recall\"], tmp_mean_pool_train[\"f1_score\"], tmp_mean_pool_train[\"GAIN\"]['10%'], \\\n",
    "                                                            tmp_mean_pool_train[\"AUC\"], tmp_mean_pool_train[\"pr_auc\"], utils.format_time(t2-t1)))\n",
    "\n",
    "    #====================================#\n",
    "    #            Validation-set          #\n",
    "    #====================================#\n",
    "    \n",
    "    model.eval()\n",
    "    print()\n",
    "    print(\"\")\n",
    "    print(\"Running Validation on validation set\")\n",
    "    print(\"\")\n",
    "    \n",
    "    fin_outputs, fin_targets, losses_tmp=eval_loop_func(model, val_loader, binary_label,  device, loss_weight, num_classes)\n",
    "    \n",
    "    avg_loss_val=np.mean(losses_tmp)\n",
    "    \n",
    "    tmp_mean_pool_val=evaluate(fin_targets.reshape(-1),fin_outputs)\n",
    "    \n",
    "    t3=time.time()\n",
    "    \n",
    "    print(\"avg_loss: {:.2f} | True_Prediction: {:,} | False_Prediction: {:,} | accuracy: {:.2%} |  precision: {:.2%} | recall: {:.2%} | F1_score: {:.2%} | Gain_top-10%: {:.1f} |\\\n",
    "    ROC_AUC: {:.1%} | PR_AUC: {:.1%} | Elapsed: {:}\".format(avg_loss_val, tmp_mean_pool_val[\"true_prediction\"], tmp_mean_pool_val[\"false_prediction\"], tmp_mean_pool_val[\"accuracy\"], \\\n",
    "                                                            tmp_mean_pool_val[\"precision\"], tmp_mean_pool_val[\"recall\"], tmp_mean_pool_val[\"f1_score\"], tmp_mean_pool_val[\"GAIN\"]['10%'], \\\n",
    "                                                            tmp_mean_pool_val[\"AUC\"], tmp_mean_pool_val[\"pr_auc\"], utils.format_time(t3-t2)))\n",
    "    \n",
    "# if args.model_path is not None:\n",
    "#     th.save(model.state_dict(), args.model_path)\n",
    "    \n",
    "#====================================#\n",
    "#            Test-set                #\n",
    "#====================================#\n",
    "print()\n",
    "print(\"\")\n",
    "print(\"Running Validation in Test Dataset\")\n",
    "print(\"\")\n",
    "model.eval()\n",
    "\n",
    "fin_outputs, fin_targets, losses_tmp=eval_loop_func(model, test_loader, binary_label,  device, loss_weight, num_classes)\n",
    "    \n",
    "avg_loss_test=np.mean(losses_tmp)\n",
    "\n",
    "tmp_mean_pool_test=evaluate(fin_targets.reshape(-1),fin_outputs)\n",
    "\n",
    "t4=time.time()\n",
    "\n",
    "print(\"avg_loss: {:.2f} | True_Prediction: {:,} | False_Prediction: {:,} | accuracy: {:.2%} |  precision: {:.2%} | recall: {:.2%} | F1_score: {:.2%} | Gain_top-10%: {:.1f} |\\\n",
    "ROC_AUC: {:.1%} | PR_AUC: {:.1%} | Elapsed: {:}\".format(avg_loss_test, tmp_mean_pool_test[\"true_prediction\"], tmp_mean_pool_test[\"false_prediction\"], tmp_mean_pool_test[\"accuracy\"], \\\n",
    "                                                        tmp_mean_pool_test[\"precision\"], tmp_mean_pool_test[\"recall\"], tmp_mean_pool_test[\"f1_score\"], tmp_mean_pool_test[\"GAIN\"]['10%'], \\\n",
    "                                                        tmp_mean_pool_test[\"AUC\"], tmp_mean_pool_test[\"pr_auc\"], utils.format_time(t4-t3)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nb_example': 104481,\n",
       " 'true_prediction': 101027,\n",
       " 'false_prediction': 3454,\n",
       " 'accuracy': 0.966941357758827,\n",
       " 'precision': 0.9379823535664931,\n",
       " 'recall': 0.9879744709439032,\n",
       " 'f1_score': 0.9623295888319336,\n",
       " 'AUC': 0.963625678952222,\n",
       " 'pr_auc': 0.9317275666992473,\n",
       " 'GAIN': {'1%': 0.02, '5%': 0.11, '10%': 0.22}}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_mean_pool_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nb_example': 13038,\n",
       " 'true_prediction': 12619,\n",
       " 'false_prediction': 419,\n",
       " 'accuracy': 0.9678631691977297,\n",
       " 'precision': 0.9425072291205987,\n",
       " 'recall': 0.9855923159018143,\n",
       " 'f1_score': 0.9635683853577949,\n",
       " 'AUC': 0.9648474152054055,\n",
       " 'pr_auc': 0.9360147234413201,\n",
       " 'GAIN': {'1%': 0.02, '5%': 0.11, '10%': 0.22}}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_mean_pool_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nb_example': 13101,\n",
       " 'true_prediction': 12644,\n",
       " 'false_prediction': 457,\n",
       " 'accuracy': 0.9651171666285017,\n",
       " 'precision': 0.9354947652819994,\n",
       " 'recall': 0.9866429207479964,\n",
       " 'f1_score': 0.9603883158533415,\n",
       " 'AUC': 0.9611748044256668,\n",
       " 'pr_auc': 0.9257848436491132,\n",
       " 'GAIN': {'1%': 0.02, '5%': 0.11, '10%': 0.22}}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_mean_pool_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
