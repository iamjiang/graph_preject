{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --quiet --pre dgl-cu101\n",
    "# !pip install --quiet torch==1.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version is 1.6.0\n",
      "DGL version is 0.7a210520\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import itertools\n",
    "import os\n",
    "import numpy as np\n",
    "from numpy import save,load,savetxt,loadtxt,savez_compressed\n",
    "from sklearn.metrics import roc_auc_score, f1_score,average_precision_score\n",
    "from sklearn.metrics import precision_recall_fscore_support \n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import auc as auc_score\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "import time\n",
    "from tqdm import tqdm, tqdm_notebook,tnrange\n",
    "tqdm.pandas(position=0, leave=True)\n",
    "import math \n",
    "\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import dgl\n",
    "import dgl.nn as dglnn\n",
    "import dgl.function as fn\n",
    "from dgl.ops import edge_softmax\n",
    "\n",
    "from functools import partial\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import random\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize']=(5.0,4.0)\n",
    "plt.rcParams['image.interpolation']='nearest'\n",
    "plt.rcParams['image.cmap']='gray'\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import utils\n",
    "import tsne_func\n",
    "print(\"torch version is {}\".format(th.__version__))\n",
    "print(\"DGL version is {}\".format(dgl.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    th.manual_seed(seed)\n",
    "    th.cuda.manual_seed(seed)\n",
    "    th.cuda.manual_seed_all(seed)\n",
    "    th.backends.cudnn.deterministic = True\n",
    "    th.backends.cudnn.benchmark = False\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    \n",
    "seed_everything(101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 12.8735 seconds to load graph\n"
     ]
    }
   ],
   "source": [
    "data_dir=\"/workspace/cjiang/eagle_project/CAP_graph/CAP_Update/\"\n",
    "\n",
    "start=time.time()\n",
    "with open(os.path.join(data_dir,\"CAP_Graph_New\"),\"rb\") as f:\n",
    "    G,multi_label,binary_label,\\\n",
    "    train_mask_multi_label,  val_mask_multi_label,  test_mask_multi_label,\\\n",
    "    train_mask_binary_label, val_mask_binary_label, test_mask_binary_label=pickle.load(f)\n",
    "end=time.time()\n",
    "print(\"It took {:0.4f} seconds to load graph\".format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 10.2244 seconds to load graph\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "with open(os.path.join(data_dir,\"CAP_Graph\"),\"rb\") as f:\n",
    "    G_orig,multi_label_orig,binary_label_orig,\\\n",
    "    train_mask_multi_label_orig,  val_mask_multi_label_orig,  test_mask_multi_label_orig,\\\n",
    "    train_mask_binary_label_orig, val_mask_binary_label_orig, test_mask_binary_label_orig=pickle.load(f)\n",
    "end=time.time()\n",
    "print(\"It took {:0.4f} seconds to load graph\".format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 1]), tensor([19358913,  6309591]))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "th.unique(binary_label,return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 1]), tensor([19358913,  6309591]))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "th.unique(binary_label_orig,return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(15939498), 25668504)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "th.sum(th.eq(binary_label,binary_label_orig)), binary_label.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(17454180), tensor(21049994), tensor(21048350))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "th.sum(th.eq(train_mask_binary_label,train_mask_binary_label_orig)),\\\n",
    "th.sum(th.eq(val_mask_binary_label,val_mask_binary_label_orig)),\\\n",
    "th.sum(th.eq(test_mask_binary_label,test_mask_binary_label_orig))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(20534804), tensor(2566850), tensor(2566850))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "th.sum(train_mask_binary_label_orig),th.sum(val_mask_binary_label_orig),th.sum(test_mask_binary_label_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Boolean value of Tensor with more than one value is ambiguous",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-e7301752e6bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mtrain_mask_binary_label\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mtrain_mask_binary_label_orig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: Boolean value of Tensor with more than one value is ambiguous"
     ]
    }
   ],
   "source": [
    "assert train_mask_binary_label.squeeze()==train_mask_binary_label_orig.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Node_types:  ['usaanr']\n",
      "Edge_types:  ['AUTO_RELATED', 'Brother_Sister', 'Busi_rel_Other', 'Child', 'Ex-Spouse', 'Parent', 'Pers_rel_Other', 'SPONSEE', 'SPONSOR', 'Spouse', 'Step-Child', 'Step-Parent']\n",
      "**************************************************\n",
      "Canonical Etypes of Graph is:\n",
      "\n",
      "usaanr              AUTO_RELATED        usaanr              \n",
      "usaanr              Brother_Sister      usaanr              \n",
      "usaanr              Busi_rel_Other      usaanr              \n",
      "usaanr              Child               usaanr              \n",
      "usaanr              Ex-Spouse           usaanr              \n",
      "usaanr              Parent              usaanr              \n",
      "usaanr              Pers_rel_Other      usaanr              \n",
      "usaanr              SPONSEE             usaanr              \n",
      "usaanr              SPONSOR             usaanr              \n",
      "usaanr              Spouse              usaanr              \n",
      "usaanr              Step-Child          usaanr              \n",
      "usaanr              Step-Parent         usaanr              \n",
      "**************************************************\n",
      "number of ntype=usaanr                25,668,504     \n",
      "**************************************************\n",
      "Total number of nodes is 25,668,504\n",
      "**************************************************\n",
      "number of etype=AUTO_RELATED          7,947,060      \n",
      "number of etype=Brother_Sister        651,444        \n",
      "number of etype=Busi_rel_Other        1,039,214      \n",
      "number of etype=Child                 11,765,795     \n",
      "number of etype=Ex-Spouse             3,877,972      \n",
      "number of etype=Parent                11,765,795     \n",
      "number of etype=Pers_rel_Other        2,012,514      \n",
      "number of etype=SPONSEE               16,775,141     \n",
      "number of etype=SPONSOR               16,775,141     \n",
      "number of etype=Spouse                16,016,119     \n",
      "number of etype=Step-Child            1,240,193      \n",
      "number of etype=Step-Parent           1,240,193      \n",
      "**************************************************\n",
      "Total number of edges is 91,106,581\n",
      "**************************************************\n",
      "**************************************************\n",
      "The attributes for the node type=usaanr\n",
      "**************************************************\n",
      "usaayr                                  torch.Size([25668504, 1])\n",
      "AGE                                     torch.Size([25668504, 1])\n",
      "AGE_BAND                                torch.Size([25668504, 1])\n",
      "ORIGEL                                  torch.Size([25668504, 1])\n",
      "ELIG2                                   torch.Size([25668504, 1])\n",
      "cmpyelig                                torch.Size([25668504, 1])\n",
      "SEX                                     torch.Size([25668504, 1])\n",
      "MARST                                   torch.Size([25668504, 1])\n",
      "PERSST                                  torch.Size([25668504, 1])\n",
      "DEATHSDT                                torch.Size([25668504, 1])\n",
      "BRANCH                                  torch.Size([25668504, 1])\n",
      "MILST                                   torch.Size([25668504, 1])\n",
      "MLIST_OrigStat                          torch.Size([25668504, 1])\n",
      "enl1stsdt                               torch.Size([25668504, 1])\n",
      "COMMSDT                                 torch.Size([25668504, 1])\n",
      "ENLPAYGD                                torch.Size([25668504, 1])\n",
      "ACTCORP                                 torch.Size([25668504, 1])\n",
      "STATE                                   torch.Size([25668504, 1])\n",
      "Segment                                 torch.Size([25668504, 1])\n",
      "ZIPCD                                   torch.Size([25668504, 1])\n"
     ]
    }
   ],
   "source": [
    "utils.graph_show(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The features associated with USAA Member are\n",
      " \n",
      "usaayr\n",
      "AGE_BAND\n",
      "ORIGEL\n",
      "ELIG2\n",
      "cmpyelig\n",
      "SEX\n",
      "MARST\n",
      "BRANCH\n",
      "MILST\n",
      "MLIST_OrigStat\n",
      "ENLPAYGD\n",
      "ACTCORP\n",
      "STATE\n",
      "Segment\n"
     ]
    }
   ],
   "source": [
    "usaanr_feat=[]\n",
    "for key, scheme in G.node_attr_schemes(ntype=\"usaanr\").items():\n",
    "    usaanr_feat.append(key)\n",
    "usaanr_feat=[x for x in usaanr_feat if x not in ['ZIPCD','AGE','PERSST','DEATHSDT','enl1stsdt','COMMSDT']]\n",
    "\n",
    "print()\n",
    "print(\"The features associated with USAA Member are\\n \")\n",
    "for i in usaanr_feat:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## USAA Members Features Embedding\n",
    "class USAANR_Embedding(nn.Module):\n",
    "    def __init__(self,G,feature_size):\n",
    "        super(USAANR_Embedding,self).__init__()\n",
    "        self.G=G.to(device)\n",
    "        self.feature_size=feature_size\n",
    "        ## Embedding matrices for features of nodes.\n",
    "        self.emb = nn.ModuleDict()\n",
    "        \n",
    "        for i,col in enumerate(usaanr_feat):\n",
    "            self.emb[col]=nn.Embedding(G.nodes['usaanr'].data[col].max().item()+1, feature_size)\n",
    "    \n",
    "    def forward(self,nid):\n",
    "        nid=nid.to(device)\n",
    "        extra_repr=[]\n",
    "        for i,col in enumerate(usaanr_feat):\n",
    "            ndata=self.G.nodes['usaanr'].data[col]\n",
    "            extra_repr.append(self.emb[col](ndata[nid]).squeeze(1))\n",
    "        return th.stack(extra_repr, 0).sum(0)\n",
    "\n",
    "\n",
    "class RelGraphConvLayer(nn.Module):\n",
    "    r\"\"\"Relational graph convolution layer.\n",
    "    Parameters\n",
    "    ----------\n",
    "    in_feat : int\n",
    "        Input feature size.\n",
    "    out_feat : int\n",
    "        Output feature size.\n",
    "    rel_names : list[str]\n",
    "        Relation names.\n",
    "    num_bases : int, optional\n",
    "        Number of bases. If is none, use number of relations. Default: None.\n",
    "    weight : bool, optional\n",
    "        True if a linear layer is applied after message passing. Default: True\n",
    "    bias : bool, optional\n",
    "        True if bias is added. Default: True\n",
    "    activation : callable, optional\n",
    "        Activation function. Default: None\n",
    "    self_loop : bool, optional\n",
    "        True to include self loop message. Default: False\n",
    "    dropout : float, optional\n",
    "        Dropout rate. Default: 0.0\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 in_feat,\n",
    "                 out_feat,\n",
    "                 rel_names,\n",
    "                 num_bases,\n",
    "                 *,\n",
    "                 weight=True,\n",
    "                 bias=True,\n",
    "                 activation=None,\n",
    "                 self_loop=False,\n",
    "                 dropout=0.0):\n",
    "        super(RelGraphConvLayer, self).__init__()\n",
    "        self.in_feat = in_feat\n",
    "        self.out_feat = out_feat\n",
    "        self.rel_names = rel_names\n",
    "        self.num_bases = num_bases\n",
    "        self.bias = bias\n",
    "        self.activation = activation\n",
    "        self.self_loop = self_loop\n",
    "        self.conv = dglnn.HeteroGraphConv({\n",
    "                rel : dglnn.GraphConv(in_feat, out_feat, norm=\"both\", weight=False, bias=False)\n",
    "#                 rel : dglnn.SAGEConv(in_feat, out_feat, aggregator_type='mean',feat_drop=0.,bias=True,norm=None)\n",
    "                for rel in rel_names\n",
    "            })\n",
    "        self.use_weight = weight\n",
    "        self.use_basis = num_bases < len(self.rel_names) and weight\n",
    "        if self.use_weight:\n",
    "            if self.use_basis:\n",
    "                self.basis = dglnn.WeightBasis((in_feat, out_feat), num_bases, len(self.rel_names))\n",
    "            else:\n",
    "                self.weight = nn.Parameter(th.Tensor(len(self.rel_names), in_feat, out_feat))\n",
    "                nn.init.xavier_uniform_(self.weight, gain=nn.init.calculate_gain('relu'))\n",
    "        # bias\n",
    "        if bias:\n",
    "            self.h_bias = nn.Parameter(th.Tensor(out_feat))\n",
    "            nn.init.zeros_(self.h_bias)\n",
    "        # weight for self loop\n",
    "        if self.self_loop:\n",
    "            self.loop_weight = nn.Parameter(th.Tensor(in_feat, out_feat))\n",
    "            nn.init.xavier_uniform_(self.loop_weight,\n",
    "                                    gain=nn.init.calculate_gain('relu'))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, g, inputs):\n",
    "        \"\"\"Forward computation\n",
    "        Parameters\n",
    "        ----------\n",
    "        g : DGLHeteroGraph\n",
    "            Input graph.\n",
    "        inputs : dict[str, torch.Tensor]\n",
    "            Node feature for each node type.\n",
    "        Returns\n",
    "        -------\n",
    "        dict[str, torch.Tensor]\n",
    "            New node features for each node type.\n",
    "        \"\"\"\n",
    "        g = g.local_var()\n",
    "        if self.use_weight:\n",
    "            weight = self.basis() if self.use_basis else self.weight\n",
    "            wdict = {self.rel_names[i] : {'weight' : w.squeeze(0)}\n",
    "                     for i, w in enumerate(th.split(weight, 1, dim=0))}\n",
    "        else:\n",
    "            wdict = {}\n",
    "        if g.is_block:\n",
    "            inputs_src = inputs\n",
    "            inputs_dst = {k: v[:g.number_of_dst_nodes(k)] for k, v in inputs.items()}\n",
    "        else:\n",
    "            inputs_src = inputs_dst = inputs\n",
    "        hs = self.conv(g, inputs, mod_kwargs=wdict)\n",
    "        def _apply(ntype, h):\n",
    "            if self.self_loop:\n",
    "                h = h + th.matmul(inputs_dst[ntype], self.loop_weight)\n",
    "            if self.bias:\n",
    "                h = h + self.h_bias\n",
    "            if self.activation:\n",
    "                h = self.activation(h)\n",
    "            return self.dropout(h)\n",
    "        return {ntype : _apply(ntype, h) for ntype, h in hs.items()}\n",
    "    \n",
    "\n",
    "class Entity_Classify(nn.Module):\n",
    "    def __init__(self,\n",
    "                 g,\n",
    "                 h_dim,\n",
    "                 out_dim,\n",
    "                 num_bases,\n",
    "#                  embed_layer,\n",
    "                 num_hidden_layers=1,\n",
    "                 dropout=0,\n",
    "                 use_self_loop=False):\n",
    "        super(Entity_Classify, self).__init__()\n",
    "        self.g = g\n",
    "        self.h_dim = h_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.rel_names = list(set(g.etypes))\n",
    "#         self.num_bases = None if num_bases < 0 else num_bases\n",
    "        if num_bases < 0 or num_bases > len(self.rel_names):\n",
    "            self.num_bases = len(self.rel_names)\n",
    "        else:\n",
    "            self.num_bases = num_bases\n",
    "            \n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.dropout = dropout\n",
    "        self.use_self_loop = use_self_loop\n",
    "        \n",
    "#         self.node_embed={}\n",
    "        self.node_embed=nn.ModuleDict()\n",
    "        self.node_embed['usaanr'] = USAANR_Embedding(self.g,self.h_dim)\n",
    "#         self.node_embed['zipcode'] = Zipcode_Embedding(self.g,self.h_dim)\n",
    "        self.layers = nn.ModuleList()\n",
    "        #i2h\n",
    "        self.layers.append(RelGraphConvLayer(\n",
    "                    self.h_dim, self.h_dim, self.rel_names,\n",
    "                    self.num_bases, activation=F.relu, self_loop=self.use_self_loop,\n",
    "                    dropout=self.dropout, weight=True))\n",
    "        # h2h\n",
    "        if self.num_hidden_layers>1:\n",
    "            for i in range(0,self.num_hidden_layers-1):\n",
    "                self.layers.append(RelGraphConvLayer(\n",
    "                    self.h_dim, self.h_dim, self.rel_names,\n",
    "                    self.num_bases, activation=F.relu, self_loop=self.use_self_loop,\n",
    "                    dropout=self.dropout))\n",
    "        # h2o\n",
    "#         self.layers.append(RelGraphConvLayer(\n",
    "#             self.h_dim, self.out_dim, self.rel_names, \n",
    "#             self.num_bases, activation=partial(F.softmax, dim=1),\n",
    "#             self_loop=self.use_self_loop))\n",
    "        self.classifier = nn.Linear(self.h_dim, self.out_dim)\n",
    "    \n",
    "    def forward(self, input_nodes, blocks=None):\n",
    "        H={}\n",
    "        for ntype, nid in input_nodes.items():\n",
    "            nid = input_nodes[ntype]\n",
    "            H[ntype] = self.node_embed[ntype](nid)\n",
    "        if blocks is None:\n",
    "            for layer in self.layers:\n",
    "                H = layer(self.g, H)\n",
    "        else:\n",
    "            for layer, block in zip(self.layers, blocks):\n",
    "                H = layer(block, H)\n",
    "        output = self.classifier(H[\"usaanr\"])\n",
    "    \n",
    "        return output, H[\"usaanr\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_count_weight(y,n_classes):\n",
    "    classes_count=[]\n",
    "    weight=[]\n",
    "    for i in range(n_classes):\n",
    "        count=th.sum(y.squeeze()==i).item()\n",
    "        classes_count.append(count)\n",
    "        weight.append(len(y)/(n_classes*count))\n",
    "    return classes_count,weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lift_gain_eval(logit,label,topk):\n",
    "    DF=pd.DataFrame(columns=[\"pred_score\",\"actual_label\"])\n",
    "    DF[\"pred_score\"]=logit\n",
    "    DF[\"actual_label\"]=label\n",
    "    DF.sort_values(by=\"pred_score\", ascending=False, inplace=True)\n",
    "    gain={}\n",
    "    for p in topk:\n",
    "        N=math.ceil(int(DF.shape[0]*p))\n",
    "        DF2=DF.nlargest(N,\"pred_score\",keep=\"first\")\n",
    "        gain[str(int(p*100))+\"%\"]=round(DF2.actual_label.sum()/(DF.actual_label.sum()*p),2)\n",
    "    return gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_loop_func(model, loader, labels, device, loss_weight, num_classes):\n",
    "    model.eval()\n",
    "    fin_targets=[]\n",
    "    fin_outputs=[]\n",
    "    losses=[]\n",
    "    for input_nodes_raw, seeds, blocks in tqdm(loader, position=0, leave=True):\n",
    "        blocks = [blk.to(device) for blk in blocks]\n",
    "        seeds = seeds.to(device)\n",
    "        input_nodes={}\n",
    "        input_nodes[\"usaanr\"]=input_nodes_raw\n",
    "        input_nodes={k : e.to(device) for k, e in input_nodes.items()}\n",
    "        lbl = labels[seeds].squeeze().to(device)\n",
    "        with th.no_grad():\n",
    "            logits,h = model(input_nodes,blocks)\n",
    "            if loss_weight is None:\n",
    "                loss = F.cross_entropy(logits.view(-1, num_classes), lbl.to(device))\n",
    "            else:\n",
    "                loss = F.cross_entropy(logits.view(-1, num_classes), lbl.to(device),weight=loss_weight.float())\n",
    "            losses.append(loss.item())\n",
    "        fin_targets.append(lbl.cpu().detach().numpy())\n",
    "        fin_outputs.append(logits.cpu().detach().numpy())\n",
    "    return np.concatenate(fin_outputs), np.concatenate(fin_targets), losses\n",
    "\n",
    "def evaluate(target, predicted):\n",
    "    true_label_mask=[1 if (np.argmax(x)-target[i])==0 else 0 for i,x in enumerate(predicted)]\n",
    "    nb_prediction=len(true_label_mask)\n",
    "    true_prediction=sum(true_label_mask)\n",
    "    false_prediction=nb_prediction-true_prediction\n",
    "    accuracy=true_prediction/nb_prediction\n",
    "    precision, recall, fscore, support = precision_recall_fscore_support(target, predicted.argmax(axis=1))\n",
    "    auc = roc_auc_score(target.ravel(), th.sigmoid(th.from_numpy(predicted))[:,1].numpy().ravel())\n",
    "    prec,rec,_ = precision_recall_curve(target.ravel(), th.sigmoid(th.from_numpy(predicted))[:,1].numpy().ravel())\n",
    "    pr_auc=auc_score(rec,prec)\n",
    "    arg1=predicted[:,1]\n",
    "    arg2=target\n",
    "    gain = lift_gain_eval(arg1,arg2,topk=[0.01,0.05,0.10])\n",
    "    return {\n",
    "        \"nb_example\":len(target),\n",
    "        \"true_prediction\":true_prediction,\n",
    "        \"false_prediction\":false_prediction,\n",
    "        \"accuracy\":accuracy,\n",
    "        \"precision\":precision[1],\n",
    "        \"recall\":recall[1],\n",
    "        \"f1_score\":fscore[1],\n",
    "        \"AUC\":auc,\n",
    "        \"pr_auc\":pr_auc,\n",
    "        \"GAIN\":gain\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate(model, loader, labels, category, device):\n",
    "#     model.eval()\n",
    "#     total_loss = 0\n",
    "#     total_acc = 0\n",
    "#     total_precision=0\n",
    "#     total_recall=0\n",
    "#     total_fscore=0\n",
    "#     total_auc=0\n",
    "#     total_pr_auc=0\n",
    "    \n",
    "#     total_gain={}\n",
    "#     for p in [0.01,0.05,0.10]:\n",
    "#         total_gain[str(int(p*100))+\"%\"]=0\n",
    "        \n",
    "#     count = 0\n",
    "#     count_loss=0\n",
    "    \n",
    "#     for input_nodes_raw, seeds_raw, blocks in tqdm(loader, position=0, leave=True):\n",
    "#         blocks = [blk.to(device) for blk in blocks]\n",
    "        \n",
    "#         seeds=seeds_raw.to(device)\n",
    "\n",
    "#         input_nodes={}\n",
    "#         input_nodes[category]=input_nodes_raw\n",
    "#         input_nodes={k : e.to(device) for k, e in input_nodes.items()}\n",
    "\n",
    "#         lbl = labels[seeds].to(device)\n",
    "#         logits,h = model(input_nodes,blocks)\n",
    "#         loss = F.cross_entropy(logits, lbl.squeeze(1).to(device))\n",
    "#         loss = F.cross_entropy(logits, lbl.squeeze(1),weight=th.Tensor([1,args.weight]).to(device))\n",
    "\n",
    "#         acc = th.sum(logits.argmax(dim=1) == lbl.squeeze(1)).item() / logits.shape[0]\n",
    "#         precision, recall, fscore, support = score(lbl.squeeze(1).cpu().numpy(), logits.argmax(dim=1).cpu().numpy())\n",
    "\n",
    "      \n",
    "#         auc = roc_auc_score(lbl.detach().cpu().numpy().ravel(), th.sigmoid(logits)[:,1].detach().cpu().numpy().ravel())\n",
    "#         prec,rec,_ = precision_recall_curve(lbl.detach().cpu().numpy().ravel(), th.sigmoid(logits)[:,1].detach().cpu().numpy().ravel())\n",
    "#         pr_auc=auc_score(rec,prec)\n",
    "        \n",
    "#         total_loss += loss.item() * len(seeds) \n",
    "#         total_acc += acc\n",
    "#         total_precision += precision[1]\n",
    "#         total_recall += recall[1]\n",
    "#         total_fscore += fscore[1]\n",
    "#         total_auc += auc\n",
    "#         total_pr_auc += pr_auc\n",
    "#         count += 1\n",
    "#         count_loss += len(seeds)\n",
    "        \n",
    "#         arg1=logits[:,1].detach().cpu().numpy()\n",
    "#         arg2=lbl.cpu().numpy()\n",
    "#         gain = lift_gain_eval(arg1,arg2,topk=[0.01,0.05,0.10])\n",
    "#         for k in gain.keys():\n",
    "#             total_gain[k] += gain[k] \n",
    "    \n",
    "#     GAIN={}\n",
    "#     for k in total_gain.keys():\n",
    "#         GAIN[k]=total_gain[k]/count\n",
    "    \n",
    "#     ACCURACY=total_acc / count\n",
    "#     LOSS=total_loss / count_loss\n",
    "#     AUC=total_auc/count\n",
    "#     PR_AUC=total_pr_auc/count\n",
    "#     PRECISION=total_precision/count\n",
    "#     RECALL=total_recall/count\n",
    "#     F1_SCORE=total_fscore/count\n",
    "    \n",
    "#     return LOSS, ACCURACY, PRECISION, RECALL, F1_SCORE, GAIN, AUC, PR_AUC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create subgraph for the purpose of preliminary test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_nodes={\"usaanr\":th.arange(G.num_nodes('usaanr'))[0:1000],'zipcode':th.arange(G.num_nodes('zipcode'))[0:100]}\n",
    "# sg=dgl.node_subgraph(G,dict_nodes)\n",
    "\n",
    "# G=G.node_type_subgraph(['usaanr'])\n",
    "# dict_edges={}\n",
    "# for etype in G.etypes:\n",
    "#     dict_edges[etype]=th.arange(G.num_edges(etype))[0:5000]\n",
    "# G=dgl.edge_subgraph(G,dict_edges)\n",
    "\n",
    "# G.nodes['usaanr'].data[\"_ID\"].numpy().shape,binary_label.shape, binary_label[G.nodes['usaanr'].data[\"_ID\"]].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=10240, dropout=0.2, fanout=None, gpu=0, h_dim=64, l2norm=0.001, loss_weight=True, lr=0.001, n_epochs=1, num_bases=5, num_layers=1, num_mini_batch=8, num_worker=0, out_dim=1, seed=101, use_self_loop=True, validation=True)\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='RGCN')\n",
    "parser.add_argument(\"--dropout\", type=float, default=0,\n",
    "        help=\"dropout probability\")\n",
    "parser.add_argument(\"--h_dim\", type=int, default=128,\n",
    "        help=\"number of hidden units\")\n",
    "parser.add_argument(\"--out_dim\", type=int, default=1,\n",
    "        help=\"output dimension\")\n",
    "parser.add_argument(\"--gpu\", type=int, default=0,\n",
    "        help=\"gpu\")\n",
    "parser.add_argument(\"--lr\", type=float, default=1e-5,\n",
    "        help=\"learning rate\")\n",
    "parser.add_argument(\"--num_bases\", type=int, default=-1,\n",
    "        help=\"number of filter weight matrices, default: -1 [use all]\")\n",
    "parser.add_argument(\"--num_layers\", type=int, default=1,\n",
    "        help=\"number of propagation rounds\")\n",
    "parser.add_argument(\"-e\", \"--n_epochs\", type=int, default=1,\n",
    "        help=\"number of training epochs\")\n",
    "# parser.add_argument(\"--model_path\", type=str, default=\"/workspace/cjiang/eagle_project/CAP_graph/CAP_without_zipcode/rgcn_model_param.pt\",\n",
    "#         help='path for save the model')\n",
    "parser.add_argument(\"--l2norm\", type=float, default=0,\n",
    "        help=\"l2 norm coef\")\n",
    "parser.add_argument(\"--use_self_loop\", default=True, action='store_true',\n",
    "        help=\"include self feature as a special relation\")\n",
    "parser.add_argument(\"--batch-size\", type=int, default=1024,\n",
    "        help=\"Mini-batch size. If -1, use full graph training.\")\n",
    "parser.add_argument(\"--num_mini_batch\", type=int, default=8,\n",
    "        help=\"Number of minibatch.\")\n",
    "parser.add_argument(\"--fanout\", type=int, default=None,\n",
    "        help=\"Fan-out of neighbor sampling.\")\n",
    "parser.add_argument(\"--validation\",  default=True,\n",
    "        help=\"set up validation .\")\n",
    "parser.add_argument(\"--seed\",  type=int,default=101,\n",
    "        help=\"random seed for np.random.seed, torch.manual_seed and torch.cuda.manual_seed.\")\n",
    "\n",
    "parser.add_argument(\"--loss_weight\",  type=bool,default=True,  ## number of label=0/number of label=1\n",
    "        help=\"weight for unbalance data\")\n",
    "parser.add_argument(\"--num_worker\",  type=int,default=0,  \n",
    "        help=\"number of worker for neighbor sampling\") \n",
    "\n",
    "args,unknown=parser.parse_known_args()\n",
    "\n",
    "args.num_layers=1\n",
    "args.dropout=0.2\n",
    "args.lr=1e-3\n",
    "args.l2norm=1e-3\n",
    "args.n_epochs=1\n",
    "args.num_bases=5\n",
    "args.h_dim=64\n",
    "args.batch_size=10240\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "</style><table id=\"T_416487b4_e032_11eb_b047_0242ac110003\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >label_class</th>        <th class=\"col_heading level0 col1\" >count</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_416487b4_e032_11eb_b047_0242ac110003level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "                        <td id=\"T_416487b4_e032_11eb_b047_0242ac110003row0_col0\" class=\"data row0 col0\" >0</td>\n",
       "                        <td id=\"T_416487b4_e032_11eb_b047_0242ac110003row0_col1\" class=\"data row0 col1\" >19,358,913</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_416487b4_e032_11eb_b047_0242ac110003level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "                        <td id=\"T_416487b4_e032_11eb_b047_0242ac110003row1_col0\" class=\"data row1 col0\" >1</td>\n",
       "                        <td id=\"T_416487b4_e032_11eb_b047_0242ac110003row1_col1\" class=\"data row1 col1\" >6,309,591</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f555a7bc320>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_rels=len(G.etypes)\n",
    "# LABEL=th.tensor(binary_label[G.nodes['usaanr'].data[\"_ID\"]]).long()\n",
    "LABEL=th.tensor(binary_label).long()\n",
    "labels, count=th.unique(LABEL,return_counts=True)\n",
    "num_classes=labels.shape[0]\n",
    "pd.DataFrame({\"label_class\":labels, \"count\":count}).style.format({'count':'{:,}'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set    20,534,804\n",
      "validation set  2,566,850 \n",
      "test set        2,566,850 \n"
     ]
    }
   ],
   "source": [
    "# train_mask=train_mask_binary_label[G.nodes['usaanr'].data[\"_ID\"]] \n",
    "# val_mask=val_mask_binary_label[G.nodes['usaanr'].data[\"_ID\"]]\n",
    "# test_mask=test_mask_binary_label[G.nodes['usaanr'].data[\"_ID\"]]\n",
    "\n",
    "train_mask=train_mask_binary_label  \n",
    "val_mask=val_mask_binary_label\n",
    "test_mask=test_mask_binary_label\n",
    "\n",
    "train_idx=th.nonzero(train_mask.squeeze()).numpy()\n",
    "val_idx=th.nonzero(val_mask.squeeze()).numpy()\n",
    "test_idx=th.nonzero(test_mask.squeeze()).numpy()\n",
    "\n",
    "train_idx=th.from_numpy(train_idx).squeeze(1)    \n",
    "val_idx=th.from_numpy(val_idx).squeeze(1)    \n",
    "test_idx=th.from_numpy(test_idx).squeeze(1)\n",
    "\n",
    "train_label=LABEL[train_idx]\n",
    "val_label=LABEL[val_idx]\n",
    "test_label=LABEL[test_idx]\n",
    "\n",
    "print('{:<15} {:<10,}'.format(\"Training set\",train_idx.shape[0]))\n",
    "print('{:<15} {:<10,}'.format(\"validation set\",val_idx.shape[0]))\n",
    "print('{:<15} {:<10,}'.format(\"test set\",test_idx.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert train_idx.shape[0]+val_idx.shape[0]+test_idx.shape[0] == G.num_nodes('usaanr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "##### check cuda\n",
    "device=\"cpu\"\n",
    "use_cuda=args.gpu>=0 and th.cuda.is_available()\n",
    "if use_cuda:\n",
    "    th.cuda.set_device(args.gpu)\n",
    "    device='cuda:%d' % args.gpu\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.loss_weight:\n",
    "    train_classes_num,train_classes_weight=get_class_count_weight(train_label.squeeze(),num_classes)\n",
    "    loss_weight=th.tensor(train_classes_weight).to(device)\n",
    "else:\n",
    "    loss_weight=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0): RelGraphConvLayer(\n",
       "    (conv): HeteroGraphConv(\n",
       "      (mods): ModuleDict(\n",
       "        (AUTO_RELATED): GraphConv(in=64, out=64, normalization=both, activation=None)\n",
       "        (Brother_Sister): GraphConv(in=64, out=64, normalization=both, activation=None)\n",
       "        (Busi_rel_Other): GraphConv(in=64, out=64, normalization=both, activation=None)\n",
       "        (Child): GraphConv(in=64, out=64, normalization=both, activation=None)\n",
       "        (Ex-Spouse): GraphConv(in=64, out=64, normalization=both, activation=None)\n",
       "        (Parent): GraphConv(in=64, out=64, normalization=both, activation=None)\n",
       "        (Pers_rel_Other): GraphConv(in=64, out=64, normalization=both, activation=None)\n",
       "        (SPONSEE): GraphConv(in=64, out=64, normalization=both, activation=None)\n",
       "        (SPONSOR): GraphConv(in=64, out=64, normalization=both, activation=None)\n",
       "        (Spouse): GraphConv(in=64, out=64, normalization=both, activation=None)\n",
       "        (Step-Child): GraphConv(in=64, out=64, normalization=both, activation=None)\n",
       "        (Step-Parent): GraphConv(in=64, out=64, normalization=both, activation=None)\n",
       "      )\n",
       "    )\n",
       "    (basis): WeightBasis()\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create model\n",
    "model = Entity_Classify(G,\n",
    "                       args.h_dim,\n",
    "                       num_classes,\n",
    "                       num_bases=args.num_bases,\n",
    "                       num_hidden_layers=args.num_layers,\n",
    "                       dropout=args.dropout,\n",
    "                       use_self_loop=args.use_self_loop)\n",
    "if use_cuda:\n",
    "    model.cuda()\n",
    "\n",
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = th.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.l2norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of minibatch in training set is 2,006\n",
      "The number of minibatch in validation set is 251\n",
      "The number of minibatch in test set is 251\n"
     ]
    }
   ],
   "source": [
    "# train sampler\n",
    "sampler = dgl.dataloading.MultiLayerNeighborSampler([args.fanout] * args.num_layers)\n",
    "train_loader = dgl.dataloading.NodeDataLoader(\n",
    "    G, {'usaanr': train_idx}, sampler,\n",
    "    batch_size=args.batch_size, shuffle=True, num_workers=args.num_worker)\n",
    "# validation sampler\n",
    "# we do not use full neighbor to save computation resources\n",
    "val_sampler = dgl.dataloading.MultiLayerNeighborSampler([args.fanout] * args.num_layers)\n",
    "val_loader = dgl.dataloading.NodeDataLoader(\n",
    "    G, {'usaanr': val_idx}, val_sampler,\n",
    "    batch_size=args.batch_size, shuffle=False, num_workers=args.num_worker)\n",
    "\n",
    "test_sampler = dgl.dataloading.MultiLayerNeighborSampler([args.fanout] * args.num_layers)\n",
    "test_loader = dgl.dataloading.NodeDataLoader(\n",
    "    G, {'usaanr': test_idx}, test_sampler,\n",
    "    batch_size=args.batch_size, shuffle=False, num_workers=args.num_worker)\n",
    "\n",
    "print(\"The number of minibatch in training set is {:,}\".format(len(train_loader)))\n",
    "print(\"The number of minibatch in validation set is {:,}\".format(len(val_loader)))\n",
    "print(\"The number of minibatch in test set is {:,}\".format(len(test_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total # of parameter is 42,750\n"
     ]
    }
   ],
   "source": [
    "print(\"The total # of parameter is {:,}\".format(sum([p.nelement() for p in model.parameters()]) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node_embed.usaanr.emb.usaayr.weight                                   5,440          \n",
      "node_embed.usaanr.emb.AGE_BAND.weight                                 448            \n",
      "node_embed.usaanr.emb.ORIGEL.weight                                   2,752          \n",
      "node_embed.usaanr.emb.ELIG2.weight                                    384            \n",
      "node_embed.usaanr.emb.cmpyelig.weight                                 320            \n",
      "node_embed.usaanr.emb.SEX.weight                                      320            \n",
      "node_embed.usaanr.emb.MARST.weight                                    640            \n",
      "node_embed.usaanr.emb.BRANCH.weight                                   1,088          \n",
      "node_embed.usaanr.emb.MILST.weight                                    512            \n",
      "node_embed.usaanr.emb.MLIST_OrigStat.weight                           192            \n",
      "node_embed.usaanr.emb.ENLPAYGD.weight                                 1,600          \n",
      "node_embed.usaanr.emb.ACTCORP.weight                                  128            \n",
      "node_embed.usaanr.emb.STATE.weight                                    3,712          \n",
      "node_embed.usaanr.emb.Segment.weight                                  384            \n",
      "layers.0.h_bias                                                       64             \n",
      "layers.0.loop_weight                                                  4,096          \n",
      "layers.0.basis.weight                                                 20,480         \n",
      "layers.0.basis.w_comp                                                 60             \n",
      "classifier.weight                                                     128            \n",
      "classifier.bias                                                       2              \n"
     ]
    }
   ],
   "source": [
    "param_dict={n: p.nelement() for n, p in model.named_parameters()}\n",
    "for i,j in param_dict.items():\n",
    "    print(\"{:<70}{:<15,}\".format(i,j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training...\n",
      "\n",
      "========= Epoch 1 /1\n",
      "Training...\n",
      "Batch 200 of 2006 | Loss 0.681  | Elapsed: 0:01:23\n",
      "Batch 400 of 2006 | Loss 0.676  | Elapsed: 0:02:20\n",
      "Batch 600 of 2006 | Loss 0.675  | Elapsed: 0:03:22\n",
      "Batch 800 of 2006 | Loss 0.675  | Elapsed: 0:04:23\n",
      "Batch 1000 of 2006 | Loss 0.673  | Elapsed: 0:05:19\n",
      "Batch 1200 of 2006 | Loss 0.673  | Elapsed: 0:06:15\n",
      "Batch 1400 of 2006 | Loss 0.672  | Elapsed: 0:07:30\n",
      "Batch 1600 of 2006 | Loss 0.672  | Elapsed: 0:08:17\n",
      "Batch 1800 of 2006 | Loss 0.670  | Elapsed: 0:09:28\n",
      "Batch 2000 of 2006 | Loss 0.671  | Elapsed: 0:10:46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2006 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Running Validation on training set\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2006/2006 [05:11<00:00,  6.44it/s]  \n",
      "  0%|          | 1/251 [00:00<00:28,  8.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_loss: 0.67 | True_Prediction: 13,388,379 | False_Prediction: 7,146,425 | accuracy: 65.20% |  precision: 34.72% | recall: 47.25% | F1_score: 40.03% | Gain_top-10%: 1.4 |    ROC_AUC: 61.5% | PR_AUC: 32.0% | Elapsed: 0:07:00\n",
      "\n",
      "\n",
      "Running Validation on validation set\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 221/251 [00:23<00:03,  9.51it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-43b035d2dadd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Running Validation on validation set\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m     \u001b[0mfin_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfin_targets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses_tmp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_loop_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary_label\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m     \u001b[0mavg_loss_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses_tmp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0mtmp_mean_pool_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfin_targets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfin_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-7039f31b1de1>\u001b[0m in \u001b[0;36meval_loop_func\u001b[0;34m(model, loader, labels, device, loss_weight, num_classes)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mfin_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mlosses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0minput_nodes_raw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblocks\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposition\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mblocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mblk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mblk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mblocks\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mseeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseeds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 fp_write=getattr(self.fp, 'write', sys.stderr.write))\n\u001b[1;32m   1101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1102\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m             \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/dgl/dataloading/pytorch/__init__.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;31m# input_nodes, output_nodes, blocks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m         \u001b[0mresult_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m         \u001b[0m_restore_blocks_storage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode_dataloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/dgl/utils/internal.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    690\u001b[0m         \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_group_offsets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m         \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_groups\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 692\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcompensate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morigin_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "LOSS_EPOCH=[]\n",
    "LABEL_TRAIN=[]\n",
    "# training loop\n",
    "print(\"start training...\")\n",
    "dur = []\n",
    "total_loss=0\n",
    "losses=[]\n",
    "\n",
    "LOGIT_train=[]\n",
    "LABEL_train=[]\n",
    "\n",
    "for epoch in tqdm(range(0,args.n_epochs)):\n",
    "    \n",
    "    model.train()\n",
    "    IDX=[]\n",
    "    H=[]\n",
    "    LABEL_train=[]\n",
    "    \n",
    "    #====================================#\n",
    "    #            Traning                 #\n",
    "    #====================================#\n",
    "    print(\"\")\n",
    "    print(\"========= Epoch {:} /{:}\".format(epoch+1,args.n_epochs))\n",
    "    print(\"Training...\")\n",
    "    t0 = time.time()\n",
    "    for step, (input_nodes_raw, seeds_raw, blocks) in enumerate(train_loader):\n",
    "        blocks = [blk.to(device) for blk in blocks]\n",
    "        \n",
    "        seeds=seeds_raw.to(device)\n",
    "        \n",
    "        labels_train=LABEL[seeds]       \n",
    "        labels_train = labels_train.to(device)\n",
    "        \n",
    "        input_nodes={}\n",
    "        input_nodes[\"usaanr\"]=input_nodes_raw\n",
    "        input_nodes={k : e.to(device) for k, e in input_nodes.items()}\n",
    "        \n",
    "        logits,h = model(input_nodes,blocks)\n",
    "        optimizer.zero_grad()\n",
    "        if args.loss_weight:\n",
    "            loss = F.cross_entropy(logits.view(-1,num_classes), labels_train.squeeze().to(device),weight=loss_weight.float().to(device))\n",
    "        else:\n",
    "            loss = F.cross_entropy(logits.view(-1,num_classes), labels_train.squeeze().to(device))\n",
    "        total_loss+=loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        arg1=logits[:,1].detach().cpu().numpy()\n",
    "        arg2=labels_train.cpu().numpy()\n",
    "                \n",
    "        train_gain = lift_gain_eval(arg1,arg2,topk=[0.01,0.05,0.10])\n",
    "        train_acc = th.sum(logits.argmax(dim=1) == labels_train).item() / len(seeds)\n",
    "        precision, recall, fscore, support = precision_recall_fscore_support(labels_train.cpu().numpy(), logits.argmax(dim=1).cpu().numpy())\n",
    "        try:\n",
    "            train_auc = roc_auc_score(labels_train.detach().cpu().numpy().ravel(), th.sigmoid(logits)[:,1].detach().cpu().numpy().ravel())\n",
    "        except ValueError:\n",
    "            pass\n",
    "        prec,rec,_ = precision_recall_curve(labels_train.detach().cpu().numpy().ravel(), th.sigmoid(logits)[:,1].detach().cpu().numpy().ravel())\n",
    "        if math.isnan(rec[0])==False:\n",
    "            train_pr_auc=auc_score(rec,prec)\n",
    "        IDX.extend(seeds.detach().cpu().numpy().tolist())\n",
    "        H.extend(h.detach().cpu().numpy().tolist())\n",
    "        LOGIT_train.extend(logits.detach().cpu().numpy().tolist())\n",
    "        LABEL_train.extend(binary_label[blocks[-1].dstdata[dgl.NID].cpu().numpy()].tolist())\n",
    "        if step%(len(train_loader)//10)==0 and not step==0:\n",
    "            t1 = time.time()\n",
    "            elapsed=utils.format_time(t1-t0)\n",
    "            print(\"Batch {:} of {:} | Loss {:.3f}  | Elapsed: {:}\".\\\n",
    "                  format(step,len(train_loader),np.mean(losses[-10:]),elapsed))\n",
    "    LOSS_EPOCH.append(loss)\n",
    "    LABEL_TRAIN.append(binary_label[blocks[-1].ndata[dgl.NID]['usaanr'].cpu().numpy()])\n",
    "    model.eval()\n",
    "    print()\n",
    "    print(\"\")\n",
    "    print(\"Running Validation on training set\")\n",
    "    print(\"\")\n",
    "    fin_outputs, fin_targets, losses_tmp=eval_loop_func(model, train_loader, binary_label,  device, loss_weight, num_classes)\n",
    "    avg_loss_train=np.mean(losses_tmp)\n",
    "    tmp_mean_pool_train=evaluate(fin_targets.reshape(-1),fin_outputs)\n",
    "    t2=time.time()\n",
    "    print(\"avg_loss: {:.2f} | True_Prediction: {:,} | False_Prediction: {:,} | accuracy: {:.2%} |  precision: {:.2%} | recall: {:.2%} | F1_score: {:.2%} | Gain_top-10%: {:.1f} |\\\n",
    "    ROC_AUC: {:.1%} | PR_AUC: {:.1%} | Elapsed: {:}\".format(avg_loss_train, tmp_mean_pool_train[\"true_prediction\"], tmp_mean_pool_train[\"false_prediction\"], tmp_mean_pool_train[\"accuracy\"], \\\n",
    "                                                            tmp_mean_pool_train[\"precision\"], tmp_mean_pool_train[\"recall\"], tmp_mean_pool_train[\"f1_score\"], tmp_mean_pool_train[\"GAIN\"]['10%'], \\\n",
    "                                                            tmp_mean_pool_train[\"AUC\"], tmp_mean_pool_train[\"pr_auc\"], utils.format_time(t2-t1)))\n",
    "    #====================================#\n",
    "    #            Validation-set          #\n",
    "    #====================================#\n",
    "    model.eval()\n",
    "    print()\n",
    "    print(\"\")\n",
    "    print(\"Running Validation on validation set\")\n",
    "    print(\"\")\n",
    "    fin_outputs, fin_targets, losses_tmp=eval_loop_func(model, val_loader, binary_label,  device, loss_weight, num_classes)\n",
    "    avg_loss_val=np.mean(losses_tmp)\n",
    "    tmp_mean_pool_val=evaluate(fin_targets.reshape(-1),fin_outputs)\n",
    "    t3=time.time()\n",
    "    print(\"avg_loss: {:.2f} | True_Prediction: {:,} | False_Prediction: {:,} | accuracy: {:.2%} |  precision: {:.2%} | recall: {:.2%} | F1_score: {:.2%} | Gain_top-10%: {:.1f} |\\\n",
    "    ROC_AUC: {:.1%} | PR_AUC: {:.1%} | Elapsed: {:}\".format(avg_loss_val, tmp_mean_pool_val[\"true_prediction\"], tmp_mean_pool_val[\"false_prediction\"], tmp_mean_pool_val[\"accuracy\"], \\\n",
    "                                                            tmp_mean_pool_val[\"precision\"], tmp_mean_pool_val[\"recall\"], tmp_mean_pool_val[\"f1_score\"], tmp_mean_pool_val[\"GAIN\"]['10%'], \\\n",
    "                                                            tmp_mean_pool_val[\"AUC\"], tmp_mean_pool_val[\"pr_auc\"], utils.format_time(t3-t2)))\n",
    "# if args.model_path is not None:\n",
    "#     th.save(model.state_dict(), args.model_path)\n",
    "#====================================#\n",
    "#            Test-set                #\n",
    "#====================================#\n",
    "print()\n",
    "print(\"\")\n",
    "print(\"Running Validation in Test Dataset\")\n",
    "print(\"\")\n",
    "model.eval()\n",
    "fin_outputs, fin_targets, losses_tmp=eval_loop_func(model, test_loader, binary_label,  device, loss_weight, num_classes)\n",
    "avg_loss_test=np.mean(losses_tmp)\n",
    "tmp_mean_pool_test=evaluate(fin_targets.reshape(-1),fin_outputs)\n",
    "t4=time.time()\n",
    "print(\"avg_loss: {:.2f} | True_Prediction: {:,} | False_Prediction: {:,} | accuracy: {:.2%} |  precision: {:.2%} | recall: {:.2%} | F1_score: {:.2%} | Gain_top-10%: {:.1f} |\\\n",
    "ROC_AUC: {:.1%} | PR_AUC: {:.1%} | Elapsed: {:}\".format(avg_loss_test, tmp_mean_pool_test[\"true_prediction\"], tmp_mean_pool_test[\"false_prediction\"], tmp_mean_pool_test[\"accuracy\"], \\\n",
    "                                                        tmp_mean_pool_test[\"precision\"], tmp_mean_pool_test[\"recall\"], tmp_mean_pool_test[\"f1_score\"], tmp_mean_pool_test[\"GAIN\"]['10%'], \\\n",
    "                                                        tmp_mean_pool_test[\"AUC\"], tmp_mean_pool_test[\"pr_auc\"], utils.format_time(t4-t3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
