{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade --quiet pip\n",
    "# !pip install --quiet torch==1.6.0 --index-url https://repo.usaa.com/artifactory/api/pypi/usaa-pypi-eval/simple --trusted-host repo.usaa.com\n",
    "# !pip install --quiet /mnt/dgl_cu102-0.6.0-cp36-cp36m-manylinux1_x86_64.whl\n",
    "# !pip install --upgrade --quiet tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version is 1.6.0\n",
      "DGL version is 0.6.0\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import itertools\n",
    "import os\n",
    "import numpy as np\n",
    "from numpy import save,load,savetxt,loadtxt,savez_compressed\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score, f1_score,average_precision_score\n",
    "from sklearn.metrics import precision_recall_fscore_support \n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import auc as auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.preprocessing import LabelEncoder, label_binarize\n",
    "\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "import time\n",
    "from tqdm import tqdm, tqdm_notebook,tnrange\n",
    "tqdm.pandas(position=0, leave=True)\n",
    "import math \n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import dgl\n",
    "import dgl.nn as dglnn\n",
    "import dgl.function as Fn\n",
    "from functools import partial\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import random\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize']=(5.0,4.0)\n",
    "plt.rcParams['image.interpolation']='nearest'\n",
    "plt.rcParams['image.cmap']='gray'\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import utils\n",
    "import tsne_func\n",
    "print(\"torch version is {}\".format(th.__version__))\n",
    "print(\"DGL version is {}\".format(dgl.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    th.manual_seed(seed)\n",
    "    th.cuda.manual_seed_all(seed)\n",
    "    th.backends.cudnn.deterministic = True\n",
    "    th.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    \n",
    "seed_everything(101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-a8796572a24d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKG_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'CAP_Graph'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"It took {:0.4f} seconds to load graph\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "KG_dir=\"/home/ubuntu/\"\n",
    "\n",
    "if os.path.isfile(\"/home/ubuntu/CAP_Graph\")==False:\n",
    "    !hdfs dfs -get /dz/dz_6104/disc.db/CAP_Graph  ~/CAP_Graph\n",
    "\n",
    "start=time.time()\n",
    "with open(os.path.join(KG_dir,'CAP_Graph'), 'rb') as f:\n",
    "    G, node_labels = pickle.load(f)\n",
    "end=time.time()\n",
    "print(\"It took {:0.4f} seconds to load graph\".format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 5.4741 seconds to load graph\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "with open(os.path.join(KG_dir,'CAP_Graph'), 'rb') as f:\n",
    "    G,multi_label,binary_label,\\\n",
    "    train_mask_multi_label,  val_mask_multi_label,  test_mask_multi_label,\\\n",
    "    train_mask_binary_label, val_mask_binary_label, test_mask_binary_label= pickle.load(f)\n",
    "end=time.time()\n",
    "print(\"It took {:0.4f} seconds to load graph\".format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Graph(num_nodes={'usaanr': 25668504},\n",
       "      num_edges={('usaanr', 'AUTO_RELATED', 'usaanr'): 7947060, ('usaanr', 'Brother_Sister', 'usaanr'): 651444, ('usaanr', 'Busi_rel_Other', 'usaanr'): 1039214, ('usaanr', 'Child', 'usaanr'): 11765795, ('usaanr', 'Ex-Spouse', 'usaanr'): 3877972, ('usaanr', 'Parent', 'usaanr'): 11765795, ('usaanr', 'Pers_rel_Other', 'usaanr'): 2012514, ('usaanr', 'SPONSEE', 'usaanr'): 16775141, ('usaanr', 'SPONSOR', 'usaanr'): 16775141, ('usaanr', 'Spouse', 'usaanr'): 16016119, ('usaanr', 'Step-Child', 'usaanr'): 1240193, ('usaanr', 'Step-Parent', 'usaanr'): 1240193},\n",
       "      metagraph=[('usaanr', 'usaanr', 'AUTO_RELATED'), ('usaanr', 'usaanr', 'Brother_Sister'), ('usaanr', 'usaanr', 'Busi_rel_Other'), ('usaanr', 'usaanr', 'Child'), ('usaanr', 'usaanr', 'Ex-Spouse'), ('usaanr', 'usaanr', 'Parent'), ('usaanr', 'usaanr', 'Pers_rel_Other'), ('usaanr', 'usaanr', 'SPONSEE'), ('usaanr', 'usaanr', 'SPONSOR'), ('usaanr', 'usaanr', 'Spouse'), ('usaanr', 'usaanr', 'Step-Child'), ('usaanr', 'usaanr', 'Step-Parent')])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Node_types:  ['usaanr']\n",
      "Edge_types:  ['AUTO_RELATED', 'Brother_Sister', 'Busi_rel_Other', 'Child', 'Ex-Spouse', 'Parent', 'Pers_rel_Other', 'SPONSEE', 'SPONSOR', 'Spouse', 'Step-Child', 'Step-Parent']\n",
      "**************************************************\n",
      "Canonical Etypes of Graph is:\n",
      "\n",
      "usaanr              AUTO_RELATED        usaanr              \n",
      "usaanr              Brother_Sister      usaanr              \n",
      "usaanr              Busi_rel_Other      usaanr              \n",
      "usaanr              Child               usaanr              \n",
      "usaanr              Ex-Spouse           usaanr              \n",
      "usaanr              Parent              usaanr              \n",
      "usaanr              Pers_rel_Other      usaanr              \n",
      "usaanr              SPONSEE             usaanr              \n",
      "usaanr              SPONSOR             usaanr              \n",
      "usaanr              Spouse              usaanr              \n",
      "usaanr              Step-Child          usaanr              \n",
      "usaanr              Step-Parent         usaanr              \n",
      "**************************************************\n",
      "number of ntype=usaanr                25,668,504     \n",
      "**************************************************\n",
      "Total number of nodes is 25,668,504\n",
      "**************************************************\n",
      "number of etype=AUTO_RELATED          7,947,060      \n",
      "number of etype=Brother_Sister        651,444        \n",
      "number of etype=Busi_rel_Other        1,039,214      \n",
      "number of etype=Child                 11,765,795     \n",
      "number of etype=Ex-Spouse             3,877,972      \n",
      "number of etype=Parent                11,765,795     \n",
      "number of etype=Pers_rel_Other        2,012,514      \n",
      "number of etype=SPONSEE               16,775,141     \n",
      "number of etype=SPONSOR               16,775,141     \n",
      "number of etype=Spouse                16,016,119     \n",
      "number of etype=Step-Child            1,240,193      \n",
      "number of etype=Step-Parent           1,240,193      \n",
      "**************************************************\n",
      "Total number of edges is 91,106,581\n",
      "**************************************************\n",
      "**************************************************\n",
      "The attributes for the node type=usaanr\n",
      "**************************************************\n",
      "usaayr                                  torch.Size([25668504, 1])\n",
      "AGE_BAND                                torch.Size([25668504, 1])\n",
      "ORIGEL                                  torch.Size([25668504, 1])\n",
      "ELIG2                                   torch.Size([25668504, 1])\n",
      "cmpyelig                                torch.Size([25668504, 1])\n",
      "SEX                                     torch.Size([25668504, 1])\n",
      "MARST                                   torch.Size([25668504, 1])\n",
      "BRANCH                                  torch.Size([25668504, 1])\n",
      "ENLPAYGD                                torch.Size([25668504, 1])\n",
      "MILST                                   torch.Size([25668504, 1])\n",
      "MLIST_OrigStat                          torch.Size([25668504, 1])\n",
      "ACTCORP                                 torch.Size([25668504, 1])\n",
      "STATE                                   torch.Size([25668504, 1])\n",
      "Segment                                 torch.Size([25668504, 1])\n",
      "ZIPCD                                   torch.Size([25668504, 1])\n"
     ]
    }
   ],
   "source": [
    "utils.graph_show(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usaanr_feat=[]\n",
    "for key, scheme in G.node_attr_schemes(ntype=\"usaanr\").items():\n",
    "    usaanr_feat.append(key)\n",
    "usaanr_feat=[x for x in usaanr_feat if x not in ['ZIPCD','train_mask','val_mask','test_mask']]\n",
    "\n",
    "print()\n",
    "print(\"The features associated with USAA Member are\\n \")\n",
    "for i in usaanr_feat:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## USAA Members Features Embedding\n",
    "class USAANR_Embedding(nn.Module):\n",
    "    def __init__(self,G,feature_size):\n",
    "        super(USAANR_Embedding,self).__init__()\n",
    "        self.G=G.to(device)\n",
    "        self.feature_size=feature_size\n",
    "        ## Embedding matrices for features of nodes.\n",
    "        self.emb = nn.ModuleDict()\n",
    "        \n",
    "        for i,col in enumerate(usaanr_feat):\n",
    "            self.emb[col]=nn.Embedding(G.nodes['usaanr'].data[col].max().item()+1, feature_size)\n",
    "    \n",
    "    def forward(self,nid):\n",
    "        nid=nid.to(device)\n",
    "        extra_repr=[]\n",
    "        \n",
    "        for i,col in enumerate(usaanr_feat):\n",
    "            ndata=self.G.nodes['usaanr'].data[col]\n",
    "            extra_repr.append(self.emb[col](ndata[nid]).squeeze(1))\n",
    "        return th.stack(extra_repr, 0).sum(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelGraphConvLayer(nn.Module):\n",
    "    r\"\"\"Relational graph convolution layer.\n",
    "    Parameters\n",
    "    ----------\n",
    "    in_feat : int\n",
    "        Input feature size.\n",
    "    out_feat : int\n",
    "        Output feature size.\n",
    "    rel_names : list[str]\n",
    "        Relation names.\n",
    "    num_bases : int, optional\n",
    "        Number of bases. If is none, use number of relations. Default: None.\n",
    "    weight : bool, optional\n",
    "        True if a linear layer is applied after message passing. Default: True\n",
    "    bias : bool, optional\n",
    "        True if bias is added. Default: True\n",
    "    activation : callable, optional\n",
    "        Activation function. Default: None\n",
    "    self_loop : bool, optional\n",
    "        True to include self loop message. Default: False\n",
    "    dropout : float, optional\n",
    "        Dropout rate. Default: 0.0\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 in_feat,\n",
    "                 out_feat,\n",
    "                 rel_names,\n",
    "                 num_bases,\n",
    "                 *,\n",
    "                 weight=True,\n",
    "                 bias=True,\n",
    "                 activation=None,\n",
    "                 self_loop=False,\n",
    "                 dropout=0.0):\n",
    "        super(RelGraphConvLayer, self).__init__()\n",
    "        self.in_feat = in_feat\n",
    "        self.out_feat = out_feat\n",
    "        self.rel_names = rel_names\n",
    "        self.num_bases = num_bases\n",
    "        self.bias = bias\n",
    "        self.activation = activation\n",
    "        self.self_loop = self_loop\n",
    "        self.conv = dglnn.HeteroGraphConv({\n",
    "                rel : dglnn.GraphConv(in_feat, out_feat, norm=\"both\", weight=False, bias=False)\n",
    "                for rel in rel_names\n",
    "            })\n",
    "        self.use_weight = weight\n",
    "        self.use_basis = num_bases < len(self.rel_names) and weight\n",
    "        if self.use_weight:\n",
    "            if self.use_basis:\n",
    "                self.basis = dglnn.WeightBasis((in_feat, out_feat), num_bases, len(self.rel_names))\n",
    "            else:\n",
    "                self.weight = nn.Parameter(th.Tensor(len(self.rel_names), in_feat, out_feat))\n",
    "                nn.init.xavier_uniform_(self.weight, gain=nn.init.calculate_gain('relu'))\n",
    "        # bias\n",
    "        if bias:\n",
    "            self.h_bias = nn.Parameter(th.Tensor(out_feat))\n",
    "            nn.init.zeros_(self.h_bias)\n",
    "        # weight for self loop\n",
    "        if self.self_loop:\n",
    "            self.loop_weight = nn.Parameter(th.Tensor(in_feat, out_feat))\n",
    "            nn.init.xavier_uniform_(self.loop_weight,\n",
    "                                    gain=nn.init.calculate_gain('relu'))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, g, inputs):\n",
    "        \"\"\"Forward computation\n",
    "        Parameters\n",
    "        ----------\n",
    "        g : DGLHeteroGraph\n",
    "            Input graph.\n",
    "        inputs : dict[str, torch.Tensor]\n",
    "            Node feature for each node type.\n",
    "        Returns\n",
    "        -------\n",
    "        dict[str, torch.Tensor]\n",
    "            New node features for each node type.\n",
    "        \"\"\"\n",
    "        g = g.local_var()\n",
    "        if self.use_weight:\n",
    "            weight = self.basis() if self.use_basis else self.weight\n",
    "            wdict = {self.rel_names[i] : {'weight' : w.squeeze(0)}\n",
    "                     for i, w in enumerate(th.split(weight, 1, dim=0))}\n",
    "        else:\n",
    "            wdict = {}\n",
    "        if g.is_block:\n",
    "            inputs_src = inputs\n",
    "            inputs_dst = {k: v[:g.number_of_dst_nodes(k)] for k, v in inputs.items()}\n",
    "        else:\n",
    "            inputs_src = inputs_dst = inputs\n",
    "        hs = self.conv(g, inputs, mod_kwargs=wdict)\n",
    "        def _apply(ntype, h):\n",
    "            if self.self_loop:\n",
    "                h = h + th.matmul(inputs_dst[ntype], self.loop_weight)\n",
    "            if self.bias:\n",
    "                h = h + self.h_bias\n",
    "            if self.activation:\n",
    "                h = self.activation(h)\n",
    "            return self.dropout(h)\n",
    "        return {ntype : _apply(ntype, h) for ntype, h in hs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Entity_Classify(nn.Module):\n",
    "    def __init__(self,\n",
    "                 g,\n",
    "                 h_dim,\n",
    "                 out_dim,\n",
    "                 num_bases,\n",
    "#                  embed_layer,\n",
    "                 num_hidden_layers=1,\n",
    "                 dropout=0,\n",
    "                 use_self_loop=False):\n",
    "        super(Entity_Classify, self).__init__()\n",
    "        self.g = g\n",
    "        self.h_dim = h_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.rel_names = list(set(g.etypes))\n",
    "#         self.num_bases = None if num_bases < 0 else num_bases\n",
    "        if num_bases < 0 or num_bases > len(self.rel_names):\n",
    "            self.num_bases = len(self.rel_names)\n",
    "        else:\n",
    "            self.num_bases = num_bases\n",
    "            \n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.dropout = dropout\n",
    "        self.use_self_loop = use_self_loop\n",
    "        \n",
    "#         self.node_embed={}\n",
    "        self.node_embed=nn.ModuleDict()\n",
    "        self.node_embed['usaanr'] = USAANR_Embedding(self.g,self.h_dim)\n",
    "#         self.node_embed['zipcode'] = Zipcode_Embedding(self.g,self.h_dim)\n",
    "        self.layers = nn.ModuleList()\n",
    "        #i2h\n",
    "        self.layers.append(RelGraphConvLayer(\n",
    "                    self.h_dim, self.h_dim, self.rel_names,\n",
    "                    self.num_bases, activation=F.relu, self_loop=self.use_self_loop,\n",
    "                    dropout=self.dropout, weight=True))\n",
    "        # h2h\n",
    "        if self.num_hidden_layers>1:\n",
    "            for i in range(0,self.num_hidden_layers-1):\n",
    "                self.layers.append(RelGraphConvLayer(\n",
    "                    self.h_dim, self.h_dim, self.rel_names,\n",
    "                    self.num_bases, activation=F.relu, self_loop=self.use_self_loop,\n",
    "                    dropout=self.dropout))\n",
    "        # h2o\n",
    "#         self.layers.append(RelGraphConvLayer(\n",
    "#             self.h_dim, self.out_dim, self.rel_names, \n",
    "#             self.num_bases, activation=partial(F.softmax, dim=1),\n",
    "#             self_loop=self.use_self_loop))\n",
    "        self.classifier = nn.Linear(self.h_dim, self.out_dim)\n",
    "    \n",
    "    def forward(self, input_nodes, blocks=None):\n",
    "        H={}\n",
    "        for ntype, nid in input_nodes.items():\n",
    "            nid = input_nodes[ntype]\n",
    "            H[ntype] = self.node_embed[ntype](nid)\n",
    "        if blocks is None:\n",
    "            for layer in self.layers:\n",
    "                H = layer(self.g, H)\n",
    "        else:\n",
    "            for layer, block in zip(self.layers, blocks):\n",
    "                H = layer(block, H)\n",
    "        output = self.classifier(H[\"usaanr\"])\n",
    "    \n",
    "        return output, H[\"usaanr\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_count_weight(y,n_classes):\n",
    "    classes_count=[]\n",
    "    weight=[]\n",
    "    for i in range(n_classes):\n",
    "        count=np.sum(y.squeeze()==i)\n",
    "        classes_count.append(count)\n",
    "        weight.append(len(y)/(n_classes*count))\n",
    "    return classes_count,weight\n",
    "\n",
    "def eval_loop_func(model, loader, labels, device, loss_weight, num_classes):\n",
    "    model.eval()\n",
    "    fin_targets=[]\n",
    "    fin_outputs=[]\n",
    "    losses=[]\n",
    "    for input_nodes_raw, seeds, blocks in tqdm(loader, position=0, leave=True):\n",
    "        blocks = [blk.to(device) for blk in blocks]\n",
    "        seeds = seeds.to(device)\n",
    "        \n",
    "        input_nodes={}\n",
    "        input_nodes[\"usaanr\"]=input_nodes_raw\n",
    "        input_nodes={k : e.to(device) for k, e in input_nodes.items()}\n",
    "\n",
    "        lbl = labels[seeds].squeeze().to(device)\n",
    "        \n",
    "        with th.no_grad():\n",
    "            logits,h = model(input_nodes,blocks)\n",
    "            if loss_weight is None:\n",
    "                loss = F.cross_entropy(logits.view(-1, num_classes), lbl.to(device))\n",
    "            else:\n",
    "                loss = F.cross_entropy(logits.view(-1, num_classes), lbl.to(device),weight=th.Tensor(loss_weight).to(device))        \n",
    "            losses.append(loss.item())\n",
    "        fin_targets.append(lbl.cpu().detach().numpy())\n",
    "        fin_outputs.append(logits.cpu().detach().numpy())\n",
    "    return np.concatenate(fin_outputs), np.concatenate(fin_targets), losses\n",
    "\n",
    "def evaluate(pred_test, y_test):\n",
    "    \n",
    "    ## convert logits into probability\n",
    "    pred_test=th.nn.functional.softmax(th.from_numpy(pred_test),dim=1).numpy()\n",
    "    \n",
    "    acc = np.sum(pred_test.argmax(axis=1) == y_test.squeeze()) / y_test.shape[0]\n",
    "    prec_macro, recall_macro, fscore_macro, _ = precision_recall_fscore_support(y_test.squeeze(), pred_test.argmax(axis=1), average='macro')\n",
    "    prec_micro, recall_micro, fscore_micro, _ = precision_recall_fscore_support(y_test.squeeze(), pred_test.argmax(axis=1), average='micro')\n",
    "    prec_weighted, recall_weighted, fscore_weighted, _ = precision_recall_fscore_support(y_test.squeeze(), pred_test.argmax(axis=1), average='weighted')\n",
    "    \n",
    "    macro_roc_auc_ovo=roc_auc_score(y_test,pred_test,multi_class=\"ovo\",average=\"macro\")\n",
    "    weighted_roc_auc_ovo=roc_auc_score(y_test,pred_test,multi_class=\"ovo\",average=\"weighted\")\n",
    "\n",
    "    macro_roc_auc_ovr=roc_auc_score(y_test,pred_test,multi_class=\"ovr\",average=\"macro\")\n",
    "    weighted_roc_auc_ovr=roc_auc_score(y_test,pred_test,multi_class=\"ovr\",average=\"weighted\")\n",
    "    \n",
    "    \n",
    "    _, count=np.unique(y_test,return_counts=True)\n",
    "    weight=count/count.sum()\n",
    "    \n",
    "    y_test_binary=label_binarize(y_test, classes=np.unique(y_test).tolist())\n",
    "    \n",
    "    roc_auc = dict()\n",
    "    pr_auc = dict()\n",
    "    n_classes = y_test_binary.shape[1]\n",
    "    for i in range(n_classes):\n",
    "        fpr, tpr, _ = roc_curve(y_test_binary[:, i],pred_test[:, i])\n",
    "        roc_auc[i] = auc_score(fpr, tpr)\n",
    "        \n",
    "        prec,rec,_ = precision_recall_curve(y_test_binary[:, i], th.sigmoid(th.from_numpy(pred_test))[:,i].numpy())\n",
    "        pr_auc[i]=auc_score(rec,prec)\n",
    "\n",
    "    # Compute micro-average ROC curve and ROC area\n",
    "    fpr, tpr, _ = roc_curve(y_test_binary.ravel(), pred_test.ravel())\n",
    "    roc_auc[\"micro\"] = auc_score(fpr, tpr)\n",
    "    roc_auc[\"macro\"]=0\n",
    "    roc_auc[\"weighted\"]=0\n",
    "    for i in range(n_classes):\n",
    "        roc_auc[\"macro\"]+=roc_auc[i]\n",
    "        roc_auc[\"weighted\"]+=roc_auc[i]*weight[i]\n",
    "    roc_auc[\"macro\"]/=n_classes\n",
    "    \n",
    "    prec,rec,_ = precision_recall_curve(y_test_binary.ravel(), th.sigmoid(th.from_numpy(pred_test)).numpy().ravel())\n",
    "    pr_auc[\"micro\"]=auc_score(rec,prec)\n",
    "\n",
    "    pr_auc[\"macro\"]=0\n",
    "    pr_auc[\"weighted\"]=0\n",
    "    for i in range(n_classes):\n",
    "        pr_auc[\"macro\"]+=pr_auc[i]\n",
    "        pr_auc[\"weighted\"]+=pr_auc[i]*weight[i]\n",
    "    pr_auc[\"macro\"]/=n_classes\n",
    "\n",
    "    metrics = {}\n",
    "    metrics['acc'] = acc\n",
    "    metrics['prec_macro'] = prec_macro\n",
    "    metrics['recall_macro'] = recall_macro\n",
    "    metrics['fscore_macro'] = fscore_macro\n",
    "\n",
    "    metrics['prec_micro'] = prec_micro\n",
    "    metrics['recall_micro'] = recall_micro\n",
    "    metrics['fscore_micro'] = fscore_micro\n",
    "\n",
    "    metrics['prec_weighted'] = prec_weighted\n",
    "    metrics['recall_weighted'] = recall_weighted\n",
    "    metrics['fscore_weighted'] = fscore_weighted\n",
    "    \n",
    "    metrics['auc_micro']=roc_auc[\"micro\"]\n",
    "    \n",
    "    metrics['auc_macro_ovo']=macro_roc_auc_ovo\n",
    "    metrics['auc_macro_ovr']=macro_roc_auc_ovr\n",
    "    \n",
    "    metrics['auc_weighted_ovo']=weighted_roc_auc_ovo\n",
    "    metrics['auc_weighted_ovr']=weighted_roc_auc_ovr  \n",
    "    \n",
    "    metrics['pr_auc_micro']=pr_auc[\"micro\"]\n",
    "    metrics['pr_auc_macro']=pr_auc[\"macro\"]\n",
    "    metrics['pr_auc_weighted']=pr_auc[\"weighted\"]\n",
    "\n",
    "    return metrics, roc_auc, pr_auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate(model, loader, labels, category, device):\n",
    "#     model.eval()\n",
    "#     total_loss = 0\n",
    "#     total_acc = 0\n",
    "# #     total_precision=0\n",
    "# #     total_recall=0\n",
    "#     total_fscore_macro=0\n",
    "#     total_fscore_micro=0\n",
    "#     total_fscore_weighted=0\n",
    "# #     total_auc=0\n",
    "# #     total_pr_auc=0\n",
    "    \n",
    "#     count = 0\n",
    "#     count_loss=0\n",
    "    \n",
    "#     y_pred=[]\n",
    "#     y_true=[]\n",
    "    \n",
    "#     for input_nodes_raw, seeds_raw, blocks in tqdm(loader, position=0, leave=True):\n",
    "#         blocks = [blk.to(device) for blk in blocks]\n",
    "        \n",
    "#         seeds=seeds_raw.to(device)\n",
    "        \n",
    "#         input_nodes={}\n",
    "#         input_nodes[category]=input_nodes_raw\n",
    "#         input_nodes={k : e.to(device) for k, e in input_nodes.items()}\n",
    "        \n",
    "#         lbl = labels[seeds].to(device)\n",
    "#         logits,h = model(input_nodes,blocks)\n",
    "#         loss = F.cross_entropy(logits, lbl.squeeze(1).to(device))\n",
    "# #         loss = F.cross_entropy(logits, lbl.squeeze(1),weight=th.Tensor([1,args.weight]).to(device))\n",
    "#         acc = th.sum(logits.argmax(dim=1) == lbl.squeeze(1)).item() / logits.shape[0]\n",
    "# #         precision, recall, fscore, support = score(lbl.squeeze(1).cpu().numpy(), logits.argmax(dim=1).cpu().numpy())\n",
    "\n",
    "#         precision_macro, recall_macro, fscore_macro, _ = score(lbl.squeeze().cpu().numpy(), logits.argmax(dim=1).cpu().numpy(), average='macro')\n",
    "#         precision_micro, recall_micro, fscore_micro, _ = score(lbl.squeeze().cpu().numpy(), logits.argmax(dim=1).cpu().numpy(), average='micro')\n",
    "#         precision_weighted, recall_weighted, fscore_weighted, _ = score(lbl.squeeze().cpu().numpy(), logits.argmax(dim=1).cpu().numpy(), average='weighted')\n",
    "        \n",
    "#         y_pred.extend(logits.argmax(dim=1).tolist())\n",
    "#         y_true.extend(lbl.squeeze().tolist())\n",
    "        \n",
    "# #         tempt=lbl.detach().cpu().numpy()\n",
    "# #         labels_train_one_hot=np.zeros(shape=(tempt.shape[0],8),dtype=np.float32)\n",
    "# #         labels_train_one_hot[np.arange(tempt.shape[0]),np.array([ele.item() for ele in tempt])]=1\n",
    "# #         auc = roc_auc_score(labels_train_one_hot.ravel(), th.sigmoid(logits).detach().cpu().numpy().ravel())\n",
    "        \n",
    "# #         auc = roc_auc_score(lbl.detach().cpu().numpy().ravel(), th.sigmoid(logits)[:,1].detach().cpu().numpy().ravel())\n",
    "# #         prec,rec,_ = precision_recall_curve(lbl.detach().cpu().numpy().ravel(), th.sigmoid(logits)[:,1].detach().cpu().numpy().ravel())\n",
    "# #         pr_auc=auc_score(rec,prec)\n",
    "        \n",
    "#         total_loss += loss.item() * len(seeds) \n",
    "#         total_acc += acc\n",
    "# #         total_precision += precision[1]\n",
    "# #         total_recall += recall[1]\n",
    "#         total_fscore_macro += fscore_macro\n",
    "#         total_fscore_micro += fscore_micro\n",
    "#         total_fscore_weighted += fscore_weighted\n",
    "# #         total_auc += auc\n",
    "# #         total_pr_auc += pr_auc\n",
    "#         count += 1\n",
    "#         count_loss += len(seeds)\n",
    "    \n",
    "#     ACCURACY=total_acc / count\n",
    "#     LOSS=total_loss / count_loss\n",
    "# #     PRECISION=total_precision/count\n",
    "# #     RECALL=total_recall/count\n",
    "#     F1_SCORE_macro=total_fscore_macro/count\n",
    "#     F1_SCORE_micro=total_fscore_micro/count\n",
    "#     F1_SCORE_weighted=total_fscore_weighted/count\n",
    "# #     AUC=total_auc/count\n",
    "# #     PR_AUC=total_pr_auc/count\n",
    "    \n",
    "#     return ACCURACY, LOSS, F1_SCORE_macro, F1_SCORE_micro, F1_SCORE_weighted, y_pred, y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, graph, input_features, batch_size):\n",
    "    \"\"\"Minibatch inference of final representation over all node types.\n",
    "    ***NOTE***\n",
    "    For node classification, the model is trained to predict on only one node type's\n",
    "    label.  Therefore, only that type's final representation is meaningful.\n",
    "    \"\"\"\n",
    "    with th.no_grad():\n",
    "        for l, layer in enumerate(model.layers):\n",
    "            \n",
    "            output_features = {\n",
    "                k: th.zeros(\n",
    "                    graph.number_of_nodes(k),\n",
    "                    args.h_dim )\n",
    "                for k in graph.ntypes}\n",
    "            sampler = dgl.dataloading.MultiLayerFullNeighborSampler(1)\n",
    "            dataloader = dgl.dataloading.NodeDataLoader(\n",
    "                graph,\n",
    "                {k: th.arange(graph.number_of_nodes(k)) for k in graph.ntypes},\n",
    "                sampler,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True,\n",
    "                drop_last=False,\n",
    "                num_workers=0)\n",
    "            \n",
    "#             dataloader=[next(iter(dataloader)) for idx in range(10)]\n",
    "            for input_nodes, output_nodes, blocks in tqdm(dataloader, position=0, leave=True):\n",
    "                block = blocks[0].to(device)\n",
    "                h = {k: input_features[k][input_nodes[k]].to(device) for k in input_nodes.keys()}\n",
    "                h = layer(block, h)\n",
    "                for k in h.keys():\n",
    "#                     h[k]=th.matmul(h[k],model.classifier.weight.t())+model.classifier.bias\n",
    "                    output_features[k][output_nodes[k]] = h[k].cpu()\n",
    "            input_features = output_features\n",
    "        \n",
    "    return model.classifier(output_features[\"usaanr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_edges={}\n",
    "# for etype in G.etypes:\n",
    "#     dict_edges[etype]=th.arange(G.num_edges(etype))[0:5000]\n",
    "# G=dgl.edge_subgraph(G,dict_edges)\n",
    "\n",
    "# G.nodes['usaanr'].data[\"_ID\"].numpy().shape,node_labels.shape, node_labels[G.nodes['usaanr'].data[\"_ID\"]].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='RGCN')\n",
    "parser.add_argument(\"--dropout\", type=float, default=0,\n",
    "        help=\"dropout probability\")\n",
    "parser.add_argument(\"--h_dim\", type=int, default=128,\n",
    "        help=\"number of hidden units\")\n",
    "parser.add_argument(\"--out_dim\", type=int, default=1,\n",
    "        help=\"output dimension\")\n",
    "parser.add_argument(\"--gpu\", type=int, default=0,\n",
    "        help=\"gpu\")\n",
    "parser.add_argument(\"--lr\", type=float, default=1e-5,\n",
    "        help=\"learning rate\")\n",
    "parser.add_argument(\"--num_bases\", type=int, default=-1,\n",
    "        help=\"number of filter weight matrices, default: -1 [use all]\")\n",
    "parser.add_argument(\"--num_layers\", type=int, default=1,\n",
    "        help=\"number of propagation rounds\")\n",
    "parser.add_argument(\"-e\", \"--n_epochs\", type=int, default=1,\n",
    "        help=\"number of training epochs\")\n",
    "parser.add_argument(\"--model_path\", type=str, default=\"/workspace/cjiang/eagle_project/CAP_graph/CAP_without_zipcode/rgcn_model_param.pt\",\n",
    "        help='path for save the model')\n",
    "parser.add_argument(\"--l2norm\", type=float, default=0,\n",
    "        help=\"l2 norm coef\")\n",
    "parser.add_argument(\"--use_self_loop\", default=True, action='store_true',\n",
    "        help=\"include self feature as a special relation\")\n",
    "parser.add_argument(\"--batch-size\", type=int, default=1024,\n",
    "        help=\"Mini-batch size. If -1, use full graph training.\")\n",
    "parser.add_argument(\"--num_mini_batch\", type=int, default=8,\n",
    "        help=\"Number of minibatch.\")\n",
    "parser.add_argument(\"--fanout\", type=int, default=None,\n",
    "        help=\"Fan-out of neighbor sampling.\")\n",
    "parser.add_argument(\"--validation\",  default=True,\n",
    "        help=\"set up validation .\")\n",
    "parser.add_argument(\"--seed\",  type=int,default=101,\n",
    "        help=\"random seed for np.random.seed, torch.manual_seed and torch.cuda.manual_seed.\")\n",
    "\n",
    "parser.add_argument(\"--weight\",  type=float,default=1,  ## number of label=0/number of label=1\n",
    "        help=\"weight for unbalance data\")\n",
    "parser.add_argument(\"--num_worker\",  type=int,default=0,  \n",
    "        help=\"number of worker for neighbor sampling\") \n",
    "    \n",
    "args,unknown=parser.parse_known_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.num_layers=1\n",
    "args.dropout=0.2\n",
    "args.lr=1e-3\n",
    "args.l2norm=1e-3\n",
    "args.n_epochs=1\n",
    "args.num_bases=5\n",
    "args.h_dim=64\n",
    "args.batch_size=1024*10\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### regroup label to eliminate rare category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=node_labels.squeeze().numpy()\n",
    "n_classes=th.unique(node_labels).shape[0]\n",
    "classes_count,weight=get_class_count_weight(y,n_classes)\n",
    "imbalance_classes={}\n",
    "imbalance_classes[\"category\"]=th.unique(node_labels).tolist()\n",
    "imbalance_classes[\"product_type\"]=[\"No Product\",\"Rental Only\",\"Home Only\",\"Home + Rental\",\"Auto Only\",\"Auto + Rental\",\"Auto + Home\",\"Auto + Home + Rental\"]\n",
    "imbalance_classes[\"count\"]=classes_count\n",
    "imbalance_classes[\"weight\"]=weight\n",
    "imbalance_classes=pd.DataFrame(imbalance_classes)\n",
    "imbalance_classes.style.format({\"count\":\"{:,}\",\"weight\":\"{:.2f}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_labels_new=node_labels.clone()\n",
    "node_labels_new[node_labels_new==3]=2\n",
    "node_labels_new[node_labels_new==4]=3\n",
    "node_labels_new[node_labels_new==5]=4\n",
    "node_labels_new[node_labels_new==6]=5\n",
    "node_labels_new[node_labels_new==7]=5\n",
    "\n",
    "label_map={0:\"No Product\",1:\"Rental Only\",2:\"Home Only\",3:\"Auto Only\",4:\"Auto + Rental\",5:\"Auto + Home\"}\n",
    "\n",
    "y=node_labels_new.squeeze().numpy()\n",
    "n_classes=th.unique(node_labels_new).shape[0]\n",
    "classes_count,weight=get_class_count_weight(y,n_classes)\n",
    "imbalance_classes={}\n",
    "imbalance_classes[\"category\"]=th.unique(node_labels_new).tolist()\n",
    "# imbalance_classes[\"product_type\"]=[\"No Product\",\"Rental Only\",\"Home Only\",\"Auto Only\",\"Auto + Rental\",\"Auto + Home\"]\n",
    "imbalance_classes[\"product_type\"]=[label_map[i] for i in range(n_classes)]\n",
    "imbalance_classes[\"count\"]=classes_count\n",
    "imbalance_classes[\"weight\"]=weight\n",
    "imbalance_classes=pd.DataFrame(imbalance_classes)\n",
    "imbalance_classes.style.format({\"count\":\"{:,}\",\"weight\":\"{:.2f}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.distplot(node_labels_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### setting up training, validation and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mask=G.nodes[\"usaanr\"].data.pop('train_mask')\n",
    "val_mask=G.nodes[\"usaanr\"].data.pop('val_mask')\n",
    "test_mask=G.nodes[\"usaanr\"].data.pop('test_mask')\n",
    "\n",
    "mask=((train_mask==True) | (val_mask==True))\n",
    "\n",
    "train_idx=th.nonzero(train_mask.squeeze(1)).numpy()\n",
    "val_idx=th.nonzero(val_mask.squeeze(1)).numpy()\n",
    "test_idx=th.nonzero(test_mask.squeeze(1)).numpy()\n",
    "\n",
    "train_idx=th.from_numpy(train_idx).squeeze(1)    \n",
    "val_idx=th.from_numpy(val_idx).squeeze(1)    \n",
    "test_idx=th.from_numpy(test_idx).squeeze(1)\n",
    "\n",
    "train_label=node_labels_new[train_idx].squeeze().numpy()\n",
    "val_label=node_labels_new[val_idx].squeeze().numpy()\n",
    "test_label=node_labels_new[test_idx].squeeze().numpy()\n",
    "\n",
    "print('{:<15} {:<10,}'.format(\"Training set\",train_idx.shape[0]))\n",
    "print('{:<15} {:<10,}'.format(\"validation set\",val_idx.shape[0]))\n",
    "print('{:<15} {:<10,}'.format(\"test set\",test_idx.shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### check cuda\n",
    "device=\"cpu\"\n",
    "use_cuda=args.gpu>=0 and th.cuda.is_available()\n",
    "if use_cuda:\n",
    "    th.cuda.set_device(args.gpu)\n",
    "    device='cuda:%d' % args.gpu\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "model = Entity_Classify(G,\n",
    "                       args.h_dim,\n",
    "                       n_classes,\n",
    "                       num_bases=args.num_bases,\n",
    "                       num_hidden_layers=args.num_layers,\n",
    "                       dropout=args.dropout,\n",
    "                       use_self_loop=args.use_self_loop)\n",
    "if use_cuda:\n",
    "    model.cuda()\n",
    "\n",
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = th.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.l2norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train sampler\n",
    "sampler = dgl.dataloading.MultiLayerNeighborSampler([args.fanout] * args.num_layers)\n",
    "train_loader = dgl.dataloading.NodeDataLoader(\n",
    "    G, {'usaanr': train_idx}, sampler,\n",
    "    batch_size=args.batch_size, shuffle=True, num_workers=args.num_worker)\n",
    "# validation sampler\n",
    "# we do not use full neighbor to save computation resources\n",
    "val_sampler = dgl.dataloading.MultiLayerNeighborSampler([args.fanout] * args.num_layers)\n",
    "val_loader = dgl.dataloading.NodeDataLoader(\n",
    "    G, {'usaanr': val_idx}, val_sampler,\n",
    "    batch_size=args.batch_size, shuffle=False, num_workers=args.num_worker)\n",
    "\n",
    "test_sampler = dgl.dataloading.MultiLayerNeighborSampler([args.fanout] * args.num_layers)\n",
    "test_loader = dgl.dataloading.NodeDataLoader(\n",
    "    G, {'usaanr': test_idx}, test_sampler,\n",
    "    batch_size=args.batch_size, shuffle=False, num_workers=args.num_worker)\n",
    "\n",
    "print(\"The number of minibatch in training set is {:,}\".format(len(train_loader)))\n",
    "print(\"The number of minibatch in validation set is {:,}\".format(len(val_loader)))\n",
    "print(\"The number of minibatch in test set is {:,}\".format(len(test_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The total # of parameter is {:,}\".format(sum([p.nelement() for p in model.parameters()]) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dict={n: p.nelement() for n, p in model.named_parameters()}\n",
    "for i,j in param_dict.items():\n",
    "    print(\"{:<70}{:<15,}\".format(i,j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pdb\n",
    "LOSS_EPOCH=[]\n",
    "LABEL_TRAIN=[]\n",
    "\n",
    "# training loop\n",
    "print(\"start training...\")\n",
    "\n",
    "total_loss=0\n",
    "losses=[]\n",
    "\n",
    "train_true=[]\n",
    "train_pred=[]\n",
    "\n",
    "# th.manual_seed(args.seed)\n",
    "# th.cuda.manual_seed(args.seed)\n",
    "# th.cuda.manual_seed_all(args.seed)\n",
    "# np.ranom.seed(args.seed)\n",
    "# random.seed(args.seed)\n",
    "# th.backends.cudnn.deterministic=True\n",
    "\n",
    "for epoch in tqdm(range(0,args.n_epochs)):\n",
    "    \n",
    "    model.train()\n",
    "    IDX=[]\n",
    "    H=[]\n",
    "    \n",
    "    #====================================#\n",
    "    #            Traning                 #\n",
    "    #====================================#\n",
    "    print(\"\")\n",
    "    print(\"========= Epoch {:} /{:}\".format(epoch+1,args.n_epochs))\n",
    "    print(\"Training...\")\n",
    "    t0 = time.time()\n",
    "    for step, (input_nodes_raw, seeds_raw, blocks) in enumerate(train_loader):\n",
    "        blocks = [blk.to(device) for blk in blocks]\n",
    "        \n",
    "        seeds=seeds_raw.to(device)\n",
    "        \n",
    "        labels_train=node_labels_new[seeds]       \n",
    "        labels_train = labels_train.to(device)\n",
    "        \n",
    "        input_nodes={}\n",
    "        input_nodes[\"usaanr\"]=input_nodes_raw\n",
    "        input_nodes={k : e.to(device) for k, e in input_nodes.items()}\n",
    "        \n",
    "        logits,h = model(input_nodes,blocks)\n",
    "        optimizer.zero_grad()\n",
    "        loss = F.cross_entropy(logits, labels_train.squeeze(1),weight=th.Tensor(weight).to(device))\n",
    "#         loss = F.cross_entropy(logits, labels_train.squeeze(1).to(device))\n",
    "        total_loss+=loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        train_pred.extend(logits.argmax(dim=1).tolist())\n",
    "        train_true.extend(labels_train.squeeze(1).tolist())\n",
    "\n",
    "        train_acc = th.sum(logits.argmax(dim=1) == labels_train.squeeze(1)).item() / len(seeds)\n",
    "        precision_macro, recall_macro, fscore_macro, _ = precision_recall_fscore_support(labels_train.squeeze(1).cpu().numpy(), logits.argmax(dim=1).cpu().numpy(), average='macro')\n",
    "        precision_micro, recall_micro, fscore_micro, _ = precision_recall_fscore_support(labels_train.squeeze(1).cpu().numpy(), logits.argmax(dim=1).cpu().numpy(), average='micro')\n",
    "        precision_weighted, recall_weighted, fscore_weighted, _ = precision_recall_fscore_support(labels_train.squeeze(1).cpu().numpy(), logits.argmax(dim=1).cpu().numpy(), average='weighted')\n",
    "\n",
    "#         macro_roc_auc_ovo=roc_auc_score(labels_train,logits,multi_class=\"ovo\",average=\"macro\")\n",
    "#         weighted_roc_auc_ovo=roc_auc_score(labels_train,logits,multi_class=\"ovo\",average=\"weighted\")\n",
    "        \n",
    "        \n",
    "#         _, count=np.unique(labels_train,return_counts=True)\n",
    "#         W=count/count.sum()\n",
    "\n",
    "#         labels_train_binary=label_binarize(labels_train, classes=np.unique(labels_train).tolist())\n",
    "\n",
    "#         roc_auc = dict()\n",
    "#         pr_auc = dict()\n",
    "#         n_classes = labels_train_binary.shape[1]\n",
    "#         for i in range(n_classes):\n",
    "#             fpr, tpr, _ = roc_curve(labels_train_binary[:, i],preds[:, i])\n",
    "#             roc_auc[i] = auc_score(fpr, tpr)\n",
    "\n",
    "#             prec,rec,_ = precision_recall_curve(labels_train_binary[:, i], th.sigmoid(th.from_numpy(logits))[:,i].numpy())\n",
    "#             pr_auc[i]=auc_score(rec,prec)\n",
    "\n",
    "#         # Compute micro-average ROC curve and ROC area\n",
    "#         fpr, tpr, _ = roc_curve(labels_train_binary.ravel(), logits.ravel())\n",
    "#         roc_auc[\"micro\"] = auc_score(fpr, tpr)\n",
    "#         roc_auc[\"macro\"]=0\n",
    "#         roc_auc[\"weighted\"]=0\n",
    "#         for i in range(n_classes):\n",
    "#             roc_auc[\"macro\"]+=roc_auc[i]\n",
    "#             roc_auc[\"weighted\"]+=roc_auc[i]*weight[i]\n",
    "#         roc_auc[\"macro\"]/=n_classes\n",
    "\n",
    "#         prec,rec,_ = precision_recall_curve(labels_train_binary.ravel(), torch.sigmoid(torch.from_numpy(logits)).numpy().ravel())\n",
    "#         pr_auc[\"micro\"]=auc_score(rec,prec)\n",
    "\n",
    "#         pr_auc[\"macro\"]=0\n",
    "#         pr_auc[\"weighted\"]=0\n",
    "#         for i in range(n_classes):\n",
    "#             pr_auc[\"macro\"]+=pr_auc[i]\n",
    "#             pr_auc[\"weighted\"]+=pr_auc[i]*weight[i]\n",
    "#         pr_auc[\"macro\"]/=n_classes\n",
    "\n",
    "\n",
    "        IDX.extend(seeds.detach().cpu().numpy().tolist())\n",
    "        H.extend(h.detach().cpu().numpy().tolist())\n",
    "        \n",
    "        if step%(len(train_loader)//10)==0 and not step==0:\n",
    "\n",
    "            t1 = time.time()\n",
    "            elapsed=utils.format_time(t1-t0)\n",
    "            print(\"Batch {:} of {:} | Loss {:.3f}  | Accuracy {:.2%} | F1-Score(macro) {:.2%}  | F1-Score(micro) {:.2%} | F1-Score(weighted) {:.2%} | Elapsed: {:}\".\\\n",
    "                  format(step,len(train_loader),np.mean(losses[-10:]),train_acc,fscore_macro,fscore_micro,fscore_weighted,elapsed))    \n",
    "    \n",
    "    LOSS_EPOCH.append(loss)\n",
    "\n",
    "    LABEL_TRAIN.append(node_labels_new[blocks[-1].nodes[\"usaanr\"].data[dgl.NID].cpu().numpy()])\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    print()\n",
    "    print(\"\")\n",
    "    print(\"Running Validation on training set\")\n",
    "    print(\"\")\n",
    "    fin_outputs, fin_targets, losses_tmp=eval_loop_func(model, train_loader, node_labels_new,  device, weight, n_classes)\n",
    "    \n",
    "    avg_loss_train=np.mean(losses_tmp)\n",
    "    \n",
    "    metrics_train, roc_auc_train, pr_auc_train = evaluate(fin_outputs, fin_targets)\n",
    "    \n",
    "    t2=time.time()\n",
    "    \n",
    "    print(\"loss: {:.3f} |  Accuracy {:.2%} | F1-Score(macro) {:.2%}  | F1-Score(weighted) {:.2%}  | roc-auc(macro) {:.2%}  | roc-auc(weighted) {:.2%}  | pr-auc(macro) {:.2%}  | pr-auc(weighted) {:.2%}  | Elapsed: {:}\"\\\n",
    "      .format(avg_loss_train, metrics_train[\"acc\"],  metrics_train['fscore_macro'], metrics_train['fscore_weighted'],  \\\n",
    "              metrics_train['auc_macro_ovo'], metrics_train['auc_weighted_ovo'],metrics_train['pr_auc_macro'], \\\n",
    "              metrics_train['pr_auc_weighted'],utils.format_time(t2-t1)))\n",
    "\n",
    "    #====================================#\n",
    "    #            Validation-set          #\n",
    "    #====================================#\n",
    "    \n",
    "    model.eval()\n",
    "    print()\n",
    "    print(\"\")\n",
    "    print(\"Running Validation on validation set\")\n",
    "    print(\"\")\n",
    "    \n",
    "    fin_outputs, fin_targets, losses_tmp=eval_loop_func(model, val_loader, node_labels_new, device, weight, n_classes)\n",
    "    \n",
    "    avg_loss_val=np.mean(losses_tmp)\n",
    "    \n",
    "    metrics_val, roc_auc_val, pr_auc_val = evaluate(fin_outputs, fin_targets)\n",
    "    \n",
    "    t3=time.time()\n",
    "    \n",
    "    print(\"loss: {:.3f} |  Accuracy {:.2%} | F1-Score(macro) {:.2%}  | F1-Score(weighted) {:.2%}  | roc-auc(macro) {:.2%}  | roc-auc(weighted) {:.2%}  | pr-auc(macro) {:.2%}  | pr-auc(weighted) {:.2%}  | Elapsed: {:}\"\\\n",
    "          .format(avg_loss_val, metrics_val[\"acc\"],  metrics_val['fscore_macro'], metrics_val['fscore_weighted'],  \\\n",
    "          metrics_val['auc_macro_ovo'], metrics_val['auc_weighted_ovo'],metrics_val['pr_auc_macro'], \\\n",
    "          metrics_val['pr_auc_weighted'],utils.format_time(t3-t2)))\n",
    "    \n",
    "# if args.model_path is not None:\n",
    "#     th.save(model.state_dict(), args.model_path)\n",
    "    \n",
    "#====================================#\n",
    "#            Test-set                #\n",
    "#====================================#\n",
    "print()\n",
    "print(\"\")\n",
    "print(\"Running Validation in Test Dataset\")\n",
    "print(\"\")\n",
    "model.eval()\n",
    "\n",
    "fin_outputs, fin_targets, losses_tmp=eval_loop_func(model, test_loader, node_labels_new,  device, weight, n_classes)\n",
    "    \n",
    "avg_loss_test=np.mean(losses_tmp)\n",
    "\n",
    "metrics_test, roc_auc_test, pr_auc_test = evaluate(fin_outputs, fin_targets)\n",
    "\n",
    "t4=time.time()\n",
    "\n",
    "print(\"loss: {:.3f} |  Accuracy {:.2%} | F1-Score(macro) {:.2%}  | F1-Score(weighted) {:.2%}  | roc-auc(macro) {:.2%}  | roc-auc(weighted) {:.2%}  | pr-auc(macro) {:.2%}  | pr-auc(weighted) {:.2%}  | Elapsed: {:}\"\\\n",
    "  .format(avg_loss_test, metrics_test[\"acc\"],  metrics_test['fscore_macro'], metrics_test['fscore_weighted'],  \\\n",
    "          metrics_test['auc_macro_ovo'], metrics_test['auc_weighted_ovo'],metrics_test['pr_auc_macro'], \\\n",
    "          metrics_test['pr_auc_weighted'],utils.format_time(t4-t3)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print()\n",
    "print(\"********  model performance in training set **********  \\n \")\n",
    "print()\n",
    "    \n",
    "fin_outputs, fin_targets, losses_tmp=eval_loop_func(model, train_loader, node_labels_new, device, weight, n_classes)\n",
    "metrics_dict, roc_auc, pr_auc = evaluate(fin_outputs, fin_targets)\n",
    "\n",
    "print(\"{:<20}{:<10.2%}\".format(\"accuracy\", metrics_dict['acc']))\n",
    "print()\n",
    "print(\"{:<20}{:<10,.2%}{:<16}{:<10,.2%}{:<18}{:<10,.2%}{:<17}{:<10,.2%}{:<16}{:<10,.2%}\"\\\n",
    "      .format(\"precision(macro):\",metrics_dict['prec_macro'],\"recall(macro):\",metrics_dict['recall_macro'],\\\n",
    "              \"f1-score(macro):\",metrics_dict['fscore_macro'],\"ROC-AUC(macro):\",metrics_dict['auc_macro_ovo'],\\\n",
    "             \"PR-AUC(macro):\",metrics_dict['pr_auc_macro']))\n",
    "\n",
    "print(\"{:<20}{:<10,.2%}{:<16}{:<10,.2%}{:<18}{:<10,.2%}{:<17}{:<10,.2%}{:<16}{:<10,.2%}\"\\\n",
    "      .format(\"precision(micro):\",metrics_dict['prec_micro'],\"recall(micro):\",metrics_dict['recall_micro'],\\\n",
    "              \"f1-score(micro):\",metrics_dict['fscore_micro'],\"ROC-AUC(micro):\",metrics_dict['auc_micro'],\\\n",
    "             \"PR-AUC(micro):\",metrics_dict['pr_auc_micro']))\n",
    "\n",
    "print(\"{:<20}{:<10,.2%}{:<16}{:<10,.2%}{:<18}{:<10,.2%}{:<17}{:<10,.2%}{:<16}{:<10,.2%}\"\\\n",
    "      .format(\"precision(weight):\",metrics_dict['prec_weighted'],\"recall(weight):\",metrics_dict['recall_weighted'],\\\n",
    "              \"f1-score(weight):\",metrics_dict['fscore_weighted'],\"ROC-AUC(weight):\",metrics_dict['auc_weighted_ovo'],\\\n",
    "             \"PR-AUC(weight):\",metrics_dict['pr_auc_weighted']))\n",
    "\n",
    "print()\n",
    "n_classes=th.unique(node_labels_new).shape[0]\n",
    "label_map={0:\"No Product\",1:\"Rental Only\",2:\"Home Only\",3:\"Auto Only\",4:\"Auto + Rental\",5:\"Auto + Home\"}\n",
    "\n",
    "report=metrics.classification_report(fin_targets.squeeze(), fin_outputs.argmax(axis=1), output_dict=True)\n",
    "\n",
    "table = pd.DataFrame(report).transpose().iloc[:n_classes,:]\n",
    "table[\"count\"]=table[\"support\"].astype(int)\n",
    "table[\"roc_auc\"]=[roc_auc[i] for i in range(n_classes)]\n",
    "table[\"pr_auc\"]=[pr_auc[i] for i in range(n_classes)]\n",
    "table[\"product_type\"]=[label_map[i] for i in range(n_classes)]\n",
    "table=table[['product_type','count','precision','recall','f1-score','roc_auc','pr_auc']]\n",
    "\n",
    "total=table['count'].sum()\n",
    "\n",
    "table.loc[len(table.index)]=[\"MACRO\",total,metrics_dict['prec_macro'],metrics_dict['recall_macro'],metrics_dict['fscore_macro'],\\\n",
    "                        metrics_dict['auc_macro_ovo'],metrics_dict['pr_auc_macro']]\n",
    "\n",
    "table.loc[len(table.index)]=[\"MICRO\",total,metrics_dict['prec_micro'],metrics_dict['recall_micro'],metrics_dict['fscore_micro'],\\\n",
    "                            metrics_dict['auc_micro'],metrics_dict['pr_auc_micro']]\n",
    "\n",
    "table.loc[len(table.index)]=[\"WEIGHT\",total,metrics_dict['prec_weighted'],metrics_dict['recall_weighted'],metrics_dict['fscore_weighted'],\\\n",
    "                        metrics_dict['auc_weighted_ovo'],metrics_dict['pr_auc_weighted']]\n",
    "\n",
    "table.style.format({\"count\":\"{:,}\",\"f1-score\":\"{:.2%}\",\"precision\":\"{:.2%}\",\"recall\":\"{:.2%}\",\"roc_auc\":\"{:.2%}\",\"pr_auc\":\"{:.2%}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print()\n",
    "print(\"********  model performance in validation set **********  \\n \")\n",
    "print()\n",
    "    \n",
    "fin_outputs, fin_targets, losses_tmp=eval_loop_func(model, val_loader, node_labels_new, device, weight, n_classes)\n",
    "metrics_dict, roc_auc, pr_auc = evaluate(fin_outputs, fin_targets)\n",
    "\n",
    "print(\"{:<20}{:<10.2%}\".format(\"accuracy\", metrics_dict['acc']))\n",
    "print()\n",
    "print(\"{:<20}{:<10,.2%}{:<16}{:<10,.2%}{:<18}{:<10,.2%}{:<17}{:<10,.2%}{:<16}{:<10,.2%}\"\\\n",
    "      .format(\"precision(macro):\",metrics_dict['prec_macro'],\"recall(macro):\",metrics_dict['recall_macro'],\\\n",
    "              \"f1-score(macro):\",metrics_dict['fscore_macro'],\"ROC-AUC(macro):\",metrics_dict['auc_macro_ovo'],\\\n",
    "             \"PR-AUC(macro):\",metrics_dict['pr_auc_macro']))\n",
    "\n",
    "print(\"{:<20}{:<10,.2%}{:<16}{:<10,.2%}{:<18}{:<10,.2%}{:<17}{:<10,.2%}{:<16}{:<10,.2%}\"\\\n",
    "      .format(\"precision(micro):\",metrics_dict['prec_micro'],\"recall(micro):\",metrics_dict['recall_micro'],\\\n",
    "              \"f1-score(micro):\",metrics_dict['fscore_micro'],\"ROC-AUC(micro):\",metrics_dict['auc_micro'],\\\n",
    "             \"PR-AUC(micro):\",metrics_dict['pr_auc_micro']))\n",
    "\n",
    "print(\"{:<20}{:<10,.2%}{:<16}{:<10,.2%}{:<18}{:<10,.2%}{:<17}{:<10,.2%}{:<16}{:<10,.2%}\"\\\n",
    "      .format(\"precision(weight):\",metrics_dict['prec_weighted'],\"recall(weight):\",metrics_dict['recall_weighted'],\\\n",
    "              \"f1-score(weight):\",metrics_dict['fscore_weighted'],\"ROC-AUC(weight):\",metrics_dict['auc_weighted_ovo'],\\\n",
    "             \"PR-AUC(weight):\",metrics_dict['pr_auc_weighted']))\n",
    "\n",
    "print()\n",
    "n_classes=th.unique(node_labels_new).shape[0]\n",
    "label_map={0:\"No Product\",1:\"Rental Only\",2:\"Home Only\",3:\"Auto Only\",4:\"Auto + Rental\",5:\"Auto + Home\"}\n",
    "\n",
    "report=metrics.classification_report(fin_targets.squeeze(), fin_outputs.argmax(axis=1), output_dict=True)\n",
    "\n",
    "table = pd.DataFrame(report).transpose().iloc[:n_classes,:]\n",
    "table[\"count\"]=table[\"support\"].astype(int)\n",
    "table[\"roc_auc\"]=[roc_auc[i] for i in range(n_classes)]\n",
    "table[\"pr_auc\"]=[pr_auc[i] for i in range(n_classes)]\n",
    "table[\"product_type\"]=[label_map[i] for i in range(n_classes)]\n",
    "table=table[['product_type','count','precision','recall','f1-score','roc_auc','pr_auc']]\n",
    "\n",
    "total=table['count'].sum()\n",
    "\n",
    "table.loc[len(table.index)]=[\"MACRO\",total,metrics_dict['prec_macro'],metrics_dict['recall_macro'],metrics_dict['fscore_macro'],\\\n",
    "                        metrics_dict['auc_macro_ovo'],metrics_dict['pr_auc_macro']]\n",
    "\n",
    "table.loc[len(table.index)]=[\"MICRO\",total,metrics_dict['prec_micro'],metrics_dict['recall_micro'],metrics_dict['fscore_micro'],\\\n",
    "                            metrics_dict['auc_micro'],metrics_dict['pr_auc_micro']]\n",
    "\n",
    "table.loc[len(table.index)]=[\"WEIGHT\",total,metrics_dict['prec_weighted'],metrics_dict['recall_weighted'],metrics_dict['fscore_weighted'],\\\n",
    "                        metrics_dict['auc_weighted_ovo'],metrics_dict['pr_auc_weighted']]\n",
    "\n",
    "table.style.format({\"count\":\"{:,}\",\"f1-score\":\"{:.2%}\",\"precision\":\"{:.2%}\",\"recall\":\"{:.2%}\",\"roc_auc\":\"{:.2%}\",\"pr_auc\":\"{:.2%}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print()\n",
    "print(\"********  model performance in test set **********  \\n \")\n",
    "print()\n",
    "    \n",
    "fin_outputs, fin_targets, losses_tmp=eval_loop_func(model, test_loader, node_labels_new, device, weight, n_classes)\n",
    "metrics_dict, roc_auc, pr_auc = evaluate(fin_outputs, fin_targets)\n",
    "\n",
    "print(\"{:<20}{:<10.2%}\".format(\"accuracy\", metrics_dict['acc']))\n",
    "print()\n",
    "print(\"{:<20}{:<10,.2%}{:<16}{:<10,.2%}{:<18}{:<10,.2%}{:<17}{:<10,.2%}{:<16}{:<10,.2%}\"\\\n",
    "      .format(\"precision(macro):\",metrics_dict['prec_macro'],\"recall(macro):\",metrics_dict['recall_macro'],\\\n",
    "              \"f1-score(macro):\",metrics_dict['fscore_macro'],\"ROC-AUC(macro):\",metrics_dict['auc_macro_ovo'],\\\n",
    "             \"PR-AUC(macro):\",metrics_dict['pr_auc_macro']))\n",
    "\n",
    "print(\"{:<20}{:<10,.2%}{:<16}{:<10,.2%}{:<18}{:<10,.2%}{:<17}{:<10,.2%}{:<16}{:<10,.2%}\"\\\n",
    "      .format(\"precision(micro):\",metrics_dict['prec_micro'],\"recall(micro):\",metrics_dict['recall_micro'],\\\n",
    "              \"f1-score(micro):\",metrics_dict['fscore_micro'],\"ROC-AUC(micro):\",metrics_dict['auc_micro'],\\\n",
    "             \"PR-AUC(micro):\",metrics_dict['pr_auc_micro']))\n",
    "\n",
    "print(\"{:<20}{:<10,.2%}{:<16}{:<10,.2%}{:<18}{:<10,.2%}{:<17}{:<10,.2%}{:<16}{:<10,.2%}\"\\\n",
    "      .format(\"precision(weight):\",metrics_dict['prec_weighted'],\"recall(weight):\",metrics_dict['recall_weighted'],\\\n",
    "              \"f1-score(weight):\",metrics_dict['fscore_weighted'],\"ROC-AUC(weight):\",metrics_dict['auc_weighted_ovo'],\\\n",
    "             \"PR-AUC(weight):\",metrics_dict['pr_auc_weighted']))\n",
    "\n",
    "print()\n",
    "n_classes=th.unique(node_labels_new).shape[0]\n",
    "label_map={0:\"No Product\",1:\"Rental Only\",2:\"Home Only\",3:\"Auto Only\",4:\"Auto + Rental\",5:\"Auto + Home\"}\n",
    "\n",
    "report=metrics.classification_report(fin_targets.squeeze(), fin_outputs.argmax(axis=1), output_dict=True)\n",
    "\n",
    "table = pd.DataFrame(report).transpose().iloc[:n_classes,:]\n",
    "table[\"count\"]=table[\"support\"].astype(int)\n",
    "table[\"roc_auc\"]=[roc_auc[i] for i in range(n_classes)]\n",
    "table[\"pr_auc\"]=[pr_auc[i] for i in range(n_classes)]\n",
    "table[\"product_type\"]=[label_map[i] for i in range(n_classes)]\n",
    "table=table[['product_type','count','precision','recall','f1-score','roc_auc','pr_auc']]\n",
    "\n",
    "total=table['count'].sum()\n",
    "\n",
    "table.loc[len(table.index)]=[\"MACRO\",total,metrics_dict['prec_macro'],metrics_dict['recall_macro'],metrics_dict['fscore_macro'],\\\n",
    "                        metrics_dict['auc_macro_ovo'],metrics_dict['pr_auc_macro']]\n",
    "\n",
    "table.loc[len(table.index)]=[\"MICRO\",total,metrics_dict['prec_micro'],metrics_dict['recall_micro'],metrics_dict['fscore_micro'],\\\n",
    "                            metrics_dict['auc_micro'],metrics_dict['pr_auc_micro']]\n",
    "\n",
    "table.loc[len(table.index)]=[\"WEIGHT\",total,metrics_dict['prec_weighted'],metrics_dict['recall_weighted'],metrics_dict['fscore_weighted'],\\\n",
    "                        metrics_dict['auc_weighted_ovo'],metrics_dict['pr_auc_weighted']]\n",
    "\n",
    "table.style.format({\"count\":\"{:,}\",\"f1-score\":\"{:.2%}\",\"precision\":\"{:.2%}\",\"recall\":\"{:.2%}\",\"roc_auc\":\"{:.2%}\",\"pr_auc\":\"{:.2%}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# print(\"Classification Report for Training Dataset: \\n\")\n",
    "# print()\n",
    "# precision_macro, recall_macro, fscore_macro, _ = score(train_true, train_pred, average='macro')\n",
    "# precision_micro, recall_micro, fscore_micro, _ = score(train_true, train_pred, average='micro')\n",
    "# precision_weighted, recall_weighted, fscore_weighted, _ = score(train_true, train_pred, average='weighted')\n",
    "# print(\"{:<20}{:<10,.2%}{:<18}{:<10,.2%}{:<20}{:<10,.2%}\".format(\"precision(macro):\",precision_macro,\"recall(macro):\",recall_macro,\"f1-score(macro):\",fscore_macro))\n",
    "# print(\"{:<20}{:<10,.2%}{:<18}{:<10,.2%}{:<20}{:<10,.2%}\".format(\"precision(micro):\",precision_micro,\"recall(micro):\",recall_micro,\"f1-score(micro):\",fscore_micro))\n",
    "# print(\"{:<20}{:<10,.2%}{:<18}{:<10,.2%}{:<20}{:<10,.2%}\".format(\"precision(weight):\",precision_weighted,\"recall(weight):\",recall_weighted,\"f1-score(weight):\",fscore_weighted))\n",
    "# print()\n",
    "# print(metrics.classification_report(train_true, train_pred, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  '000': 0,  17,906,619\n",
    "#  '001': 1,     222,153\n",
    "#  '010': 2,     206,359\n",
    "#  '011': 3,       2,522\n",
    "#  '100': 4,   2,123,313\n",
    "#  '101': 5,   1,257,366\n",
    "#  '110': 6,   2,790,185\n",
    "#  '111': 7,      53,393"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sparsity rate of embedding vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H=np.array(H)\n",
    "non_zero=np.count_nonzero(H)\n",
    "total_val=np.product(H.shape)\n",
    "sparsity=(total_val-non_zero)/total_val\n",
    "density=non_zero/total_val\n",
    "print(\"sparsity rate is {:.2%}\".format(sparsity))\n",
    "print(\"density rate is {:.2%}\".format(density))\n",
    "print(\"embedding vector shape is {}\".format(H.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDX_train=np.array(IDX)\n",
    "H_train=np.array(H)\n",
    "mask_train=np.array(['train']*len(IDX_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDX_val=[]\n",
    "H_val=[]\n",
    "for input_nodes_raw, seeds_raw, blocks in tqdm(val_loader, position=0, leave=True):\n",
    "    blocks = [blk.to(device) for blk in blocks]\n",
    "    seeds=seeds_raw.to(device)\n",
    "    input_nodes={}\n",
    "    input_nodes[\"usaanr\"]=input_nodes_raw\n",
    "    input_nodes={k : e.to(device) for k, e in input_nodes.items()}\n",
    "    model.eval()\n",
    "    logits,h = model(input_nodes,blocks)\n",
    "    IDX_val.extend(seeds.detach().cpu().numpy().tolist())\n",
    "    H_val.extend(h.detach().cpu().numpy().tolist())\n",
    "    \n",
    "IDX_val=np.array(IDX_val)\n",
    "H_val=np.array(H_val)\n",
    "mask_val=np.array(['val']*len(IDX_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDX_test=[]\n",
    "H_test=[]\n",
    "for input_nodes_raw, seeds_raw, blocks in tqdm(test_loader, position=0, leave=True):\n",
    "    blocks = [blk.to(device) for blk in blocks]\n",
    "    seeds=seeds_raw.to(device)\n",
    "    input_nodes={}\n",
    "    input_nodes[\"usaanr\"]=input_nodes_raw\n",
    "    input_nodes={k : e.to(device) for k, e in input_nodes.items()}\n",
    "    model.eval()\n",
    "    logits,h = model(input_nodes,blocks)\n",
    "    IDX_test.extend(seeds.detach().cpu().numpy().tolist())\n",
    "    H_test.extend(h.detach().cpu().numpy().tolist())\n",
    "    \n",
    "IDX_test=np.array(IDX_test)\n",
    "H_test=np.array(H_test)\n",
    "mask_test=np.array(['test']*len(IDX_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDX=np.concatenate((IDX_train,IDX_val, IDX_test))\n",
    "H=np.concatenate((H_train,H_val, H_test))\n",
    "mask=np.concatenate((mask_train,mask_val, mask_test))\n",
    "\n",
    "_idx=IDX.argsort()  #### sort the node id from 0 to max_num, so that it can be matched with LABEL\n",
    "embedding_vector=H[_idx]\n",
    "mask=mask[_idx]\n",
    "\n",
    "print(\"{:<30}{}\".format(\"shape of embedding vector\",embedding_vector.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# savez_compressed(\"embedding_vector.npz\",embedding_vector)\n",
    "# embedding_vector=load(\"embedding_vector.npz\")['arr_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# embedding_vector=load(\"embedding_vector.npz\")['arr_0']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### visualization of embedding vectors for different categories of USAA Members"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF=pd.DataFrame({\"IDX\":_idx.tolist(), \"MASK\":mask.tolist(), \"Product\":LABEL.squeeze().tolist()})\n",
    "prod_map={0:\"No_Product\",1:\"Rental_only\",2:\"Home_only\",3:\"Auto_only\",4:\"Auto+Rental\",5:\"Auto+Home\"}\n",
    "DF['Product'] = list(map(prod_map.get, DF['Product']))\n",
    "\n",
    "### the caterogry of \"Home+Rental\" is too rare, only 2767 in total. So remove it.\n",
    "# inx=np.where(DF[\"Product\"].values!=\"Home+Rental\")[0]\n",
    "# DF=DF[DF[\"Product\"]!=\"Home+Rental\"]\n",
    "# embedding_vector=embedding_vector[inx]\n",
    "# DF[\"IDX\"]=np.arange(DF.shape[0])  ## Reindex the DF since the caterogry of \"Home+Rental\" has been removed\n",
    "\n",
    "N=1000\n",
    "\n",
    "# sample_df=DF.groupby(['Product'], group_keys=False).apply(lambda x: x.sample(n=N))\n",
    "# sample_id=sample_df[\"IDX\"].values\n",
    "# embedding_sample=embedding_vector[sample_id]\n",
    "# prod_sample=DF['Product'].values[sample_id]\n",
    "\n",
    "train_df=DF[DF[\"MASK\"]==\"train\"]\n",
    "sample_df_train=train_df.groupby(['Product'], group_keys=False).apply(lambda x: x.sample(n=N,random_state=101))\n",
    "sample_id_train=sample_df_train[\"IDX\"].values\n",
    "prod_sample_train=DF['Product'].values[sample_id_train]\n",
    "embedding_sample_train=embedding_vector[sample_id_train]\n",
    "\n",
    "val_df=DF[DF[\"MASK\"]==\"val\"]\n",
    "sample_df_val=val_df.groupby(['Product'], group_keys=False).apply(lambda x: x.sample(n=N,random_state=101))\n",
    "sample_id_val=sample_df_val[\"IDX\"].values\n",
    "prod_sample_val=DF['Product'].values[sample_id_val]\n",
    "embedding_sample_val=embedding_vector[sample_id_val]\n",
    "\n",
    "test_df=DF[DF[\"MASK\"]==\"test\"]\n",
    "sample_df_test=test_df.groupby(['Product'], group_keys=False).apply(lambda x: x.sample(n=N,random_state=101))\n",
    "sample_id_test=sample_df_test[\"IDX\"].values\n",
    "prod_sample_test=DF['Product'].values[sample_id_test]\n",
    "embedding_sample_test=embedding_vector[sample_id_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(\"t-SNE dimension reduction for training embedding vector:\")\n",
    "print()\n",
    "train_embedding = tsne_func.fit(embedding_sample_train)\n",
    "\n",
    "savez_compressed(\"RGCN_train_embedding_tsne.npz\",train_embedding)\n",
    "train_embedding=load(\"RGCN_train_embedding_tsne.npz\")['arr_0']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(\"t-SNE dimension reduction for validation embedding:\")\n",
    "print()\n",
    "val_embedding = tsne_func.fit(embedding_sample_val)\n",
    "\n",
    "savez_compressed(\"RGCN_val_embedding_tsne.npz\",val_embedding)\n",
    "val_embedding=load(\"RGCN_val_embedding_tsne.npz\")['arr_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(\"t-SNE dimension reduction for test embedding:\")\n",
    "print()\n",
    "test_embedding = tsne_func.fit(embedding_sample_test)\n",
    "\n",
    "savez_compressed(\"RGCN_test_embedding_tsne.npz\",test_embedding)\n",
    "test_embedding=load(\"RGCN_test_embedding_tsne.npz\")['arr_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b=np.unique(prod_sample_train,return_counts=True)\n",
    "pd.DataFrame({\"product\":a,\"sampled_#\":b}).style.format({'sampled_#':'{:,}'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "palette = sns.color_palette(\"bright\", 7)\n",
    "ax=sns.scatterplot(train_embedding[:,0], train_embedding[:,1], hue=prod_sample_train.squeeze(), legend='full', palette=palette)\n",
    "ax.set_title(\"Embedding Vectors t-SNE(sample=1000) \\n Training Set \",fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b=np.unique(prod_sample_val,return_counts=True)\n",
    "pd.DataFrame({\"product\":a,\"sampled_#\":b}).style.format({'sampled_#':'{:,}'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "palette = sns.color_palette(\"bright\", 7)\n",
    "ax=sns.scatterplot(val_embedding[:,0], val_embedding[:,1], hue=prod_sample_val.squeeze(), legend='full', palette=palette)\n",
    "ax.set_title(\"Embedding Vectors t-SNE (sample=1000) \\n Validation Set \",fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b=np.unique(prod_sample_test,return_counts=True)\n",
    "pd.DataFrame({\"product\":a,\"sampled_#\":b}).style.format({'sampled_#':'{:,}'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "palette = sns.color_palette(\"bright\", 7)\n",
    "ax=sns.scatterplot(test_embedding[:,0], test_embedding[:,1], hue=prod_sample_test.squeeze(), legend='full', palette=palette)\n",
    "ax.set_title(\"Embedding Vectors t-SNE (sample=1000) \\n Test Set \",fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
